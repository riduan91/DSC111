{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amphi 7 - More About Natural Language Processing [1] - Around Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Word Embeddings. Word Analogy\n",
    "\n",
    "## 1.1 Introduction\n",
    "\n",
    "In last amphi, we learned how to represent words by a one-hot vector of $K$ dimensions, where $K$ is a vocabulary of size $V$. Each word is denoted by a vector $O_{j}, j = 1, \\ldots, V$ (stands for one-hot vector). This representation, however, does not provide a good representation of similarity of words.\n",
    "\n",
    "We would like to have a representation, called **featurized representation** of words as vectors that allow us to build a \"similarity function\" between 2 words. This will group words of same concept (animal, fruit, people, etc.) together.\n",
    "\n",
    "We would like something like this\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <th>Word</th>\n",
    "    <th>Dimension 1 - People</th>\n",
    "    <th>Dimension 2 - Gender</th>\n",
    "    <th>Dimension 3 - Fruit</th>\n",
    "    <th>Dimension 4 - Animal</th>\n",
    "    <th>Dimension 5 - Age</th>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Man</td>\n",
    "    <td>1</td>\n",
    "    <td>0.97</td>\n",
    "    <td>0.02</td>\n",
    "    <td>0.32</td>\n",
    "    <td>0.45</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Queen</td>\n",
    "    <td>0.99</td>\n",
    "    <td>-0.97</td>\n",
    "    <td>-0.02</td>\n",
    "    <td>0.24</td>\n",
    "    <td>0.45</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Dog</td>\n",
    "    <td>0.11</td>\n",
    "    <td>0.07</td>\n",
    "    <td>-0.01</td>\n",
    "    <td>0.98</td>\n",
    "    <td>0.06</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Child</td>\n",
    "    <td>0.96</td>\n",
    "    <td>0.22</td>\n",
    "    <td>0.03</td>\n",
    "    <td>0.31</td>\n",
    "    <td>-0.87</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "</tr>  \n",
    "<tr>\n",
    "    <td>Apple</td>\n",
    "    <td>0.01</td>\n",
    "    <td>-0.02</td>\n",
    "    <td>-0.03</td>\n",
    "    <td>0</td>\n",
    "    <td>0.96</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The idea of word embeddings is to embed words (as vectors in high dimensionality) to some subspace of lower-dimension.\n",
    "\n",
    "**Application: **\n",
    "\n",
    "We can use these similarity to deduce concept in other sentences. For example, in name recognition.\n",
    "\n",
    "Training example: ***Henri IV is a king.***\n",
    "\n",
    "We know that ***Henri IV*** is a name entity.\n",
    "\n",
    "Test example: ***Elizabeth II is a queen.***\n",
    "\n",
    "We know that ***queen*** is very similar to ***king*** (in fact they refer to the ***people*** group), then we can deduce that ***Elizabeth II*** is a name entity. \n",
    "\n",
    "**In reality**, the meaning of the dimensions of those vectors are more complicated to figure out than something like \"gender\", \"age\", \"fruit\", etc. But it can give similarity between words and pairs of words.\n",
    "\n",
    "The strategy of reduce the dimensionality of the word space to some lower-dimensional space that allows better representation for word similarity is called **word embeddings**.\n",
    "\n",
    "**Strategy to use word embeddings in NLP problems** \n",
    "\n",
    "1. Learn word embeddings from large text corpus (or get some pre-trained word embeddings)\n",
    "\n",
    "2. Transfer embedding to new NLP problem with smaller training set (like name entity recognition)\n",
    "\n",
    "3. Continue to improve word embeddings with new data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Analogy (Word Similarity)\n",
    "\n",
    "We would like to define analogy between pairs of words like `(man, woman)` $\\sim$ `(king, queen)`.\n",
    "\n",
    "In the previous table, we can see something like this:\n",
    "$$\n",
    "e_{man} - e_{woman} \\approx (0, 2, \\ldots, 0)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "e_{king} - e_{queen} \\approx (0, 2, \\ldots, 0)\n",
    "$$\n",
    "\n",
    "This gives us an idea about the notion of similarity: $(w_1, w_2)$ are said to be similar to $(v_1, v_2)$ if $w_1 - w_2 \\approx v_1 - v_2$.\n",
    "\n",
    "We can define a function to evaluate the similarity between pairs of words:\n",
    "\n",
    "**Eucliean similarity**\n",
    "\n",
    "For 2 embeddings $w_1, w_2$, the Euclidean similarity of them is defined by\n",
    "\n",
    "$$\n",
    "d(w_1, w_2) = \\Vert w_1 - w_2 \\Vert^2\n",
    "$$\n",
    "\n",
    "Then $(w_1, w_2)$ and $(v_1, v_2)$ are said to be similar if $d(w_1 - w_2 + v_2, w_1) $ is a small positive number ($\\approx 0$)\n",
    "\n",
    "**Cosine similarity**\n",
    "\n",
    "For 2 embeddings $w_1, w_2$, the cosine similarity of them is defined by\n",
    "$$\n",
    "\\mathrm{sim}(w_1, w_2) = \\frac{w_1 \\cdot w_2}{|w_1|\\cdot |w_2|}\n",
    "$$\n",
    "\n",
    "Then $(w_1, w_2)$ is said similar to $(v_1, v_2)$ if\n",
    "$$\n",
    "\\mathrm{sim}(w_1 - w_2 + v_2, v_1) \n",
    "$$\n",
    "is near 1. ($\\approx 1$)\n",
    "\n",
    "If it is near -1, then $(w_1, w_2)$ is similar to $(-v_1, v_2)$.\n",
    "\n",
    "**Application**\n",
    "\n",
    "Suppose we have the sentences:\n",
    "\n",
    "- `Beijing is the capital of China.`\n",
    "\n",
    "and\n",
    "\n",
    "- `Hanoi is the capital of ... .`\n",
    "\n",
    "We can try to predict the missing word by solving:\n",
    "$$\n",
    "\\max \\limits_{v \\in \\textrm{The vocabulary}} \\mathrm{sim}(w_1 - w_2 + v_2, v)\n",
    "$$\n",
    "\n",
    "(for cosine similarity)\n",
    "\n",
    "or\n",
    "$$\n",
    "\\max \\limits_{v \\in \\textrm{The vocabulary}} d(w_1 - w_2 + v_2, v)\n",
    "$$\n",
    "\n",
    "(for Euclidean distance similarity)\n",
    "\n",
    "where $w_1 = $`China`, $w_2 = $`Beijing`, $v_2 =$`Hanoi`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Language Modelling with Word Embeddings\n",
    "\n",
    "We return to the language modeling problem. Given a text, predict the next word. This time, we do not use one-hot-coding, but word embeddings.\n",
    "\n",
    "<img src=\"F1.png\" width=700>\n",
    "<center>\n",
    "    In the figure, $x^{<i>}$ are now word embeddings\n",
    "</center>\n",
    "\n",
    "**Embedding matrix**\n",
    "\n",
    "Suppose we embed $V$ words into a space of $D$ dimensions. The $D\\times V$-matrix whose columns are embedded vectors of $O_1, \\ldots O_d$ are called the **embedding matrix** associated to our word embeddings. We will denote this matrix by $E$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pretrained Models (Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Pretrained Word2Vec model\n",
    "\n",
    "The Word2vec model learns word embeddings from text classification problems. We can download the pretrained model (in English) online, for example at https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "\n",
    "There are 3 million words and phrases. Words are represented by 300-dimensional vectors. Complex words like *\"New York\"* are represented with underscore \"\\_\" like *\"New\\_York\"*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from udemy NLP course by LazyProgrammer\n",
    "# Should take 1 min or more\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "FILE_LOCATION_W2V = './GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors_w2v = KeyedVectors.load_word2vec_format(\n",
    "  FILE_LOCATION_W2V,\n",
    "  binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Vector representation of a word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n",
      " -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n",
      "  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n",
      " -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n",
      "  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n",
      "  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n",
      "  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n",
      "  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n",
      "  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n",
      "  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n",
      "  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n",
      "  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n",
      " -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n",
      " -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n",
      " -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n",
      "  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n",
      "  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n",
      " -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n",
      " -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n",
      " -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n",
      "  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n",
      "  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n",
      "  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n",
      " -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n",
      " -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n",
      "  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n",
      " -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n",
      "  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n",
      " -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n",
      " -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n",
      "  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n",
      " -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n",
      " -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n",
      "  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n",
      " -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n",
      "  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n",
      "  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n",
      " -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n",
      " -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n",
      " -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n",
      " -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n",
      " -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n",
      "  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n",
      "  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n",
      "  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n",
      " -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n",
      "  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n",
      "  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n",
      " -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n",
      " -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n",
      " -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n",
      "  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n",
      "  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n",
      " -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n",
      "  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n",
      " -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n",
      "  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n",
      "  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n",
      " -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n",
      "  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n",
      "  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n",
      "  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n",
      "  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n",
      "  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n",
      " -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n",
      "  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n",
      " -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n",
      "  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n",
      "  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n",
      " -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n",
      "  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n",
      "  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n",
      "  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n",
      " -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n",
      " -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors_w2v.get_vector('king'))\n",
    "print(word_vectors_w2v.get_vector('king').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Find analogy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogies_w2v(w1, w2, w3):\n",
    "  r = word_vectors_w2v.most_similar(positive=[w1, w3], negative=[w2])\n",
    "  print(\"%s - %s = %s - %s\" % (w1, w2, r[0][0], w3))\n",
    "\n",
    "def nearest_neighbors_w2v(w):\n",
    "  r = word_vectors_w2v.most_similar(positive=[w])\n",
    "  print(\"neighbors of: %s\" % w)\n",
    "  for word, score in r:\n",
    "    print(\"\\t%s\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Evaluate the word embeddings by word analogy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man = queen - woman\n",
      "france - paris = england - london\n",
      "france - paris = italy - rome\n",
      "paris - france = lohan - italy\n",
      "france - french = england - english\n",
      "japan - japanese = tibet - chinese\n",
      "japan - japanese = italy - italian\n",
      "japan - japanese = queensland - australian\n",
      "december - november = september - june\n",
      "miami - florida = dallas - texas\n",
      "einstein - scientist = jude - painter\n",
      "china - rice = dinnerware - bread\n",
      "man - woman = he - she\n",
      "man - woman = uncle - aunt\n",
      "man - woman = brother - sister\n",
      "man - woman = son - wife\n",
      "man - woman = actor - actress\n",
      "man - woman = father - mother\n",
      "heir - heiress = prince - princess\n",
      "nephew - niece = uncle - aunt\n",
      "france - paris = japan - tokyo\n",
      "france - paris = chinese - beijing\n",
      "february - january = april - november\n",
      "france - paris = italy - rome\n",
      "paris - france = lohan - italy\n"
     ]
    }
   ],
   "source": [
    "find_analogies_w2v('king', 'man', 'woman')\n",
    "find_analogies_w2v('france', 'paris', 'london')\n",
    "find_analogies_w2v('france', 'paris', 'rome')\n",
    "find_analogies_w2v('paris', 'france', 'italy')\n",
    "find_analogies_w2v('france', 'french', 'english')\n",
    "find_analogies_w2v('japan', 'japanese', 'chinese')\n",
    "find_analogies_w2v('japan', 'japanese', 'italian')\n",
    "find_analogies_w2v('japan', 'japanese', 'australian')\n",
    "find_analogies_w2v('december', 'november', 'june')\n",
    "find_analogies_w2v('miami', 'florida', 'texas')\n",
    "find_analogies_w2v('einstein', 'scientist', 'painter')\n",
    "find_analogies_w2v('china', 'rice', 'bread')\n",
    "find_analogies_w2v('man', 'woman', 'she')\n",
    "find_analogies_w2v('man', 'woman', 'aunt')\n",
    "find_analogies_w2v('man', 'woman', 'sister')\n",
    "find_analogies_w2v('man', 'woman', 'wife')\n",
    "find_analogies_w2v('man', 'woman', 'actress')\n",
    "find_analogies_w2v('man', 'woman', 'mother')\n",
    "find_analogies_w2v('heir', 'heiress', 'princess')\n",
    "find_analogies_w2v('nephew', 'niece', 'aunt')\n",
    "find_analogies_w2v('france', 'paris', 'tokyo')\n",
    "find_analogies_w2v('france', 'paris', 'beijing')\n",
    "find_analogies_w2v('february', 'january', 'november')\n",
    "find_analogies_w2v('france', 'paris', 'rome')\n",
    "find_analogies_w2v('paris', 'france', 'italy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the word embeddings by nearest neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors of: king\n",
      "\tkings\n",
      "\tqueen\n",
      "\tmonarch\n",
      "\tcrown_prince\n",
      "\tprince\n",
      "\tsultan\n",
      "\truler\n",
      "\tprinces\n",
      "\tPrince_Paras\n",
      "\tthrone\n",
      "neighbors of: france\n",
      "\tspain\n",
      "\tfrench\n",
      "\tgermany\n",
      "\teurope\n",
      "\titaly\n",
      "\tengland\n",
      "\teuropean\n",
      "\tbelgium\n",
      "\tusa\n",
      "\tserbia\n",
      "neighbors of: japan\n",
      "\tjapanese\n",
      "\ttokyo\n",
      "\tamerica\n",
      "\teurope\n",
      "\tgermany\n",
      "\tchinese\n",
      "\tindia\n",
      "\thawaii\n",
      "\tusa\n",
      "\tkorea\n",
      "neighbors of: einstein\n",
      "\tnikki\n",
      "\tlmfao\n",
      "\talbert\n",
      "\tarmstrong\n",
      "\tjoan\n",
      "\tbecky\n",
      "\tmcmahon\n",
      "\tconrad\n",
      "\tlori\n",
      "\thaley\n",
      "neighbors of: woman\n",
      "\tman\n",
      "\tgirl\n",
      "\tteenage_girl\n",
      "\tteenager\n",
      "\tlady\n",
      "\tteenaged_girl\n",
      "\tmother\n",
      "\tpolicewoman\n",
      "\tboy\n",
      "\tWoman\n",
      "neighbors of: nephew\n",
      "\tson\n",
      "\tuncle\n",
      "\tbrother\n",
      "\tgrandson\n",
      "\tcousin\n",
      "\tfather\n",
      "\tniece\n",
      "\tyounger_brother\n",
      "\tnephews\n",
      "\tstepson\n",
      "neighbors of: february\n",
      "\tjanuary\n",
      "\tapril\n",
      "\tseptember\n",
      "\tdecember\n",
      "\tjuly\n",
      "\toctober\n",
      "\tnovember\n",
      "\tjune\n",
      "\tfeb\n",
      "\tnorway\n",
      "neighbors of: rome\n",
      "\tathens\n",
      "\talbert\n",
      "\tholmes\n",
      "\tdonnie\n",
      "\titaly\n",
      "\ttoni\n",
      "\tspain\n",
      "\tjh\n",
      "\tpablo\n",
      "\tmalta\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors_w2v('king')\n",
    "nearest_neighbors_w2v('france')\n",
    "nearest_neighbors_w2v('japan')\n",
    "nearest_neighbors_w2v('einstein')\n",
    "nearest_neighbors_w2v('woman')\n",
    "nearest_neighbors_w2v('nephew')\n",
    "nearest_neighbors_w2v('february')\n",
    "nearest_neighbors_w2v('rome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris - france = samuel - vietnam\n",
      "paris - france = bangkok - thailand\n"
     ]
    }
   ],
   "source": [
    "find_analogies_w2v('paris', 'france', 'vietnam')\n",
    "find_analogies_w2v('paris', 'france', 'thailand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate word embeddings by classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Pretrained GloVe model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe model learns word embeddings from word similarity. We can download the pretrained model (in English) online, for example at http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "There are 50000-300000 words. Words are represented by 300-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_LOCATION_GLOVE = './glove.6B/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define some metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "def dist1(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "def dist2(a, b):\n",
    "    return 1 - a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# pick a distance type\n",
    "dist, metric = dist2, 'cosine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loading word vectors...')\n",
    "word_vectors_glove = {}\n",
    "embedding = []\n",
    "idx2word = []\n",
    "with open(FILE_LOCATION_GLOVE, encoding='utf-8') as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word_vectors_glove[word] = vec\n",
    "        embedding.append(vec)\n",
    "        idx2word.append(word)\n",
    "print('Found %s word vectors.' % len(word_vectors_glove))\n",
    "embedding = np.array(embedding)\n",
    "V, D = embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Word analogy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogies_glove(w1, w2, w3, word_vectors_glove = word_vectors_glove):\n",
    "    for w in (w1, w2, w3):\n",
    "        if w not in word_vectors_glove:\n",
    "            print(\"%s not in dictionary\" % w)\n",
    "            return\n",
    "\n",
    "    king = word_vectors_glove[w1]\n",
    "    man = word_vectors_glove[w2]\n",
    "    woman = word_vectors_glove[w3]\n",
    "    v0 = king - man + woman\n",
    "\n",
    "    distances = pairwise_distances(v0.reshape(1, D), embedding, metric=metric).reshape(V)\n",
    "    idxs = distances.argsort()[:4]\n",
    "    for idx in idxs:\n",
    "        word = idx2word[idx]\n",
    "        if word not in (w1, w2, w3): \n",
    "            best_word = word\n",
    "            break\n",
    "\n",
    "    print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)\n",
    "\n",
    "def nearest_neighbors_glove(w, n=5, word_vectors_glove = word_vectors_glove):\n",
    "    if w not in word_vectors_glove:\n",
    "        print(\"%s not in dictionary:\" % w)\n",
    "        return\n",
    "\n",
    "    v = word_vectors_glove[w]\n",
    "    distances = pairwise_distances(v.reshape(1, D), embedding, metric=metric).reshape(V)\n",
    "    idxs = distances.argsort()[1:n+1]\n",
    "    print(\"neighbors of: %s\" % w)\n",
    "    for idx in idxs:\n",
    "        print(\"\\t%s\" % idx2word[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Evaluate the model with word analogy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man = queen - woman\n",
      "france - paris = britain - london\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n",
      "france - french = england - english\n",
      "japan - japanese = china - chinese\n",
      "japan - japanese = italy - italian\n",
      "japan - japanese = australia - australian\n",
      "december - november = july - june\n",
      "miami - florida = houston - texas\n",
      "einstein - scientist = matisse - painter\n",
      "china - rice = chinese - bread\n",
      "man - woman = he - she\n",
      "man - woman = uncle - aunt\n",
      "man - woman = brother - sister\n",
      "man - woman = friend - wife\n",
      "man - woman = actor - actress\n",
      "man - woman = father - mother\n",
      "heir - heiress = queen - princess\n",
      "nephew - niece = uncle - aunt\n",
      "france - paris = japan - tokyo\n",
      "france - paris = china - beijing\n",
      "february - january = october - november\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n"
     ]
    }
   ],
   "source": [
    "find_analogies_glove('king', 'man', 'woman')\n",
    "find_analogies_glove('france', 'paris', 'london')\n",
    "find_analogies_glove('france', 'paris', 'rome')\n",
    "find_analogies_glove('paris', 'france', 'italy')\n",
    "find_analogies_glove('france', 'french', 'english')\n",
    "find_analogies_glove('japan', 'japanese', 'chinese')\n",
    "find_analogies_glove('japan', 'japanese', 'italian')\n",
    "find_analogies_glove('japan', 'japanese', 'australian')\n",
    "find_analogies_glove('december', 'november', 'june')\n",
    "find_analogies_glove('miami', 'florida', 'texas')\n",
    "find_analogies_glove('einstein', 'scientist', 'painter')\n",
    "find_analogies_glove('china', 'rice', 'bread')\n",
    "find_analogies_glove('man', 'woman', 'she')\n",
    "find_analogies_glove('man', 'woman', 'aunt')\n",
    "find_analogies_glove('man', 'woman', 'sister')\n",
    "find_analogies_glove('man', 'woman', 'wife')\n",
    "find_analogies_glove('man', 'woman', 'actress')\n",
    "find_analogies_glove('man', 'woman', 'mother')\n",
    "find_analogies_glove('heir', 'heiress', 'princess')\n",
    "find_analogies_glove('nephew', 'niece', 'aunt')\n",
    "find_analogies_glove('france', 'paris', 'tokyo')\n",
    "find_analogies_glove('france', 'paris', 'beijing')\n",
    "find_analogies_glove('february', 'january', 'november')\n",
    "find_analogies_glove('france', 'paris', 'rome')\n",
    "find_analogies_glove('paris', 'france', 'italy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors of: king\n",
      "\tprince\n",
      "\tqueen\n",
      "\tii\n",
      "\temperor\n",
      "\tson\n",
      "neighbors of: france\n",
      "\tfrench\n",
      "\tbelgium\n",
      "\tparis\n",
      "\tspain\n",
      "\tnetherlands\n",
      "neighbors of: japan\n",
      "\tjapanese\n",
      "\tchina\n",
      "\tkorea\n",
      "\ttokyo\n",
      "\ttaiwan\n",
      "neighbors of: einstein\n",
      "\trelativity\n",
      "\tbohr\n",
      "\tphysics\n",
      "\theisenberg\n",
      "\tfreud\n",
      "neighbors of: woman\n",
      "\tgirl\n",
      "\tman\n",
      "\tmother\n",
      "\ther\n",
      "\tboy\n",
      "neighbors of: nephew\n",
      "\tcousin\n",
      "\tbrother\n",
      "\tgrandson\n",
      "\tson\n",
      "\tuncle\n",
      "neighbors of: february\n",
      "\toctober\n",
      "\tdecember\n",
      "\tjanuary\n",
      "\taugust\n",
      "\tseptember\n",
      "neighbors of: rome\n",
      "\tnaples\n",
      "\tvenice\n",
      "\titaly\n",
      "\tturin\n",
      "\tpope\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors_glove('king')\n",
    "nearest_neighbors_glove('france')\n",
    "nearest_neighbors_glove('japan')\n",
    "nearest_neighbors_glove('einstein')\n",
    "nearest_neighbors_glove('woman')\n",
    "nearest_neighbors_glove('nephew')\n",
    "nearest_neighbors_glove('february')\n",
    "nearest_neighbors_glove('rome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate word embeddings by classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Language Modeling (cont'd) - Bigram and N-grams\n",
    "\n",
    "Recall that the language model is a model of the probability of a sequence of words.\n",
    "\n",
    "## 3.1 Bigrams, Trigrams, N-grams Models\n",
    "\n",
    "### 3.1.1 Definitions\n",
    "\n",
    "A **bigram**, **trigram**, **$N$-gram** is a sequence of 2, 3, $N$ consecutive words, respectively.\n",
    "\n",
    "We will denote the current word by $w^{<t>}$ and the next word by $w^{<t+1>}$\n",
    "\n",
    "\n",
    "A **bigram model** is a model that predict the probability of the next word using the current word only.\n",
    "\n",
    "For example, the model predicts something like\n",
    "$$\n",
    "\\mathbf P(w^{<t+1>} = \"juice\" | w^{<t>} = \"orange\") = 0.07\n",
    "$$\n",
    "(or shortly $\\mathbf P(\"juice\"|\"orange\") = 0.07$)\n",
    "\n",
    "Similarly,\n",
    "$$\n",
    "\\mathbf P(\"the\"| \"the\") = 0\n",
    "$$\n",
    "\n",
    "Similarly, an **$N-$gram model** is a model that predicts the probability of the next word using $N-1$ previous words. \n",
    "\n",
    "How to construct those probabilities? One strategy is counting\n",
    "\n",
    "### 3.1.2 Counting\n",
    "\n",
    "One of the strategy to construct these probabilities is **counting**. Suppose we have a corpus of huge number of documents. This strategy suggests calculating the probability\n",
    "\n",
    "$$\n",
    "\\mathbf P(w^{<t+1>} =B |w^{<t>}=A) = \\frac{count(A\\to B)}{count(A)}\n",
    "$$\n",
    "\n",
    "In probability language, we modelize the joint probability $\\mathbf P(A \\to B)$ as the frequency of \"AB\" as a consecutive sequence, then $P(A)$ as the frequency of \"A\" in the corpus. The conditional probabilities gives us the bigram model.\n",
    "\n",
    "This is also called \"maximum-likelihood\" counting because we base on the training corpus to get what is the most likely-to-happen probability for the next word.\n",
    "\n",
    "**Generalization**\n",
    "\n",
    "We can generalize counting strategy to $N-$gram model.\n",
    "$$\n",
    "\\mathbf P(A_N | A_1 \\to A_2 \\to \\ldots A_{N-1}) = \\frac{count(A_1\\to A_2\\to \\ldots\\to A_{N-1})}{count(A_1 \\to A_2 \\to \\ldots A_{N-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Add-one smoothing\n",
    "\n",
    "Instead of maximum-likelihood counting, we can add a small number to each count. Let $V$ be the vocabulary size (number of distinct words), then \n",
    "\n",
    "$$\n",
    "\\mathbf P_{smooth} (B | A) = \\frac{count(A\\to B) + 1}{count(A) + V}\n",
    "$$\n",
    "\n",
    "is also a valid probabilistic model (sum of probabilities on all different $B$ = 1). This model will avoid overfitting on the training set. For example, in our corpus if the sequence \"orange leaf\" does not exist, will still give it a small probability to be able to happen in generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Markow Assumption for N-gram Model\n",
    "\n",
    "The Markov assumption for N-gram model is: \"What we see now depends only on what we see on the previous $N-1$ steps.\" That can also be translated by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\mathbf P(w^{<t>}=A_N | w^{<1>}=B_1 \\to \\ldots \\to w^{<t-N-1>}=B_{t-N-1}, w^{<t-N>} = A_1\\to \\ldots \\to w^{t-1}=A_{N-1}) \\\\\n",
    "                 &= \\mathbf P(w^{<N>}=A_N | w^{<1>}=A_1\\to \\ldots \\to w^{N-1}=A_{N-1}) \\\\\n",
    "                 &= \\mathbf P(w^{<t>}=A_N | w^{<t-N>}=A_1\\to \\ldots \\to w^{t-1}=A_{N-1}) \\\\\n",
    "                 &= \\mathbf P(w^{<N>}=A_N | w^{<1>}=A_1\\to \\ldots \\to w^{N-1}=A_{N-1}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for any $t, A_1, \\ldots, A_N, B_1, \\ldots, B_{t-N-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corollary for a model with only bigrams**\n",
    "\n",
    "If we construct a model with only bigrams, we can refind the probabilistic model for joint probability\n",
    "$$\n",
    "\\mathbf P(A_1, \\ldots, A_N) = \\mathbf P(A_1) \\mathbf P(A_2|A_1) \\mathbf P(A_3|A_2) \\ldots \\mathbf P(A_N | A_{N-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Implementation of a bigram model\n",
    "\n",
    "In implementation, we should take $\\log$ of probabilities to avoid them to reach 0.\n",
    "\n",
    "We will use the Brown corpus at https://www.nltk.org/book/ch02.html. In file `brown.py`, we also wrote some functions to get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brown import get_sentences_with_word2idx, get_sentences_with_word2idx_limit_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ndoannguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=1):\n",
    "  # start_idx, end_idx are special \"words\". A sentence always begin with start_idx and end with end_idx\n",
    "  bigram_probs = np.ones((V, V)) * smoothing\n",
    "  for sentence in sentences:\n",
    "    for i in range(len(sentence)):\n",
    "      \n",
    "      if i == 0:\n",
    "        # beginning word\n",
    "        bigram_probs[start_idx, sentence[i]] += 1\n",
    "      else:\n",
    "        # middle word\n",
    "        bigram_probs[sentence[i-1], sentence[i]] += 1\n",
    "\n",
    "      # if we're at the final word\n",
    "      # we update the bigram for last -> current\n",
    "      # AND current -> END token\n",
    "      if i == len(sentence) - 1:\n",
    "        # final word\n",
    "        bigram_probs[sentence[i], end_idx] += 1\n",
    "\n",
    "  # normalize the counts along the rows to get probabilities\n",
    "  bigram_probs /= bigram_probs.sum(axis=1, keepdims=True)\n",
    "  return bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START inf\n",
      "END inf\n",
      "man inf\n",
      "paris inf\n",
      "britain inf\n",
      "england inf\n",
      "king inf\n",
      "woman inf\n",
      "rome inf\n",
      "london inf\n",
      "queen inf\n",
      "italy inf\n",
      "france inf\n",
      "the 69971\n",
      ", 58334\n",
      ". 49346\n",
      "of 36412\n",
      "and 28853\n",
      "to 26158\n",
      "a 23195\n",
      "in 21337\n",
      "that 10594\n",
      "is 10109\n",
      "was 9815\n",
      "he 9548\n",
      "for 9489\n",
      "`` 8837\n",
      "'' 8789\n",
      "it 8760\n",
      "with 7289\n",
      "as 7253\n",
      "his 6996\n",
      "on 6741\n",
      "be 6377\n",
      "; 5566\n",
      "at 5372\n",
      "by 5306\n",
      "i 5164\n",
      "this 5145\n",
      "had 5133\n",
      "? 4693\n",
      "not 4610\n",
      "are 4394\n",
      "but 4381\n",
      "from 4370\n",
      "or 4206\n",
      "have 3942\n",
      "an 3740\n",
      "they 3620\n",
      "which 3561\n",
      "-- 3432\n",
      "one 3292\n",
      "you 3286\n",
      "were 3284\n",
      "her 3036\n",
      "all 3001\n",
      "she 2860\n",
      "there 2728\n",
      "would 2714\n",
      "their 2669\n",
      "we 2652\n",
      "him 2619\n",
      "been 2472\n",
      ") 2466\n",
      "has 2437\n",
      "( 2435\n",
      "when 2331\n",
      "who 2252\n",
      "will 2245\n",
      "more 2215\n",
      "if 2198\n",
      "no 2139\n",
      "out 2097\n",
      "so 1985\n",
      "said 1961\n",
      "what 1908\n",
      "up 1890\n",
      "its 1858\n",
      "about 1815\n",
      ": 1795\n",
      "into 1791\n",
      "than 1790\n",
      "them 1788\n",
      "can 1772\n",
      "only 1748\n",
      "other 1702\n",
      "new 1635\n",
      "some 1618\n",
      "could 1601\n",
      "time 1598\n",
      "! 1596\n",
      "these 1573\n",
      "two 1412\n",
      "may 1402\n",
      "then 1380\n",
      "do 1363\n",
      "first 1361\n",
      "any 1344\n",
      "my 1318\n",
      "now 1314\n",
      "such 1303\n",
      "like 1292\n",
      "our 1252\n",
      "over 1236\n",
      "me 1181\n",
      "even 1170\n",
      "most 1159\n",
      "made 1125\n",
      "also 1069\n",
      "after 1069\n",
      "did 1044\n",
      "many 1030\n",
      "before 1016\n",
      "must 1013\n",
      "af 996\n",
      "through 971\n",
      "back 966\n",
      "years 950\n",
      "where 937\n",
      "much 937\n",
      "your 923\n",
      "way 908\n",
      "well 897\n",
      "down 895\n",
      "should 888\n",
      "because 883\n",
      "each 877\n",
      "just 872\n",
      "those 850\n",
      "people 847\n",
      "mr. 844\n",
      "too 834\n",
      "how 834\n",
      "little 831\n",
      "state 807\n",
      "good 806\n",
      "very 796\n",
      "make 794\n",
      "world 787\n",
      "still 782\n",
      "see 772\n",
      "own 772\n",
      "men 763\n",
      "work 762\n",
      "long 752\n",
      "here 750\n",
      "get 749\n",
      "both 730\n",
      "between 730\n",
      "life 715\n",
      "being 712\n",
      "under 707\n",
      "never 697\n",
      "day 687\n",
      "same 686\n",
      "another 684\n",
      "know 683\n",
      "while 680\n",
      "last 676\n",
      "us 675\n",
      "might 672\n",
      "great 665\n",
      "old 661\n",
      "year 658\n",
      "off 639\n",
      "come 630\n",
      "since 628\n",
      "against 627\n",
      "go 626\n",
      "came 622\n",
      "right 613\n",
      "used 611\n",
      "take 610\n",
      "three 610\n",
      "himself 603\n",
      "states 603\n",
      "few 601\n",
      "house 591\n",
      "use 591\n",
      "during 585\n",
      "without 583\n",
      "again 577\n",
      "place 570\n",
      "american 569\n",
      "around 562\n",
      "however 552\n",
      "home 547\n",
      "small 542\n",
      "found 536\n",
      "mrs. 534\n",
      "1 527\n",
      "thought 517\n",
      "went 507\n",
      "say 504\n",
      "part 500\n",
      "once 499\n",
      "general 498\n",
      "high 497\n",
      "upon 495\n",
      "school 493\n",
      "every 491\n",
      "don't 489\n",
      "does 485\n",
      "got 482\n",
      "united 482\n",
      "left 480\n",
      "number 472\n",
      "course 465\n",
      "war 464\n",
      "until 461\n",
      "always 458\n",
      "away 456\n",
      "something 450\n",
      "fact 447\n",
      "2 446\n",
      "water 445\n",
      "though 440\n",
      "public 438\n",
      "less 437\n",
      "put 437\n",
      "think 433\n",
      "almost 432\n",
      "hand 431\n",
      "enough 430\n",
      "took 426\n",
      "far 426\n",
      "head 424\n",
      "yet 419\n",
      "government 418\n",
      "system 416\n",
      "set 414\n",
      "better 414\n",
      "told 413\n",
      "night 411\n",
      "nothing 411\n",
      "end 409\n",
      "why 404\n",
      "didn't 401\n",
      "called 401\n",
      "eyes 401\n",
      "find 400\n",
      "going 399\n",
      "look 399\n",
      "asked 398\n",
      "later 397\n",
      "knew 395\n",
      "point 395\n",
      "next 394\n",
      "program 394\n",
      "city 393\n",
      "business 393\n",
      "group 390\n",
      "give 389\n",
      "toward 386\n",
      "young 385\n",
      "let 384\n",
      "days 384\n",
      "room 384\n",
      "president 382\n",
      "side 381\n",
      "social 380\n",
      "present 377\n",
      "given 377\n",
      "several 377\n",
      "order 376\n",
      "national 375\n",
      "possible 374\n",
      "rather 373\n",
      "second 373\n",
      "face 371\n",
      "per 371\n",
      "among 370\n",
      "form 370\n",
      "often 369\n",
      "important 369\n",
      "things 368\n",
      "looked 367\n",
      "early 366\n",
      "white 365\n",
      "john 362\n",
      "case 362\n",
      "large 361\n",
      "four 360\n",
      "need 360\n",
      "big 360\n",
      "become 359\n",
      "within 359\n",
      "felt 357\n",
      "children 355\n",
      "along 355\n",
      "saw 352\n",
      "best 351\n",
      "church 348\n",
      "ever 344\n",
      "least 343\n",
      "power 342\n",
      "development 334\n",
      "seemed 333\n",
      "thing 333\n",
      "light 333\n",
      "family 331\n",
      "interest 330\n",
      "want 328\n",
      "members 325\n",
      "mind 325\n",
      "area 324\n",
      "country 324\n",
      "others 323\n",
      "although 321\n",
      "turned 320\n",
      "done 319\n",
      "open 318\n",
      "' 317\n",
      "god 316\n",
      "service 315\n",
      "problem 313\n",
      "certain 313\n",
      "kind 313\n",
      "different 312\n",
      "thus 312\n",
      "began 312\n",
      "door 312\n",
      "help 311\n",
      "sense 311\n",
      "means 310\n",
      "whole 309\n",
      "matter 308\n",
      "perhaps 307\n",
      "itself 304\n",
      "york 302\n",
      "it's 302\n",
      "times 300\n",
      "law 299\n",
      "human 299\n",
      "line 298\n",
      "above 296\n",
      "name 294\n",
      "example 292\n",
      "action 291\n",
      "company 290\n",
      "hands 289\n",
      "local 288\n",
      "show 288\n",
      "3 287\n",
      "whether 286\n",
      "five 286\n",
      "history 286\n",
      "gave 285\n",
      "today 284\n",
      "either 284\n",
      "act 283\n",
      "feet 283\n",
      "across 282\n",
      "taken 281\n",
      "past 281\n",
      "quite 281\n",
      "anything 280\n",
      "seen 279\n",
      "having 279\n",
      "death 277\n",
      "experience 276\n",
      "body 276\n",
      "week 275\n",
      "half 275\n",
      "really 275\n",
      "word 274\n",
      "field 274\n",
      "car 274\n",
      "words 274\n",
      "already 273\n",
      "themselves 270\n",
      "i'm 269\n",
      "information 269\n",
      "tell 268\n",
      "shall 268\n",
      "together 267\n",
      "college 267\n",
      "money 265\n",
      "period 265\n",
      "held 264\n",
      "keep 264\n",
      "sure 263\n",
      "probably 261\n",
      "free 259\n",
      "seems 259\n",
      "political 258\n",
      "real 258\n",
      "cannot 258\n",
      "behind 258\n",
      "question 257\n",
      "air 257\n",
      "office 255\n",
      "making 255\n",
      "brought 253\n",
      "miss 253\n",
      "whose 251\n",
      "special 250\n",
      "major 247\n",
      "heard 247\n",
      "problems 247\n",
      "federal 246\n",
      "became 246\n",
      "study 246\n",
      "ago 246\n",
      "moment 246\n",
      "available 245\n",
      "known 245\n",
      "result 244\n",
      "street 244\n",
      "economic 243\n",
      "boy 242\n",
      "position 241\n",
      "reason 241\n",
      "change 240\n",
      "south 240\n",
      "board 239\n",
      "individual 239\n",
      "job 238\n",
      "am 237\n",
      "society 237\n",
      "areas 236\n",
      "west 235\n",
      "close 234\n",
      "turn 233\n",
      "community 231\n",
      "true 231\n",
      "love 231\n",
      "court 230\n",
      "force 230\n",
      "full 230\n",
      "cost 229\n",
      "seem 229\n",
      "wife 228\n",
      "future 227\n",
      "age 227\n",
      "wanted 226\n",
      "voice 226\n",
      "department 225\n",
      "center 224\n",
      "control 223\n",
      "common 223\n",
      "policy 222\n",
      "necessary 222\n",
      "following 221\n",
      "front 221\n",
      "sometimes 221\n",
      "six 220\n",
      "girl 220\n",
      "clear 219\n",
      "further 218\n",
      "land 218\n",
      "provide 216\n",
      "feel 216\n",
      "party 216\n",
      "able 216\n",
      "mother 216\n",
      "music 216\n",
      "education 214\n",
      "university 214\n",
      "child 213\n",
      "effect 213\n",
      "students 213\n",
      "level 213\n",
      "run 212\n",
      "stood 212\n",
      "military 212\n",
      "town 212\n",
      "short 212\n",
      "morning 211\n",
      "total 211\n",
      "outside 210\n",
      "rate 209\n",
      "figure 209\n",
      "art 208\n",
      "century 207\n",
      "class 207\n",
      "washington 206\n",
      "4 206\n",
      "north 206\n",
      "usually 206\n",
      "plan 205\n",
      "leave 205\n",
      "therefore 205\n",
      "evidence 204\n",
      "top 204\n",
      "million 204\n",
      "sound 204\n",
      "black 203\n",
      "strong 202\n",
      "hard 202\n",
      "tax 201\n",
      "various 201\n",
      "says 200\n",
      "believe 200\n",
      "type 200\n",
      "value 200\n",
      "play 200\n",
      "surface 200\n",
      "soon 199\n",
      "mean 199\n",
      "near 198\n",
      "lines 198\n",
      "table 198\n",
      "peace 198\n",
      "modern 198\n",
      "road 197\n",
      "red 197\n",
      "book 197\n",
      "personal 196\n",
      "process 196\n",
      "situation 196\n",
      "minutes 196\n",
      "increase 195\n",
      "schools 195\n",
      "idea 195\n",
      "english 195\n",
      "alone 195\n",
      "women 195\n",
      "gone 195\n",
      "nor 195\n",
      "living 194\n",
      "america 194\n",
      "started 194\n",
      "longer 193\n",
      "dr. 192\n",
      "cut 192\n",
      "finally 191\n",
      "secretary 191\n",
      "nature 191\n",
      "private 191\n",
      "third 190\n",
      "months 189\n",
      "section 189\n",
      "greater 188\n",
      "call 188\n",
      "fire 187\n",
      "expected 187\n",
      "needed 187\n",
      "that's 187\n",
      "kept 186\n",
      "ground 186\n",
      "view 186\n",
      "values 186\n",
      "everything 185\n",
      "pressure 185\n",
      "dark 185\n",
      "basis 184\n",
      "space 184\n",
      "east 183\n",
      "father 183\n",
      "required 182\n",
      "union 182\n",
      "spirit 182\n",
      "complete 182\n",
      "except 181\n",
      "wrote 181\n",
      "i'll 181\n",
      "moved 181\n",
      "support 180\n",
      "return 180\n",
      "conditions 180\n",
      "recent 179\n",
      "attention 179\n",
      "late 179\n",
      "particular 179\n",
      "live 177\n",
      "hope 177\n",
      "costs 176\n",
      "else 176\n",
      "brown 176\n",
      "taking 175\n",
      "couldn't 175\n",
      "forces 175\n",
      "nations 175\n",
      "beyond 175\n",
      "stage 175\n",
      "read 174\n",
      "report 174\n",
      "coming 174\n",
      "hours 174\n",
      "person 174\n",
      "inside 174\n",
      "dead 174\n",
      "material 174\n",
      "instead 173\n",
      "lost 173\n",
      "heart 173\n",
      "looking 173\n",
      "low 173\n",
      "miles 173\n",
      "data 173\n",
      "added 172\n",
      "pay 172\n",
      "amount 172\n",
      "followed 172\n",
      "feeling 172\n",
      "1960 172\n",
      "single 172\n",
      "makes 172\n",
      "research 171\n",
      "including 171\n",
      "basic 171\n",
      "hundred 171\n",
      "move 171\n",
      "industry 171\n",
      "cold 171\n",
      "simply 171\n",
      "developed 170\n",
      "tried 170\n",
      "hold 169\n",
      "can't 169\n",
      "reached 169\n",
      "committee 168\n",
      "island 167\n",
      "defense 167\n",
      "equipment 167\n",
      "actually 166\n",
      "shown 166\n",
      "son 165\n",
      "central 165\n",
      "religious 165\n",
      "river 165\n",
      "getting 164\n",
      "st. 164\n",
      "beginning 164\n",
      "sort 164\n",
      "ten 164\n",
      "received 163\n",
      "& 163\n",
      "doing 163\n",
      "terms 163\n",
      "trying 163\n",
      "rest 163\n",
      "medical 162\n",
      "u.s. 162\n",
      "care 162\n",
      "especially 162\n",
      "friends 162\n",
      "picture 162\n",
      "indeed 162\n",
      "administration 161\n",
      "fine 161\n",
      "subject 161\n",
      "difficult 161\n",
      "building 160\n",
      "higher 160\n",
      "wall 160\n",
      "simple 160\n",
      "meeting 159\n",
      "walked 159\n",
      "floor 158\n",
      "foreign 158\n",
      "bring 158\n",
      "similar 157\n",
      "passed 157\n",
      "range 157\n",
      "paper 157\n",
      "property 156\n",
      "natural 156\n",
      "final 156\n",
      "training 156\n",
      "county 155\n",
      "police 155\n",
      "cent 155\n",
      "international 155\n",
      "growth 155\n",
      "market 155\n",
      "wasn't 154\n",
      "talk 154\n",
      "start 154\n",
      "written 154\n",
      "hear 153\n",
      "suddenly 153\n",
      "story 153\n",
      "issue 152\n",
      "congress 152\n",
      "needs 152\n",
      "10 152\n",
      "answer 152\n",
      "hall 152\n",
      "likely 151\n",
      "working 151\n",
      "countries 151\n",
      "considered 151\n",
      "you're 151\n",
      "earth 150\n",
      "sat 150\n",
      "purpose 149\n",
      "meet 149\n",
      "labor 149\n",
      "results 149\n",
      "entire 149\n",
      "happened 149\n",
      "william 148\n",
      "cases 148\n",
      "stand 148\n",
      "difference 148\n",
      "production 148\n",
      "hair 148\n",
      "involved 147\n",
      "fall 147\n",
      "stock 147\n",
      "food 147\n",
      "earlier 146\n",
      "increased 146\n",
      "whom 146\n",
      "particularly 146\n",
      "paid 145\n",
      "sent 145\n",
      "effort 145\n",
      "knowledge 145\n",
      "hour 145\n",
      "letter 145\n",
      "club 145\n",
      "using 145\n",
      "below 145\n",
      "thinking 145\n",
      "yes 144\n",
      "christian 144\n",
      "blue 143\n",
      "ready 143\n",
      "bill 143\n",
      "deal 143\n",
      "points 143\n",
      "trade 143\n",
      "certainly 143\n",
      "ideas 143\n",
      "industrial 143\n",
      "square 143\n",
      "boys 143\n",
      "methods 142\n",
      "addition 142\n",
      "method 142\n",
      "bad 142\n",
      "due 142\n",
      "5 142\n",
      "girls 142\n",
      "moral 142\n",
      "decided 141\n",
      "reading 141\n",
      "statement 141\n",
      "weeks 141\n",
      "neither 141\n",
      "nearly 141\n",
      "directly 141\n",
      "showed 141\n",
      "throughout 141\n",
      "according 140\n",
      "questions 140\n",
      "color 140\n",
      "kennedy 140\n",
      "anyone 140\n",
      "try 140\n",
      "services 139\n",
      "programs 139\n",
      "nation 139\n",
      "lay 139\n",
      "french 139\n",
      "size 138\n",
      "remember 138\n",
      "physical 138\n",
      "record 137\n",
      "member 137\n",
      "comes 137\n",
      "understand 137\n",
      "southern 137\n",
      "western 137\n",
      "strength 137\n",
      "population 136\n",
      "normal 136\n",
      "merely 135\n",
      "district 135\n",
      "volume 135\n",
      "concerned 135\n",
      "appeared 135\n",
      "temperature 135\n",
      "1961 134\n",
      "aid 134\n",
      "trouble 134\n",
      "trial 134\n",
      "summer 134\n",
      "direction 134\n",
      "ran 134\n",
      "sales 133\n",
      "list 133\n",
      "continued 133\n",
      "friend 133\n",
      "evening 133\n",
      "maybe 133\n",
      "literature 133\n",
      "generally 132\n",
      "association 132\n",
      "provided 132\n",
      "led 132\n",
      "army 132\n",
      "met 132\n",
      "influence 132\n",
      "opened 131\n",
      "former 131\n",
      "science 131\n",
      "student 131\n",
      "step 131\n",
      "changes 131\n",
      "chance 131\n",
      "husband 131\n",
      "hot 130\n",
      "series 130\n",
      "average 130\n",
      "works 130\n",
      "month 130\n",
      "cause 130\n",
      "effective 129\n",
      "george 129\n",
      "planning 129\n",
      "systems 129\n",
      "wouldn't 129\n",
      "direct 129\n",
      "soviet 129\n",
      "stopped 129\n",
      "wrong 129\n",
      "lead 129\n",
      "myself 129\n",
      "piece 129\n",
      "theory 129\n",
      "ask 128\n",
      "worked 128\n",
      "freedom 128\n",
      "organization 128\n",
      "clearly 128\n",
      "movement 128\n",
      "ways 128\n",
      "press 127\n",
      "somewhat 127\n",
      "spring 127\n",
      "efforts 127\n",
      "consider 127\n",
      "meaning 127\n",
      "bed 127\n",
      "fear 127\n",
      "lot 127\n",
      "treatment 127\n",
      "beautiful 127\n",
      "note 127\n",
      "forms 127\n",
      "placed 126\n",
      "hotel 126\n",
      "truth 126\n",
      "apparently 125\n",
      "degree 125\n",
      "groups 125\n",
      "he's 125\n",
      "plant 125\n",
      "carried 125\n",
      "wide 125\n",
      "i've 125\n",
      "respect 125\n",
      "man's 125\n",
      "herself 125\n",
      "numbers 125\n",
      "manner 124\n",
      "reaction 124\n",
      "easy 124\n",
      "farm 124\n",
      "immediately 123\n",
      "running 123\n",
      "approach 123\n",
      "game 123\n",
      "recently 123\n",
      "larger 123\n",
      "lower 123\n",
      "charge 122\n",
      "couple 122\n",
      "de 122\n",
      "daily 122\n",
      "eye 122\n",
      "performance 122\n",
      "feed 122\n",
      "oh 122\n",
      "march 121\n",
      "persons 121\n",
      "understanding 121\n",
      "arms 121\n",
      "opportunity 121\n",
      "c 121\n",
      "blood 121\n",
      "additional 120\n",
      "j. 120\n",
      "technical 120\n",
      "fiscal 120\n",
      "radio 120\n",
      "described 120\n",
      "stop 120\n",
      "progress 120\n",
      "steps 119\n",
      "test 119\n",
      "chief 119\n",
      "reported 119\n",
      "served 119\n",
      "based 119\n",
      "main 119\n",
      "determined 119\n",
      "image 119\n",
      "decision 119\n",
      "window 119\n",
      "religion 119\n",
      "aj 118\n",
      "gun 118\n",
      "responsibility 118\n",
      "middle 118\n",
      "europe 118\n",
      "british 118\n",
      "character 118\n",
      "learned 117\n",
      "horse 117\n",
      "writing 117\n",
      "appear 117\n",
      "s. 117\n",
      "account 117\n",
      "ones 116\n",
      "serious 116\n",
      "activity 116\n",
      "types 116\n",
      "green 116\n",
      "length 116\n",
      "lived 115\n",
      "audience 115\n",
      "letters 115\n",
      "returned 115\n",
      "obtained 115\n",
      "nuclear 115\n",
      "specific 115\n",
      "corner 115\n",
      "forward 115\n",
      "activities 115\n",
      "slowly 115\n",
      "doubt 114\n",
      "6 114\n",
      "justice 114\n",
      "moving 114\n",
      "latter 114\n",
      "gives 114\n",
      "straight 114\n",
      "hit 114\n",
      "plane 114\n",
      "quality 114\n",
      "design 114\n",
      "obviously 114\n",
      "operation 113\n",
      "plans 113\n",
      "shot 113\n",
      "seven 113\n",
      "a. 113\n",
      "choice 113\n",
      "poor 113\n",
      "staff 113\n",
      "function 113\n",
      "figures 113\n",
      "parts 113\n",
      "stay 113\n",
      "saying 113\n",
      "include 113\n",
      "15 113\n",
      "born 113\n",
      "pattern 113\n",
      "30 112\n",
      "cars 112\n",
      "whatever 112\n",
      "sun 112\n",
      "faith 111\n",
      "pool 111\n",
      "hospital 110\n",
      "corps 110\n",
      "wish 110\n",
      "lack 110\n",
      "completely 110\n",
      "heavy 110\n",
      "waiting 110\n",
      "speak 110\n",
      "ball 110\n",
      "standard 110\n",
      "extent 110\n",
      "visit 109\n",
      "democratic 109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firm 109\n",
      "income 109\n",
      "ahead 109\n",
      "deep 109\n",
      "there's 109\n",
      "language 109\n",
      "principle 109\n",
      "none 108\n",
      "price 108\n",
      "designed 108\n",
      "indicated 108\n",
      "analysis 108\n",
      "distance 108\n",
      "expect 108\n",
      "established 108\n",
      "products 108\n",
      "effects 108\n",
      "growing 108\n",
      "importance 108\n",
      "continue 107\n",
      "serve 107\n",
      "determine 107\n",
      "cities 107\n",
      "elements 107\n",
      "negro 107\n",
      "leaders 107\n",
      "division 107\n",
      "pretty 107\n",
      "easily 107\n",
      "existence 107\n",
      "attitude 107\n",
      "stress 107\n",
      "8 106\n",
      "afternoon 106\n",
      "limited 106\n",
      "hardly 106\n",
      "agreement 106\n",
      "factors 106\n",
      "scene 106\n",
      "remained 106\n",
      "closed 106\n",
      "write 106\n",
      "applied 106\n",
      "health 105\n",
      "married 105\n",
      "suggested 105\n",
      "attack 105\n",
      "rhode 105\n",
      "interested 105\n",
      "station 105\n",
      "professional 105\n",
      "won't 105\n",
      "drive 105\n",
      "season 105\n",
      "reach 105\n",
      "b 105\n",
      "despite 104\n",
      "current 104\n",
      "spent 104\n",
      "eight 104\n",
      "covered 104\n",
      "role 104\n",
      "played 104\n",
      "i'd 104\n",
      "becomes 104\n",
      "date 103\n",
      "council 103\n",
      "race 103\n",
      "unit 103\n",
      "commission 103\n",
      "original 103\n",
      "mouth 103\n",
      "reasons 103\n",
      "studies 103\n",
      "exactly 103\n",
      "machine 103\n",
      "built 103\n",
      "teeth 103\n",
      "relations 102\n",
      "rise 102\n",
      "demand 102\n",
      "prepared 102\n",
      "1959 102\n",
      "related 102\n",
      "rates 102\n",
      "news 102\n",
      "supply 102\n",
      "james 101\n",
      "director 101\n",
      "sunday 101\n",
      "bit 101\n",
      "raised 101\n",
      "events 101\n",
      "unless 101\n",
      "officer 101\n",
      "dropped 101\n",
      "playing 101\n",
      "trees 101\n",
      "standing 101\n",
      "doctor 100\n",
      "places 100\n",
      "facilities 100\n",
      "walk 100\n",
      "energy 100\n",
      "thomas 100\n",
      "talking 100\n",
      "meant 100\n",
      "clay 100\n",
      "sides 100\n",
      "gas 99\n",
      "filled 99\n",
      "techniques 99\n",
      "june 99\n",
      "knows 99\n",
      "hadn't 99\n",
      "glass 99\n",
      "jazz 99\n",
      "poet 99\n",
      "actual 99\n",
      "fight 98\n",
      "concern 98\n",
      "caught 98\n",
      "share 98\n",
      "popular 98\n",
      "mass 98\n",
      "claim 98\n",
      "entered 98\n",
      "chicago 98\n",
      "happy 98\n",
      "bridge 98\n",
      "institutions 98\n",
      "style 98\n",
      "he'd 98\n",
      "follow 97\n",
      "dollars 97\n",
      "communist 97\n",
      "status 97\n",
      "included 97\n",
      "thousand 97\n",
      "christ 97\n",
      "isn't 97\n",
      "heat 97\n",
      "radiation 97\n",
      "materials 97\n",
      "cattle 97\n",
      "suppose 97\n",
      "primary 96\n",
      "accepted 96\n",
      "books 96\n",
      "charles 96\n",
      "12 96\n",
      "sitting 96\n",
      "conference 96\n",
      "opinion 96\n",
      "usual 96\n",
      "churches 96\n",
      "film 96\n",
      "giving 96\n",
      "behavior 96\n",
      "considerable 96\n",
      "funds 95\n",
      "construction 95\n",
      "attempt 95\n",
      "changed 95\n",
      "proper 95\n",
      "successful 95\n",
      "marriage 95\n",
      "sea 95\n",
      "oil 95\n",
      "sir 95\n",
      "hell 95\n",
      "wait 94\n",
      "sign 94\n",
      "worth 94\n",
      "source 94\n",
      "highly 94\n",
      "park 94\n",
      "7 94\n",
      "discussion 94\n",
      "everyone 94\n",
      "practice 94\n",
      "arm 94\n",
      "tradition 94\n",
      "shows 94\n",
      "someone 94\n",
      "authority 93\n",
      "older 93\n",
      "annual 93\n",
      "project 93\n",
      "c. 93\n",
      "americans 93\n",
      "lord 93\n",
      "success 93\n",
      "remain 93\n",
      "principal 92\n",
      "20 92\n",
      "leadership 92\n",
      "jack 92\n",
      "obvious 92\n",
      "fell 92\n",
      "thin 92\n",
      "pieces 92\n",
      "management 91\n",
      "1958 91\n",
      "measure 91\n",
      "parents 91\n",
      "security 91\n",
      "base 91\n",
      "entirely 91\n",
      "civil 91\n",
      "frequently 91\n",
      "records 91\n",
      "structure 91\n",
      "dinner 91\n",
      "weight 91\n",
      "condition 91\n",
      "mike 91\n",
      "objective 91\n",
      "complex 91\n",
      "produced 90\n",
      "noted 90\n",
      "caused 90\n",
      "equal 90\n",
      "balance 90\n",
      "you'll 90\n",
      "purposes 90\n",
      "corporation 90\n",
      "dance 90\n",
      "kitchen 90\n",
      "failure 89\n",
      "pass 89\n",
      "goes 89\n",
      "names 89\n",
      "quickly 89\n",
      "regard 89\n",
      "published 89\n",
      "famous 89\n",
      "develop 89\n",
      "clothes 89\n",
      "laws 88\n",
      "announced 88\n",
      "carry 88\n",
      "cover 88\n",
      "moreover 88\n",
      "add 88\n",
      "greatest 88\n",
      "check 88\n",
      "enemy 88\n",
      "leaving 88\n",
      "key 88\n",
      "manager 88\n",
      "doesn't 88\n",
      "active 88\n",
      "break 88\n",
      "bottom 88\n",
      "pain 88\n",
      "relationship 88\n",
      "sources 88\n",
      "poetry 88\n",
      "assistance 87\n",
      "operating 87\n",
      "battle 87\n",
      "companies 87\n",
      "fixed 87\n",
      "possibility 87\n",
      "mary 87\n",
      "product 87\n",
      "spoke 87\n",
      "units 87\n",
      "touch 87\n",
      "bright 87\n",
      "finished 87\n",
      "carefully 87\n",
      "facts 87\n",
      "previous 86\n",
      "citizens 86\n",
      "takes 86\n",
      "e. 86\n",
      "allowed 86\n",
      "require 86\n",
      "workers 86\n",
      "build 86\n",
      "patient 86\n",
      "financial 86\n",
      "philosophy 86\n",
      "loss 86\n",
      "rose 86\n",
      "died 86\n",
      "scientific 86\n",
      "otherwise 86\n",
      "inches 86\n",
      "significant 86\n",
      "seeing 86\n",
      "distribution 85\n",
      "marked 85\n",
      "post 85\n",
      "rules 85\n",
      "capital 85\n",
      "captain 85\n",
      "relatively 85\n",
      "classes 85\n",
      "variety 85\n",
      "stated 85\n",
      "shape 85\n",
      "stations 85\n",
      "german 85\n",
      "musical 85\n",
      "concept 85\n",
      "reports 84\n",
      "proposed 84\n",
      "w. 84\n",
      "begin 84\n",
      "impossible 84\n",
      "affairs 84\n",
      "named 84\n",
      "circumstances 84\n",
      "learn 84\n",
      "remains 84\n",
      "appears 84\n",
      "strange 84\n",
      "catholic 84\n",
      "operations 84\n",
      "collection 84\n",
      "aware 84\n",
      "sex 84\n",
      "broad 84\n",
      "henry 83\n",
      "robert 83\n",
      "governor 83\n",
      "offered 83\n",
      "bank 83\n",
      "team 83\n",
      "yesterday 83\n",
      "requirements 83\n",
      "capacity 83\n",
      "speed 83\n",
      "prevent 83\n",
      "regular 83\n",
      "officers 83\n",
      "houses 83\n",
      "mark 83\n",
      "opening 83\n",
      "spread 83\n",
      "winter 83\n",
      "ship 83\n",
      "slightly 83\n",
      "remembered 83\n",
      "interests 83\n",
      "sight 83\n",
      "bar 82\n",
      "produce 82\n",
      "crisis 82\n",
      "25 82\n",
      "youth 82\n",
      "presented 82\n",
      "interesting 82\n",
      "fresh 82\n",
      "train 82\n",
      "instance 82\n",
      "drink 82\n",
      "poems 82\n",
      "agreed 81\n",
      "campaign 81\n",
      "event 81\n",
      "subjects 81\n",
      "forced 81\n",
      "nine 81\n",
      "essential 81\n",
      "immediate 81\n",
      "lives 81\n",
      "file 81\n",
      "provides 81\n",
      "watch 81\n",
      "opposite 81\n",
      "apartment 81\n",
      "created 81\n",
      "germany 81\n",
      "trip 81\n",
      "neck 81\n",
      "watched 81\n",
      "index 81\n",
      "cells 81\n",
      "session 80\n",
      "offer 80\n",
      "fully 80\n",
      "teacher 80\n",
      "recognized 80\n",
      "providence 80\n",
      "explained 80\n",
      "indicate 80\n",
      "twenty 80\n",
      "lady 80\n",
      "russian 80\n",
      "features 80\n",
      "gray 80\n",
      "term 79\n",
      "studied 79\n",
      "sam 79\n",
      "economy 79\n",
      "reduced 79\n",
      "maximum 79\n",
      "separate 79\n",
      "procedure 79\n",
      "atmosphere 79\n",
      "desire 79\n",
      "mentioned 79\n",
      "reality 79\n",
      "expression 79\n",
      "differences 79\n",
      "fair 78\n",
      "enter 78\n",
      "traditional 78\n",
      "mission 78\n",
      "favor 78\n",
      "looks 78\n",
      "secret 78\n",
      "fast 78\n",
      "picked 78\n",
      "coffee 78\n",
      "smaller 78\n",
      "edge 78\n",
      "tone 78\n",
      "beside 78\n",
      "literary 78\n",
      "- 78\n",
      "election 77\n",
      "judge 77\n",
      "title 77\n",
      "permit 77\n",
      "address 77\n",
      "rights 77\n",
      "vocational 77\n",
      "laid 77\n",
      "response 77\n",
      "believed 77\n",
      "model 77\n",
      "100 77\n",
      "solid 77\n",
      "follows 77\n",
      "editor 77\n",
      "t 77\n",
      "anode 77\n",
      "receive 76\n",
      "b. 76\n",
      "quiet 76\n",
      "telephone 76\n",
      "hearing 76\n",
      "buildings 76\n",
      "formed 76\n",
      "nice 76\n",
      "watching 76\n",
      "memory 76\n",
      "presence 76\n",
      "difficulty 76\n",
      "region 76\n",
      "knife 76\n",
      "p 76\n",
      "bottle 76\n",
      "jr. 75\n",
      "personnel 75\n",
      "fit 75\n",
      "official 75\n",
      "vote 75\n",
      "junior 75\n",
      "treated 75\n",
      "expressed 75\n",
      "planned 75\n",
      "round 75\n",
      "dog 75\n",
      "virginia 75\n",
      "killed 75\n",
      "camp 75\n",
      "stayed 75\n",
      "murder 75\n",
      "removed 75\n",
      "rock 75\n",
      "turning 75\n",
      "pointed 74\n",
      "november 74\n",
      "selected 74\n",
      "berlin 74\n",
      "claims 74\n",
      "increasing 74\n",
      "leader 74\n",
      "positive 74\n",
      "frame 74\n",
      "gain 74\n",
      "twice 74\n",
      "failed 74\n",
      "nobody 74\n",
      "send 74\n",
      "ability 74\n",
      "fourth 74\n",
      "interior 74\n",
      "jewish 74\n",
      "store 74\n",
      "faculty 74\n",
      "standards 74\n",
      "rich 74\n",
      "contrast 74\n",
      "observed 74\n",
      "nevertheless 73\n",
      "brief 73\n",
      "louis 73\n",
      "individuals 73\n",
      "rule 73\n",
      "powers 73\n",
      "advantage 73\n",
      "discovered 73\n",
      "pulled 73\n",
      "writer 73\n",
      "chapter 73\n",
      "writers 73\n",
      "brother 73\n",
      "valley 73\n",
      "membership 73\n",
      "die 73\n",
      "items 72\n",
      "daughter 72\n",
      "platform 72\n",
      "allow 72\n",
      "ordinary 72\n",
      "jones 72\n",
      "faces 72\n",
      "accept 72\n",
      "plus 72\n",
      "master 72\n",
      "legal 72\n",
      "hill 72\n",
      "fighting 72\n",
      "resources 72\n",
      "increases 72\n",
      "assumed 72\n",
      "sharp 72\n",
      "everybody 72\n",
      "broke 72\n",
      "command 72\n",
      "evil 72\n",
      "wants 72\n",
      "village 72\n",
      "phase 72\n",
      "russia 72\n",
      "detail 72\n",
      "morgan 72\n",
      "somehow 72\n",
      "fields 72\n",
      "familiar 72\n",
      "upper 72\n",
      "wine 72\n",
      "boat 72\n",
      "april 71\n",
      "unity 71\n",
      "richard 71\n",
      "responsible 71\n",
      "factor 71\n",
      "h. 71\n",
      "chosen 71\n",
      "principles 71\n",
      "constant 71\n",
      "proved 71\n",
      "carrying 71\n",
      "mercer 71\n",
      "column 71\n",
      "forth 71\n",
      "beauty 71\n",
      "compared 71\n",
      "approximately 71\n",
      "du 71\n",
      "historical 71\n",
      "smiled 71\n",
      "universe 71\n",
      "fig. 71\n",
      "calls 70\n",
      "san 70\n",
      "educational 70\n",
      "independent 70\n",
      "danger 70\n",
      "dogs 70\n",
      "waited 70\n",
      "rain 70\n",
      "song 70\n",
      "naturally 70\n",
      "box 70\n",
      "buy 70\n",
      "shelter 70\n",
      "drawn 70\n",
      "dust 70\n",
      "communism 70\n",
      "exchange 70\n",
      "sections 70\n",
      "walls 70\n",
      "foot 70\n",
      "aircraft 70\n",
      "independence 70\n",
      "revolution 70\n",
      "realize 69\n",
      "texas 69\n",
      "seek 69\n",
      "willing 69\n",
      "league 69\n",
      "teachers 69\n",
      "connection 69\n",
      "politics 69\n",
      "liberal 69\n",
      "clean 69\n",
      "completed 69\n",
      "weather 69\n",
      "fashion 69\n",
      "ordered 69\n",
      "levels 69\n",
      "sweet 69\n",
      "settled 69\n",
      "realized 69\n",
      "let's 69\n",
      "ancient 69\n",
      "china 69\n",
      "lips 69\n",
      "won 68\n",
      "policies 68\n",
      "actions 68\n",
      "monday 68\n",
      "directed 68\n",
      "leading 68\n",
      "frank 68\n",
      "statements 68\n",
      "projects 68\n",
      "starting 68\n",
      "initial 68\n",
      "application 68\n",
      "traffic 68\n",
      "stands 68\n",
      "signs 68\n",
      "families 68\n",
      "quick 68\n",
      "khrushchev 68\n",
      "largely 68\n",
      "flow 68\n",
      "drew 68\n",
      "animal 68\n",
      "beat 68\n",
      "horses 68\n",
      "characteristic 68\n",
      "excellent 68\n",
      "practical 68\n",
      "electric 68\n",
      "electronic 68\n",
      "pictures 68\n",
      "ought 68\n",
      "protection 68\n",
      "article 68\n",
      "appropriate 68\n",
      "fifty 68\n",
      "minimum 68\n",
      "dry 68\n",
      "emotional 68\n",
      "she'd 68\n",
      "jury 67\n",
      "career 67\n",
      "chairman 67\n",
      "aside 67\n",
      "asking 67\n",
      "estimated 67\n",
      "teaching 67\n",
      "reference 67\n",
      "saturday 67\n",
      "flat 67\n",
      "ends 67\n",
      "background 67\n",
      "sit 67\n",
      "dress 67\n",
      "occurred 67\n",
      "warm 67\n",
      "potential 67\n",
      "impact 67\n",
      "yourself 67\n",
      "legs 67\n",
      "you've 67\n",
      "wonder 67\n",
      "communication 67\n",
      "answered 67\n",
      "thick 67\n",
      "birth 66\n",
      "declared 66\n",
      "honor 66\n",
      "july 66\n",
      "significance 66\n",
      "score 66\n",
      "helped 66\n",
      "gross 66\n",
      "issues 66\n",
      "forest 66\n",
      "search 66\n",
      "block 66\n",
      "cutting 66\n",
      "substantial 66\n",
      "gets 66\n",
      "relief 66\n",
      "plays 66\n",
      "arts 66\n",
      "besides 66\n",
      "employees 66\n",
      "page 66\n",
      "intellectual 66\n",
      "properties 66\n",
      "experiments 66\n",
      "closely 66\n",
      "chair 66\n",
      "capable 66\n",
      "adequate 66\n",
      "measured 66\n",
      "ourselves 66\n",
      "fingers 66\n",
      "hanover 66\n",
      "attorney 65\n",
      "d. 65\n",
      "passing 65\n",
      "discussed 65\n",
      "achievement 65\n",
      "headquarters 65\n",
      "rapidly 65\n",
      "object 65\n",
      "escape 65\n",
      "jobs 65\n",
      "join 65\n",
      "phil 65\n",
      "california 65\n",
      "supposed 65\n",
      "they're 65\n",
      "typical 65\n",
      "wore 65\n",
      "cell 65\n",
      "newspaper 65\n",
      "desk 65\n",
      "one's 65\n",
      "imagination 65\n",
      "hung 65\n",
      "holding 65\n",
      "objects 65\n",
      "sleep 65\n",
      "dominant 65\n",
      "reasonable 64\n",
      "matters 64\n",
      "resolution 64\n",
      "site 64\n",
      "credit 64\n",
      "aspects 64\n",
      "message 64\n",
      "maintenance 64\n",
      "laos 64\n",
      "explain 64\n",
      "we'll 64\n",
      "located 64\n",
      "towards 64\n",
      "belief 64\n",
      "yards 64\n",
      "bodies 64\n",
      "contemporary 64\n",
      "primarily 64\n",
      "grew 64\n",
      "spiritual 64\n",
      "dream 64\n",
      "empty 64\n",
      "wind 63\n",
      "tom 63\n",
      "kill 63\n",
      "benefit 63\n",
      "signal 63\n",
      "tomorrow 63\n",
      "sufficient 63\n",
      "dramatic 63\n",
      "fellow 63\n",
      "happen 63\n",
      "no. 63\n",
      "p.m. 63\n",
      "jesus 63\n",
      "contact 63\n",
      "unusual 63\n",
      "argument 63\n",
      "powerful 63\n",
      "narrow 63\n",
      "parker 63\n",
      "shop 63\n",
      "rifle 63\n",
      "highest 63\n",
      "broken 63\n",
      "appeal 63\n",
      "competition 63\n",
      "domestic 63\n",
      "grow 63\n",
      "experiment 63\n",
      "assume 63\n",
      "relation 63\n",
      "location 63\n",
      "reduce 62\n",
      "homes 62\n",
      "portion 62\n",
      "officials 62\n",
      "m. 62\n",
      "senate 62\n",
      "fund 62\n",
      "9 62\n",
      "billion 62\n",
      "rising 62\n",
      "11 62\n",
      "speaking 62\n",
      "internal 62\n",
      "struggle 62\n",
      "agencies 62\n",
      "u. 62\n",
      "december 62\n",
      "equally 62\n",
      "sets 62\n",
      "please 62\n",
      "drove 62\n",
      "arrived 62\n",
      "save 62\n",
      "achieved 62\n",
      "assignment 62\n",
      "baby 62\n",
      "guests 62\n",
      "greatly 62\n",
      "recognize 62\n",
      "wilson 62\n",
      "library 62\n",
      "careful 62\n",
      "pleasure 62\n",
      "cool 62\n",
      "extreme 62\n",
      "concerning 62\n",
      "governments 61\n",
      "procedures 61\n",
      "prices 61\n",
      "duty 61\n",
      "courses 61\n",
      "friendly 61\n",
      "we're 61\n",
      "r. 61\n",
      "coast 61\n",
      "acting 61\n",
      "50 61\n",
      "closer 61\n",
      "speech 61\n",
      "european 61\n",
      "showing 61\n",
      "boston 61\n",
      "victory 61\n",
      "beach 61\n",
      "minister 61\n",
      "commercial 61\n",
      "metal 61\n",
      "possibly 61\n",
      "tests 61\n",
      "soft 61\n",
      "kid 61\n",
      "vast 61\n",
      "continuing 61\n",
      "associated 61\n",
      "shoulder 61\n",
      "weapons 61\n",
      "shore 61\n",
      "greek 61\n",
      "travel 61\n",
      "imagine 61\n",
      "feelings 61\n",
      "organizations 61\n",
      "ideal 61\n",
      "eat 61\n",
      "friday 60\n",
      "keeping 60\n",
      "heavily 60\n",
      "armed 60\n",
      "ended 60\n",
      "learning 60\n",
      "text 60\n",
      "existing 60\n",
      "scale 60\n",
      "setting 60\n",
      "goal 60\n",
      "task 60\n",
      "contract 60\n",
      "garden 60\n",
      "nose 60\n",
      "refused 60\n",
      "streets 60\n",
      "orchestra 60\n",
      "contained 60\n",
      "machinery 60\n",
      "chemical 60\n",
      "onto 60\n",
      "circle 60\n",
      "slow 60\n",
      "maintain 60\n",
      "fat 60\n",
      "somewhere 60\n",
      "technique 60\n",
      "stared 60\n",
      "moon 60\n",
      "notice 59\n",
      "drop 59\n",
      "budget 59\n",
      "providing 59\n",
      "f. 59\n",
      "formula 59\n",
      "housing 59\n",
      "tension 59\n",
      "advance 59\n",
      "repeated 59\n",
      "parties 59\n",
      "uses 59\n",
      "judgment 59\n",
      "taste 59\n",
      "novel 59\n",
      "headed 59\n",
      "sensitive 59\n",
      "conclusion 59\n",
      "roof 59\n",
      "solution 59\n",
      "bible 59\n",
      "lie 59\n",
      "ultimate 59\n",
      "songs 59\n",
      "struck 59\n",
      "negroes 59\n",
      "snow 59\n",
      "tree 59\n",
      "plants 59\n",
      "finds 59\n",
      "stories 59\n",
      "mine 59\n",
      "painting 59\n",
      "exist 59\n",
      "thirty 59\n",
      "sexual 59\n",
      "tuesday 58\n",
      "roads 58\n",
      "commerce 58\n",
      "p. 58\n",
      "dallas 58\n",
      "establish 58\n",
      "previously 58\n",
      "causes 58\n",
      "talked 58\n",
      "railroad 58\n",
      "critical 58\n",
      "remove 58\n",
      "emphasis 58\n",
      "grounds 58\n",
      "neighborhood 58\n",
      "surprised 58\n",
      "minor 58\n",
      "india 58\n",
      "understood 58\n",
      "perfect 58\n",
      "avoid 58\n",
      "somebody 58\n",
      "hole 58\n",
      "hence 58\n",
      "leg 58\n",
      "busy 58\n",
      "occasion 58\n",
      "smile 58\n",
      "stone 58\n",
      "roman 58\n",
      "unique 58\n",
      "animals 58\n",
      "sky 58\n",
      "safe 58\n",
      "etc. 58\n",
      "orders 58\n",
      "fairly 58\n",
      "liked 58\n",
      "useful 58\n",
      "exercise 58\n",
      "lose 58\n",
      "culture 58\n",
      "pale 58\n",
      "wondered 58\n",
      "charged 57\n",
      "details 57\n",
      "informed 57\n",
      "permitted 57\n",
      "professor 57\n",
      "replied 57\n",
      "completion 57\n",
      "processes 57\n",
      "apart 57\n",
      "apparent 57\n",
      "bay 57\n",
      "truck 57\n",
      "majority 57\n",
      "afraid 57\n",
      "artist 57\n",
      "goods 57\n",
      "birds 57\n",
      "appearance 57\n",
      "baseball 57\n",
      "spot 57\n",
      "flowers 57\n",
      "lewis 57\n",
      "notes 57\n",
      "enjoyed 57\n",
      "entrance 57\n",
      "uncle 57\n",
      "alive 57\n",
      "beneath 57\n",
      "combination 57\n",
      "truly 57\n",
      "congo 57\n",
      "becoming 57\n",
      "requires 57\n",
      "sample 57\n",
      "bear 57\n",
      "dictionary 57\n",
      "shook 57\n",
      "granted 56\n",
      "l. 56\n",
      "confidence 56\n",
      "agency 56\n",
      "joined 56\n",
      "apply 56\n",
      "vital 56\n",
      "september 56\n",
      "review 56\n",
      "wage 56\n",
      "motor 56\n",
      "fifteen 56\n",
      "regarded 56\n",
      "soldiers 56\n",
      "draw 56\n",
      "wheel 56\n",
      "organized 56\n",
      "vision 56\n",
      "wild 56\n",
      "double 56\n",
      "14 56\n",
      "palmer 56\n",
      "intensity 56\n",
      "bought 56\n",
      "represented 56\n",
      "entitled 56\n",
      "hat 56\n",
      "pure 56\n",
      "academic 56\n",
      "chinese 56\n",
      "minds 56\n",
      "guess 56\n",
      "loved 56\n",
      "spite 56\n",
      "evident 56\n",
      "executive 55\n",
      "conducted 55\n",
      "item 55\n",
      "sought 55\n",
      "18 55\n",
      "firms 55\n",
      "joe 55\n",
      "fort 55\n",
      "martin 55\n",
      "minute 55\n",
      "demands 55\n",
      "extended 55\n",
      "huge 55\n",
      "joseph 55\n",
      "cross 55\n",
      "win 55\n",
      "pick 55\n",
      "worry 55\n",
      "begins 55\n",
      "divided 55\n",
      "theme 55\n",
      "rooms 55\n",
      "device 55\n",
      "conduct 55\n",
      "runs 55\n",
      "improved 55\n",
      "games 55\n",
      "cultural 55\n",
      "plenty 55\n",
      "artists 55\n",
      "components 55\n",
      "generation 55\n",
      "motion 55\n",
      "properly 55\n",
      "identity 55\n",
      "wood 55\n",
      "tall 55\n",
      "yellow 55\n",
      "marine 55\n",
      "inner 55\n",
      "wished 55\n",
      "sounds 55\n",
      "wagon 55\n",
      "publication 55\n",
      "rural 54\n",
      "phone 54\n",
      "attend 54\n",
      "decisions 54\n",
      "unable 54\n",
      "faced 54\n",
      "republican 54\n",
      "positions 54\n",
      "risk 54\n",
      "supported 54\n",
      "symbol 54\n",
      "machines 54\n",
      "description 54\n",
      "16 54\n",
      "seat 54\n",
      "smith 54\n",
      "walking 54\n",
      "lake 54\n",
      "trained 54\n",
      "suggest 54\n",
      "create 54\n",
      "soil 54\n",
      "interpretation 54\n",
      "putting 54\n",
      "forget 54\n",
      "dear 54\n",
      "jews 54\n",
      "thoughts 54\n",
      "preparation 54\n",
      "measurements 54\n",
      "practices 53\n",
      "experienced 53\n",
      "welfare 53\n",
      "crowd 53\n",
      "largest 53\n",
      "hudson 53\n",
      "pushed 53\n",
      "payment 53\n",
      "handle 53\n",
      "absence 53\n",
      "prove 53\n",
      "bitter 53\n",
      "negative 53\n",
      "vehicles 53\n",
      "spend 53\n",
      "january 53\n",
      "remarks 53\n",
      "assigned 53\n",
      "administrative 53\n",
      "percent 53\n",
      "driving 53\n",
      "grass 53\n",
      "loose 53\n",
      "wonderful 53\n",
      "advanced 53\n",
      "august 53\n",
      "troops 53\n",
      "band 53\n",
      "chest 53\n",
      "finding 53\n",
      "slight 53\n",
      "japanese 53\n",
      "windows 53\n",
      "version 53\n",
      "breakfast 53\n",
      "what's 53\n",
      "sin 53\n",
      "examples 53\n",
      "experiences 53\n",
      "depth 53\n",
      "disease 53\n",
      "wet 53\n",
      "breath 53\n",
      "practically 53\n",
      "content 53\n",
      "establishment 52\n",
      "introduced 52\n",
      "la 52\n",
      "conflict 52\n",
      "element 52\n",
      "detailed 52\n",
      "eventually 52\n",
      "theater 52\n",
      "correct 52\n",
      "widely 52\n",
      "hero 52\n",
      "trust 52\n",
      "raise 52\n",
      "developing 52\n",
      "advice 52\n",
      "centers 52\n",
      "gold 52\n",
      "dozen 52\n",
      "telling 52\n",
      "alfred 52\n",
      "bedroom 52\n",
      "detective 52\n",
      "colors 52\n",
      "indian 52\n",
      "u.n. 52\n",
      "silence 52\n",
      "contrary 52\n",
      "characteristics 52\n",
      "flesh 52\n",
      "investigation 51\n",
      "achieve 51\n",
      "approval 51\n",
      "estate 51\n",
      "elections 51\n",
      "supreme 51\n",
      "listen 51\n",
      "conventional 51\n",
      "gradually 51\n",
      "david 51\n",
      "views 51\n",
      "foods 51\n",
      "pull 51\n",
      "october 51\n",
      "arthur 51\n",
      "stream 51\n",
      "warren 51\n",
      "los 51\n",
      "surprise 51\n",
      "stages 51\n",
      "player 51\n",
      "guy 51\n",
      "agree 51\n",
      "uniform 51\n",
      "abroad 51\n",
      "devoted 51\n",
      "papers 51\n",
      "rear 51\n",
      "cousin 51\n",
      "situations 51\n",
      "boats 51\n",
      "ages 51\n",
      "begun 51\n",
      "easier 51\n",
      "shoulders 51\n",
      "sick 51\n",
      "nodded 51\n",
      "opportunities 51\n",
      "necessarily 51\n",
      "angle 51\n",
      "throat 51\n",
      "protestant 51\n",
      "waves 51\n",
      "laughed 51\n",
      "efficiency 50\n",
      "automobile 50\n",
      "mention 50\n",
      "courts 50\n",
      "issued 50\n",
      "expense 50\n",
      "extremely 50\n",
      "fill 50\n",
      "massachusetts 50\n",
      "institute 50\n",
      "television 50\n",
      "choose 50\n",
      "assembly 50\n",
      "chain 50\n",
      "latin 50\n",
      "eisenhower 50\n",
      "d 50\n",
      "knowing 50\n",
      "proud 50\n",
      "wooden 50\n",
      "worse 50\n",
      "advertising 50\n",
      "extra 50\n",
      "philadelphia 50\n",
      "pair 50\n",
      "brilliant 50\n",
      "conversation 50\n",
      "taught 50\n",
      "welcome 50\n",
      "hills 50\n",
      "conviction 50\n",
      "female 50\n",
      "strike 50\n",
      "engine 50\n",
      "moments 50\n",
      "fundamental 50\n",
      "tiny 50\n",
      "desired 50\n",
      "convinced 50\n",
      "noticed 50\n",
      "towns 50\n",
      "motors 50\n",
      "childhood 50\n",
      "employed 49\n",
      "13 49\n",
      "speaker 49\n",
      "constitution 49\n",
      "passage 49\n",
      "millions 49\n",
      "request 49\n",
      "firmly 49\n",
      "count 49\n",
      "hopes 49\n",
      "tendency 49\n",
      "acceptance 49\n",
      "driver 49\n",
      "depends 49\n",
      "ride 49\n",
      "impressive 49\n",
      "sports 49\n",
      "milk 49\n",
      "quietly 49\n",
      "holy 49\n",
      "tragedy 49\n",
      "burning 49\n",
      "incident 49\n",
      "operator 49\n",
      "payments 49\n",
      "creative 49\n",
      "silent 49\n",
      "measures 49\n",
      "consideration 49\n",
      "till 49\n",
      "leaves 49\n",
      "partly 49\n",
      "grand 48\n",
      "suit 48\n",
      "destroy 48\n",
      "24 48\n",
      "co. 48\n",
      "hoped 48\n",
      "royal 48\n",
      "limit 48\n",
      "operate 48\n",
      "twelve 48\n",
      "guard 48\n",
      "integration 48\n",
      "tired 48\n",
      "1957 48\n",
      "screen 48\n",
      "mantle 48\n",
      "charlie 48\n",
      "shooting 48\n",
      "she's 48\n",
      "cry 48\n",
      "via 48\n",
      "pink 48\n",
      "mile 48\n",
      "missile 48\n",
      "functions 48\n",
      "formal 48\n",
      "occasionally 48\n",
      "rolled 48\n",
      "comparison 48\n",
      "resistance 48\n",
      "personality 48\n",
      "concrete 48\n",
      "precisely 48\n",
      "plain 48\n",
      "swung 48\n",
      "sorry 48\n",
      "maintained 48\n",
      "drinking 48\n",
      "intelligence 48\n",
      "anger 48\n",
      "poem 48\n",
      "attitudes 48\n",
      "liquid 48\n",
      "hearst 48\n",
      "considering 47\n",
      "bonds 47\n",
      "denied 47\n",
      "bills 47\n",
      "employment 47\n",
      "cook 47\n",
      "grant 47\n",
      "fears 47\n",
      "21 47\n",
      "cuba 47\n",
      "sold 47\n",
      "thousands 47\n",
      "manufacturers 47\n",
      "engaged 47\n",
      "provision 47\n",
      "purchase 47\n",
      "safety 47\n",
      "honest 47\n",
      "representative 47\n",
      "deny 47\n",
      "northern 47\n",
      "moscow 47\n",
      "expenses 47\n",
      "expansion 47\n",
      "testimony 47\n",
      "angeles 47\n",
      "prior 47\n",
      "blind 47\n",
      "luck 47\n",
      "lights 47\n",
      "remarkable 47\n",
      "surely 47\n",
      "23 47\n",
      "humor 47\n",
      "opera 47\n",
      "italian 47\n",
      "singing 47\n",
      "mail 47\n",
      "everywhere 47\n",
      "vacation 47\n",
      "models 47\n",
      "boards 47\n",
      "supplies 47\n",
      "stairs 47\n",
      "ring 47\n",
      "concentration 47\n",
      "congregation 47\n",
      "unknown 47\n",
      "movements 47\n",
      "wearing 47\n",
      "aspect 47\n",
      "numerous 47\n",
      "instrument 47\n",
      "mere 47\n",
      "essentially 47\n",
      "soul 47\n",
      "periods 47\n",
      "patterns 47\n",
      "lincoln 47\n",
      "skin 47\n",
      "superior 46\n",
      "relative 46\n",
      "recommended 46\n",
      "legislation 46\n",
      "georgia 46\n",
      "bond 46\n",
      "violence 46\n",
      "insurance 46\n",
      "opposition 46\n",
      "creation 46\n",
      "22 46\n",
      "loan 46\n",
      "dollar 46\n",
      "difficulties 46\n",
      "atomic 46\n",
      "encourage 46\n",
      "losses 46\n",
      "trend 46\n",
      "60 46\n",
      "weakness 46\n",
      "wave 46\n",
      "identified 46\n",
      "1954 46\n",
      "native 46\n",
      "avenue 46\n",
      "decade 46\n",
      "curious 46\n",
      "anyway 46\n",
      "engineering 46\n",
      "threw 46\n",
      "flight 46\n",
      "dangerous 46\n",
      "award 46\n",
      "ain't 46\n",
      "wright 46\n",
      "panels 46\n",
      "seriously 46\n",
      "liberty 46\n",
      "shares 46\n",
      "percentage 46\n",
      "conscious 46\n",
      "salt 46\n",
      "author 46\n",
      "chamber 46\n",
      "centuries 46\n",
      "equivalent 46\n",
      "electrical 46\n",
      "fought 46\n",
      "pocket 46\n",
      "fiction 46\n",
      "doctrine 46\n",
      "precision 46\n",
      "artery 46\n",
      "shut 46\n",
      "q 46\n",
      "offices 45\n",
      "promised 45\n",
      "promise 45\n",
      "residential 45\n",
      "adopted 45\n",
      "taxes 45\n",
      "load 45\n",
      "depend 45\n",
      "sum 45\n",
      "africa 45\n",
      "sheet 45\n",
      "impression 45\n",
      "feels 45\n",
      "referred 45\n",
      "edward 45\n",
      "calling 45\n",
      "pennsylvania 45\n",
      "valuable 45\n",
      "alexander 45\n",
      "steel 45\n",
      "charges 45\n",
      "containing 45\n",
      "target 45\n",
      "includes 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nearby 45\n",
      "interference 45\n",
      "mounted 45\n",
      "cup 45\n",
      "intended 45\n",
      "brain 45\n",
      "qualities 45\n",
      "offers 45\n",
      "february 45\n",
      "riding 45\n",
      "lucy 45\n",
      "contain 45\n",
      "expenditures 45\n",
      "meat 45\n",
      "watson 45\n",
      "elsewhere 45\n",
      "prime 45\n",
      "ballet 45\n",
      "cast 45\n",
      "approached 45\n",
      "angry 45\n",
      "universal 45\n",
      "terrible 45\n",
      "medium 45\n",
      "diameter 45\n",
      "discovery 45\n",
      "ice 45\n",
      "curve 45\n",
      "mold 45\n",
      "burden 44\n",
      "listed 44\n",
      "warning 44\n",
      "considerably 44\n",
      "mostly 44\n",
      "amounts 44\n",
      "admitted 44\n",
      "errors 44\n",
      "wisdom 44\n",
      "opinions 44\n",
      "asia 44\n",
      "continuous 44\n",
      "seeking 44\n",
      "origin 44\n",
      "acres 44\n",
      "changing 44\n",
      "confusion 44\n",
      "hundreds 44\n",
      "developments 44\n",
      "enjoy 44\n",
      "fired 44\n",
      "younger 44\n",
      "helping 44\n",
      "pounds 44\n",
      "accomplished 44\n",
      "lies 44\n",
      "suffering 44\n",
      "lovely 44\n",
      "snake 44\n",
      "fun 44\n",
      "sale 44\n",
      "driven 44\n",
      "spirits 44\n",
      "agent 44\n",
      "collected 44\n",
      "extensive 44\n",
      "path 44\n",
      "climbed 44\n",
      "pilot 44\n",
      "shoes 44\n",
      "mobile 44\n",
      "tables 44\n",
      "expensive 44\n",
      "adam 44\n",
      "arranged 44\n",
      "volumes 44\n",
      "answers 44\n",
      "confused 44\n",
      "contribute 44\n",
      "recognition 44\n",
      "brush 44\n",
      "manchester 44\n",
      "odd 44\n",
      "hans 44\n",
      "slaves 44\n",
      "washing 44\n",
      "oxygen 44\n",
      "thickness 44\n",
      "mama 44\n",
      "believes 43\n",
      "mental 43\n",
      "liquor 43\n",
      "republic 43\n",
      "lawyer 43\n",
      "year's 43\n",
      "insisted 43\n",
      "technology 43\n",
      "bureau 43\n",
      "route 43\n",
      "explanation 43\n",
      "core 43\n",
      "dealing 43\n",
      "rapid 43\n",
      "salary 43\n",
      "saved 43\n",
      "transportation 43\n",
      "reader 43\n",
      "external 43\n",
      "pace 43\n",
      "recorded 43\n",
      "iron 43\n",
      "flying 43\n",
      "dirt 43\n",
      "switch 43\n",
      "concerns 43\n",
      "separated 43\n",
      "tour 43\n",
      "dancing 43\n",
      "comfort 43\n",
      "consists 43\n",
      "warfare 43\n",
      "ships 43\n",
      "investment 43\n",
      "coat 43\n",
      "raw 43\n",
      "occur 43\n",
      "reaching 43\n",
      "grown 43\n",
      "marketing 43\n",
      "resulting 43\n",
      "tend 43\n",
      "drama 43\n",
      "heads 43\n",
      "identification 43\n",
      "i.e. 43\n",
      "lifted 43\n",
      "catch 43\n",
      "mountains 43\n",
      "recreation 43\n",
      "heaven 43\n",
      "readily 43\n",
      "porch 43\n",
      "cloth 43\n",
      "darkness 43\n",
      "whenever 43\n",
      "emotions 43\n",
      "environment 43\n",
      "appointed 42\n",
      "prison 42\n",
      "obtain 42\n",
      "urban 42\n",
      "smooth 42\n",
      "holds 42\n",
      "excess 42\n",
      "waters 42\n",
      "reply 42\n",
      "unlike 42\n",
      "reduction 42\n",
      "comment 42\n",
      "g. 42\n",
      "replaced 42\n",
      "nineteenth 42\n",
      "ease 42\n",
      "throw 42\n",
      "suffered 42\n",
      "threat 42\n",
      "demanded 42\n",
      "lots 42\n",
      "crossed 42\n",
      "wire 42\n",
      "muscle 42\n",
      "anybody 42\n",
      "golden 42\n",
      "hardy 42\n",
      "anne 42\n",
      "wages 42\n",
      "hate 42\n",
      "increasingly 42\n",
      "bag 42\n",
      "bound 42\n",
      "express 42\n",
      "regional 42\n",
      "pride 42\n",
      "engineer 42\n",
      "adams 42\n",
      "sufficiently 42\n",
      "distinguished 42\n",
      "reflected 42\n",
      "reactions 42\n",
      "varying 42\n",
      "varied 42\n",
      "weapon 42\n",
      "journal 42\n",
      "touched 42\n",
      "guns 42\n",
      "long-range 42\n",
      "exists 42\n",
      "editorial 42\n",
      "seeds 42\n",
      "possibilities 42\n",
      "civilization 42\n",
      "distinct 42\n",
      "particles 42\n",
      "skill 42\n",
      "rachel 42\n",
      "cooling 42\n",
      "anxiety 42\n",
      "linda 42\n",
      "opposed 41\n",
      "17 41\n",
      "proposal 41\n",
      "storage 41\n",
      "representatives 41\n",
      "teach 41\n",
      "societies 41\n",
      "constantly 41\n",
      "removal 41\n",
      "communities 41\n",
      "vice 41\n",
      "sell 41\n",
      "visited 41\n",
      "writes 41\n",
      "rough 41\n",
      "steady 41\n",
      "spending 41\n",
      "distinction 41\n",
      "francisco 41\n",
      "o'clock 41\n",
      "carl 41\n",
      "arc 41\n",
      "comparable 41\n",
      "'em 41\n",
      "rare 41\n",
      "continues 41\n",
      "favorite 41\n",
      "sake 41\n",
      "display 41\n",
      "brothers 41\n",
      "downtown 41\n",
      "restaurant 41\n",
      "pleased 41\n",
      "institution 41\n",
      "today's 41\n",
      "assumption 41\n",
      "seed 41\n",
      "bread 41\n",
      "match 41\n",
      "musicians 41\n",
      "remaining 41\n",
      "pike 41\n",
      "shift 41\n",
      "participation 41\n",
      "virtually 41\n",
      "limits 41\n",
      "psychological 41\n",
      "funny 41\n",
      "fed 41\n",
      "smoke 41\n",
      "involves 41\n",
      "rarely 41\n",
      "whereas 41\n",
      "describe 41\n",
      "tissue 41\n",
      "henrietta 41\n",
      "kate 41\n",
      "combined 40\n",
      "exception 40\n",
      "regarding 40\n",
      "highway 40\n",
      "approved 40\n",
      "personally 40\n",
      "customers 40\n",
      "composed 40\n",
      "senator 40\n",
      "legislative 40\n",
      "dependent 40\n",
      "afford 40\n",
      "atlantic 40\n",
      "dean 40\n",
      "neighbors 40\n",
      "happens 40\n",
      "walter 40\n",
      "democrats 40\n",
      "languages 40\n",
      "goals 40\n",
      "orleans 40\n",
      "40 40\n",
      "decide 40\n",
      "notion 40\n",
      "laboratory 40\n",
      "a.m. 40\n",
      "illinois 40\n",
      "proof 40\n",
      "existed 40\n",
      "bob 40\n",
      "grace 40\n",
      "missed 40\n",
      "prominent 40\n",
      "thoroughly 40\n",
      "shared 40\n",
      "talent 40\n",
      "studying 40\n",
      "handsome 40\n",
      "automatic 40\n",
      "burned 40\n",
      "permanent 40\n",
      "observations 40\n",
      "drawing 40\n",
      "winston 40\n",
      "desegregation 40\n",
      "guidance 40\n",
      "improvement 40\n",
      "treasury 40\n",
      "presumably 40\n",
      "bars 40\n",
      "brings 40\n",
      "papa 40\n",
      "indicates 40\n",
      "discover 40\n",
      "painted 40\n",
      "intense 40\n",
      "tool 40\n",
      "necessity 40\n",
      "eleven 40\n",
      "shouted 40\n",
      "focus 40\n",
      "stepped 40\n",
      "finger 40\n",
      "conscience 40\n",
      "criticism 40\n",
      "thrown 40\n",
      "glance 40\n",
      "regions 40\n",
      "stranger 40\n",
      "joy 40\n",
      "pope 40\n",
      "e 40\n",
      "atoms 40\n",
      "visual 40\n",
      "parallel 40\n",
      "shear 40\n",
      "rode 40\n",
      "legislature 39\n",
      "authorities 39\n",
      "estimate 39\n",
      "lawrence 39\n",
      "acts 39\n",
      "improve 39\n",
      "rayburn 39\n",
      "cooperation 39\n",
      "communists 39\n",
      "neutral 39\n",
      "determination 39\n",
      "deeply 39\n",
      "assured 39\n",
      "attractive 39\n",
      "transfer 39\n",
      "represents 39\n",
      "colleges 39\n",
      "joint 39\n",
      "mississippi 39\n",
      "severe 39\n",
      "introduction 39\n",
      "emergency 39\n",
      "striking 39\n",
      "trials 39\n",
      "self 39\n",
      "gained 39\n",
      "contributed 39\n",
      "mad 39\n",
      "magazine 39\n",
      "forever 39\n",
      "mystery 39\n",
      "tv 39\n",
      "selection 39\n",
      "code 39\n",
      "anywhere 39\n",
      "furniture 39\n",
      "agents 39\n",
      "derived 39\n",
      "revealed 39\n",
      "provisions 39\n",
      "guest 39\n",
      "allotment 39\n",
      "satisfactory 39\n",
      "controlled 39\n",
      "finish 39\n",
      "maturity 39\n",
      "concert 39\n",
      "comedy 39\n",
      "stick 39\n",
      "sleeping 39\n",
      "listening 39\n",
      "soldier 39\n",
      "holes 39\n",
      "recall 39\n",
      "mankind 39\n",
      "destroyed 39\n",
      "hydrogen 39\n",
      "furthermore 39\n",
      "objectives 39\n",
      "inch 39\n",
      "defined 39\n",
      "handling 38\n",
      "mayor 38\n",
      "candidates 38\n",
      "specifically 38\n",
      "scheduled 38\n",
      "31 38\n",
      "accounts 38\n",
      "districts 38\n",
      "serving 38\n",
      "leaned 38\n",
      "200 38\n",
      "experimental 38\n",
      "tonight 38\n",
      "track 38\n",
      "r 38\n",
      "simultaneously 38\n",
      "state's 38\n",
      "handed 38\n",
      "copy 38\n",
      "glad 38\n",
      "thompson 38\n",
      "paul 38\n",
      "newspapers 38\n",
      "sharply 38\n",
      "experts 38\n",
      "reception 38\n",
      "temple 38\n",
      "fifth 38\n",
      "robinson 38\n",
      "ohio 38\n",
      "cotton 38\n",
      "attempts 38\n",
      "sudden 38\n",
      "bringing 38\n",
      "sister 38\n",
      "foundation 38\n",
      "ears 38\n",
      "japan 38\n",
      "palace 38\n",
      "arrangements 38\n",
      "corresponding 38\n",
      "consumer 38\n",
      "definition 38\n",
      "processing 38\n",
      "turns 38\n",
      "father's 38\n",
      "random 38\n",
      "piano 38\n",
      "relationships 38\n",
      "knees 38\n",
      "briefly 38\n",
      "pressures 38\n",
      "represent 38\n",
      "agricultural 38\n",
      "instant 38\n",
      "pleasant 38\n",
      "inevitably 38\n",
      "regardless 38\n",
      "god's 38\n",
      "voices 38\n",
      "thyroid 38\n",
      "destruction 38\n",
      "sacred 38\n",
      "clouds 38\n",
      "forgotten 38\n",
      "contains 38\n",
      "primitive 38\n",
      "organic 38\n",
      "haven't 38\n",
      "axis 38\n",
      "onset 38\n",
      "n 38\n",
      "thanks 37\n",
      "banks 37\n",
      "roberts 37\n",
      "effectively 37\n",
      "skills 37\n",
      "strongly 37\n",
      "nation's 37\n",
      "mood 37\n",
      "tremendous 37\n",
      "deeper 37\n",
      "assure 37\n",
      "authorized 37\n",
      "fail 37\n",
      "definite 37\n",
      "navy 37\n",
      "reserve 37\n",
      "edges 37\n",
      "feature 37\n",
      "stronger 37\n",
      "signed 37\n",
      "delivered 37\n",
      "resulted 37\n",
      "roy 37\n",
      "outstanding 37\n",
      "formation 37\n",
      "illustrated 37\n",
      "contribution 37\n",
      "push 37\n",
      "supper 37\n",
      "gate 37\n",
      "magic 37\n",
      "swimming 37\n",
      "susan 37\n",
      "chose 37\n",
      "harbor 37\n",
      "innocent 37\n",
      "atom 37\n",
      "release 37\n",
      "spoken 37\n",
      "plot 37\n",
      "survey 37\n",
      "profession 37\n",
      "male 37\n",
      "cleaning 37\n",
      "accompanied 37\n",
      "belong 37\n",
      "colonel 37\n",
      "serves 37\n",
      "chicken 37\n",
      "fool 37\n",
      "edition 37\n",
      "noise 37\n",
      "drunk 37\n",
      "hurt 37\n",
      "illusion 37\n",
      "occasional 37\n",
      "comfortable 37\n",
      "enormous 37\n",
      "admit 37\n",
      "holmes 37\n",
      "stomach 37\n",
      "readers 37\n",
      "distant 37\n",
      "aim 37\n",
      "paint 37\n",
      "foam 37\n",
      "constructed 37\n",
      "blocks 37\n",
      "devices 37\n",
      "tested 37\n",
      "mixed 37\n",
      "species 37\n",
      "images 37\n",
      "questionnaire 37\n",
      "staining 37\n",
      "attended 36\n",
      "assistant 36\n",
      "jackson 36\n",
      "automatically 36\n",
      "license 36\n",
      "printed 36\n",
      "wise 36\n",
      "football 36\n",
      "extension 36\n",
      "visiting 36\n",
      "scholarship 36\n",
      "moves 36\n",
      "affected 36\n",
      "intention 36\n",
      "challenge 36\n",
      "sees 36\n",
      "jim 36\n",
      "filling 36\n",
      "guide 36\n",
      "normally 36\n",
      "cash 36\n",
      "industries 36\n",
      "schedule 36\n",
      "bomb 36\n",
      "multiple 36\n",
      "lying 36\n",
      "satisfied 36\n",
      "doors 36\n",
      "peoples 36\n",
      "error 36\n",
      "tough 36\n",
      "maris 36\n",
      "cards 36\n",
      "thank 36\n",
      "peter 36\n",
      "baker 36\n",
      "fellowship 36\n",
      "paintings 36\n",
      "mother's 36\n",
      "supplied 36\n",
      "camera 36\n",
      "sympathy 36\n",
      "f 36\n",
      "crew 36\n",
      "equipped 36\n",
      "managed 36\n",
      "kinds 36\n",
      "occupied 36\n",
      "outlook 36\n",
      "aren't 36\n",
      "classic 36\n",
      "characters 36\n",
      "substantially 36\n",
      "worship 36\n",
      "visitors 36\n",
      "desirable 36\n",
      "conclusions 36\n",
      "you'd 36\n",
      "patients 36\n",
      "hurry 36\n",
      "spanish 36\n",
      "shadow 36\n",
      "stored 36\n",
      "beings 36\n",
      "columns 36\n",
      "scientists 36\n",
      "dressed 36\n",
      "similarly 36\n",
      "host 36\n",
      "accuracy 36\n",
      "variable 36\n",
      "smiling 36\n",
      "symbols 36\n",
      "forty 36\n",
      "ratio 36\n",
      "coating 36\n",
      "dirty 36\n",
      "binomial 36\n",
      "over-all 35\n",
      "atlanta 35\n",
      "urged 35\n",
      "counties 35\n",
      "1962 35\n",
      "wednesday 35\n",
      "meanwhile 35\n",
      "harry 35\n",
      "revenue 35\n",
      "sounded 35\n",
      "clark 35\n",
      "bench 35\n",
      "latest 35\n",
      "nationalism 35\n",
      "crime 35\n",
      "vehicle 35\n",
      "stores 35\n",
      "retired 35\n",
      "lumber 35\n",
      "preserve 35\n",
      "sympathetic 35\n",
      "returning 35\n",
      "virgin 35\n",
      "owners 35\n",
      "row 35\n",
      "meets 35\n",
      "performed 35\n",
      "knee 35\n",
      "yard 35\n",
      "claimed 35\n",
      "world's 35\n",
      "pat 35\n",
      "jumped 35\n",
      "beer 35\n",
      "jane 35\n",
      "wear 35\n",
      "bombs 35\n",
      "stanley 35\n",
      "affect 35\n",
      "grade 35\n",
      "tape 35\n",
      "eggs 35\n",
      "fruit 35\n",
      "sciences 35\n",
      "installed 35\n",
      "yield 35\n",
      "presently 35\n",
      "probability 35\n",
      "routine 35\n",
      "output 35\n",
      "adjustment 35\n",
      "dignity 35\n",
      "height 35\n",
      "calm 35\n",
      "sad 35\n",
      "isolated 35\n",
      "washed 35\n",
      "consequences 35\n",
      "accurate 35\n",
      "producing 35\n",
      "prepare 35\n",
      "instructions 35\n",
      "phenomenon 35\n",
      "tongue 35\n",
      "waste 35\n",
      "pont 35\n",
      "symbolic 35\n",
      "disappeared 35\n",
      "calculated 35\n",
      "fish 35\n",
      "context 35\n",
      "myth 35\n",
      "worried 35\n",
      "patent 35\n",
      "sequence 35\n",
      "matsuo 35\n",
      "protect 34\n",
      "candidate 34\n",
      "sept. 34\n",
      "alternative 34\n",
      "shortly 34\n",
      "smell 34\n",
      "dispute 34\n",
      "sending 34\n",
      "senior 34\n",
      "receiving 34\n",
      "tied 34\n",
      "presidential 34\n",
      "genuine 34\n",
      "facing 34\n",
      "canada 34\n",
      "raising 34\n",
      "harvard 34\n",
      "exposed 34\n",
      "clerk 34\n",
      "suggestion 34\n",
      "blame 34\n",
      "financing 34\n",
      "bigger 34\n",
      "reporters 34\n",
      "johnson 34\n",
      "badly 34\n",
      "currently 34\n",
      "samuel 34\n",
      "sentence 34\n",
      "lee 34\n",
      "realistic 34\n",
      "net 34\n",
      "golf 34\n",
      "we've 34\n",
      "arrangement 34\n",
      "logical 34\n",
      "owned 34\n",
      "metropolitan 34\n",
      "thereby 34\n",
      "worst 34\n",
      "bus 34\n",
      "folk 34\n",
      "sing 34\n",
      "roles 34\n",
      "tells 34\n",
      "crazy 34\n",
      "sugar 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duties 34\n",
      "decades 34\n",
      "vary 34\n",
      "roll 34\n",
      "visible 34\n",
      "emotion 34\n",
      "seldom 34\n",
      "swept 34\n",
      "suitable 34\n",
      "hunting 34\n",
      "lists 34\n",
      "corn 34\n",
      "mechanical 34\n",
      "quarter 34\n",
      "mistake 34\n",
      "returns 34\n",
      "frequent 34\n",
      "ocean 34\n",
      "phrase 34\n",
      "fallen 34\n",
      "tears 34\n",
      "dying 34\n",
      "openly 34\n",
      "bent 34\n",
      "tools 34\n",
      "tends 34\n",
      "reasonably 34\n",
      "findings 34\n",
      "divine 34\n",
      "stretched 34\n",
      "abstract 34\n",
      "keys 34\n",
      "measurement 34\n",
      "pencil 34\n",
      "elected 33\n",
      "filed 33\n",
      "succeeded 33\n",
      "rejected 33\n",
      "thursday 33\n",
      "missing 33\n",
      "gift 33\n",
      "favorable 33\n",
      "guilt 33\n",
      "involving 33\n",
      "benefits 33\n",
      "matching 33\n",
      "fate 33\n",
      "affair 33\n",
      "fewer 33\n",
      "naval 33\n",
      "prince 33\n",
      "stems 33\n",
      "examine 33\n",
      "advised 33\n",
      "charter 33\n",
      "presentation 33\n",
      "campus 33\n",
      "interview 33\n",
      "owner 33\n",
      "classical 33\n",
      "branches 33\n",
      "admission 33\n",
      "harmony 33\n",
      "determining 33\n",
      "accident 33\n",
      "strictly 33\n",
      "blow 33\n",
      "andy 33\n",
      "unfortunately 33\n",
      "damage 33\n",
      "rice 33\n",
      "performances 33\n",
      "drill 33\n",
      "leads 33\n",
      "fly 33\n",
      "branch 33\n",
      "lunch 33\n",
      "bride 33\n",
      "artistic 33\n",
      "nights 33\n",
      "presents 33\n",
      "jacket 33\n",
      "attempted 33\n",
      "parked 33\n",
      "survive 33\n",
      "funeral 33\n",
      "alert 33\n",
      "massive 33\n",
      "violent 33\n",
      "engineers 33\n",
      "burst 33\n",
      "farmers 33\n",
      "adjusted 33\n",
      "symphony 33\n",
      "substance 33\n",
      "child's 33\n",
      "wash 33\n",
      "precise 33\n",
      "inevitable 33\n",
      "grave 33\n",
      "demonstrated 33\n",
      "equation 33\n",
      "scheme 33\n",
      "catholics 33\n",
      "namely 33\n",
      "swift 33\n",
      "connected 33\n",
      "suffer 33\n",
      "tragic 33\n",
      "falling 33\n",
      "rector 33\n",
      "poland 33\n",
      "quantity 33\n",
      "bone 33\n",
      "prokofieff 33\n",
      "healthy 33\n",
      "mountain 33\n",
      "slavery 33\n",
      "chlorine 33\n",
      "thermal 33\n",
      "pathology 33\n",
      "jess 33\n",
      "inadequate 32\n",
      "lacking 32\n",
      "elaborate 32\n",
      "williams 32\n",
      "howard 32\n",
      "debate 32\n",
      "shouting 32\n",
      "so-called 32\n",
      "discussions 32\n",
      "spots 32\n",
      "castro 32\n",
      "gesture 32\n",
      "concluded 32\n",
      "falls 32\n",
      "factory 32\n",
      "awareness 32\n",
      "partner 32\n",
      "long-term 32\n",
      "loans 32\n",
      "universities 32\n",
      "remarked 32\n",
      "transition 32\n",
      "effectiveness 32\n",
      "depending 32\n",
      "covering 32\n",
      "harold 32\n",
      "temporary 32\n",
      "we'd 32\n",
      "mills 32\n",
      "26 32\n",
      "kids 32\n",
      "mud 32\n",
      "indians 32\n",
      "van 32\n",
      "19 32\n",
      "romantic 32\n",
      "wedding 32\n",
      "they'll 32\n",
      "eastern 32\n",
      "salvation 32\n",
      "covers 32\n",
      "excitement 32\n",
      "household 32\n",
      "promote 32\n",
      "collective 32\n",
      "efficient 32\n",
      "missiles 32\n",
      "survival 32\n",
      "fishing 32\n",
      "museum 32\n",
      "variation 32\n",
      "chandler 32\n",
      "stuff 32\n",
      "poets 32\n",
      "gathered 32\n",
      "remote 32\n",
      "confronted 32\n",
      "testing 32\n",
      "initiative 32\n",
      "e.g. 32\n",
      "eating 32\n",
      "coal 32\n",
      "cooking 32\n",
      "slipped 32\n",
      "weak 32\n",
      "courage 32\n",
      "reflection 32\n",
      "circles 32\n",
      "conception 32\n",
      "gardens 32\n",
      "crowded 32\n",
      "naked 32\n",
      "farther 32\n",
      "plastic 32\n",
      "electronics 32\n",
      "gorton 32\n",
      "skywave 32\n",
      "emission 32\n",
      "**zg 32\n",
      "scotty 32\n",
      "damn 32\n",
      "curt 32\n",
      "protected 31\n",
      "starts 31\n",
      "tossed 31\n",
      "conservative 31\n",
      "v. 31\n",
      "reducing 31\n",
      "ruled 31\n",
      "finance 31\n",
      "allowing 31\n",
      "mainly 31\n",
      "territory 31\n",
      "extraordinary 31\n",
      "enterprise 31\n",
      "remark 31\n",
      "panel 31\n",
      "consequently 31\n",
      "defeat 31\n",
      "involve 31\n",
      "kansas 31\n",
      "knocked 31\n",
      "identical 31\n",
      "mature 31\n",
      "winning 31\n",
      "checked 31\n",
      "bird 31\n",
      "seventh 31\n",
      "dave 31\n",
      "barely 31\n",
      "helps 31\n",
      "movies 31\n",
      "dancer 31\n",
      "heading 31\n",
      "pacific 31\n",
      "secondary 31\n",
      "strain 31\n",
      "fourteen 31\n",
      "ending 31\n",
      "letting 31\n",
      "successfully 31\n",
      "dancers 31\n",
      "fallout 31\n",
      "studio 31\n",
      "juniors 31\n",
      "maid 31\n",
      "decline 31\n",
      "recording 31\n",
      "parking 31\n",
      "structures 31\n",
      "selling 31\n",
      "colored 31\n",
      "competitive 31\n",
      "lightly 31\n",
      "trail 31\n",
      "tube 31\n",
      "beef 31\n",
      "christianity 31\n",
      "poetic 31\n",
      "films 31\n",
      "gallery 31\n",
      "russians 31\n",
      "troubled 31\n",
      "muscles 31\n",
      "extend 31\n",
      "1952 31\n",
      "outer 31\n",
      "markets 31\n",
      "density 31\n",
      "respectively 31\n",
      "softly 31\n",
      "shock 31\n",
      "horn 31\n",
      "invariably 31\n",
      "ceiling 31\n",
      "articles 31\n",
      "considerations 31\n",
      "perfectly 31\n",
      "counter 31\n",
      "pages 31\n",
      "composer 31\n",
      "frequencies 31\n",
      "accordingly 31\n",
      "locking 31\n",
      "gently 31\n",
      "basement 31\n",
      "evaluation 31\n",
      "plastics 31\n",
      "saline 31\n",
      "widespread 30\n",
      "sen. 30\n",
      "voting 30\n",
      "felix 30\n",
      "representing 30\n",
      "sponsored 30\n",
      "worker 30\n",
      "medicine 30\n",
      "absolute 30\n",
      "doctors 30\n",
      "allies 30\n",
      "directions 30\n",
      "reform 30\n",
      "instances 30\n",
      "expert 30\n",
      "sheets 30\n",
      "replace 30\n",
      "gay 30\n",
      "islands 30\n",
      "split 30\n",
      "suspect 30\n",
      "graduate 30\n",
      "fence 30\n",
      "suspended 30\n",
      "franklin 30\n",
      "he'll 30\n",
      "louisiana 30\n",
      "lane 30\n",
      "sacrifice 30\n",
      "et 30\n",
      "network 30\n",
      "johnny 30\n",
      "eddie 30\n",
      "dates 30\n",
      "cuts 30\n",
      "reveal 30\n",
      "nowhere 30\n",
      "comments 30\n",
      "locked 30\n",
      "ranging 30\n",
      "controls 30\n",
      "strip 30\n",
      "alex 30\n",
      "dealers 30\n",
      "excessive 30\n",
      "buying 30\n",
      "associations 30\n",
      "they'd 30\n",
      "era 30\n",
      "virtue 30\n",
      "dreams 30\n",
      "secure 30\n",
      "sharpe 30\n",
      "impressed 30\n",
      "historian 30\n",
      "listened 30\n",
      "crucial 30\n",
      "propaganda 30\n",
      "deliberately 30\n",
      "measuring 30\n",
      "hoping 30\n",
      "surprising 30\n",
      "complicated 30\n",
      "occurrence 30\n",
      "preceding 30\n",
      "skilled 30\n",
      "radical 30\n",
      "citizen 30\n",
      "slave 30\n",
      "altogether 30\n",
      "purely 30\n",
      "frontier 30\n",
      "dimensions 30\n",
      "root 30\n",
      "blanket 30\n",
      "encountered 30\n",
      "consequence 30\n",
      "consciousness 30\n",
      "flux 30\n",
      "shakespeare 30\n",
      "cried 30\n",
      "mixture 30\n",
      "asleep 30\n",
      "concentrated 30\n",
      "meal 30\n",
      "stable 30\n",
      "stem 30\n",
      "carbon 30\n",
      "grinned 30\n",
      "unconscious 30\n",
      "dartmouth 30\n",
      "sovereign 30\n",
      "miriam 30\n",
      "woodruff 30\n",
      "fees 29\n",
      "divorce 29\n",
      "republicans 29\n",
      "sherman 29\n",
      "argued 29\n",
      "tea 29\n",
      "extending 29\n",
      "utility 29\n",
      "lieutenant 29\n",
      "proposals 29\n",
      "o. 29\n",
      "questioned 29\n",
      "modest 29\n",
      "contributions 29\n",
      "mighty 29\n",
      "ignored 29\n",
      "morse 29\n",
      "allied 29\n",
      "perform 29\n",
      "transferred 29\n",
      "false 29\n",
      "guilty 29\n",
      "merit 29\n",
      "brooklyn 29\n",
      "ethical 29\n",
      "recovery 29\n",
      "sons 29\n",
      "builder 29\n",
      "threatened 29\n",
      "testament 29\n",
      "volunteers 29\n",
      "players 29\n",
      "ann 29\n",
      "mickey 29\n",
      "silver 29\n",
      "belt 29\n",
      "shots 29\n",
      "trips 29\n",
      "exciting 29\n",
      "entertainment 29\n",
      "movie 29\n",
      "albert 29\n",
      "tasks 29\n",
      "unions 29\n",
      "encouraged 29\n",
      "suburban 29\n",
      "signals 29\n",
      "barn 29\n",
      "sewage 29\n",
      "jet 29\n",
      "drying 29\n",
      "lesson 29\n",
      "furnish 29\n",
      "creating 29\n",
      "morality 29\n",
      "fabrics 29\n",
      "stars 29\n",
      "residence 29\n",
      "delight 29\n",
      "theatre 29\n",
      "subsequent 29\n",
      "jurisdiction 29\n",
      "poured 29\n",
      "vigorous 29\n",
      "argue 29\n",
      "applying 29\n",
      "prestige 29\n",
      "bare 29\n",
      "sang 29\n",
      "helpful 29\n",
      "precious 29\n",
      "constitute 29\n",
      "magnitude 29\n",
      "solutions 29\n",
      "lighted 29\n",
      "suggests 29\n",
      "shapes 29\n",
      "anxious 29\n",
      "glasses 29\n",
      "cow 29\n",
      "apparatus 29\n",
      "scenes 29\n",
      "petitioner 29\n",
      "eternal 29\n",
      "shorts 29\n",
      "proportion 29\n",
      "regulations 29\n",
      "reminded 29\n",
      "ecumenical 29\n",
      "samples 29\n",
      "commonly 29\n",
      "ear 29\n",
      "electron 29\n",
      "pressed 29\n",
      "perception 29\n",
      "examination 29\n",
      "carleton 29\n",
      "bronchial 29\n",
      "brannon 29\n",
      "appointment 28\n",
      "responses 28\n",
      "enthusiasm 28\n",
      "newly 28\n",
      "harris 28\n",
      "calendar 28\n",
      "absent 28\n",
      "innocence 28\n",
      "president's 28\n",
      "meetings 28\n",
      "diplomatic 28\n",
      "southeast 28\n",
      "specified 28\n",
      "1955 28\n",
      "profit 28\n",
      "municipal 28\n",
      "demonstrate 28\n",
      "gathering 28\n",
      "exclusive 28\n",
      "irish 28\n",
      "encounter 28\n",
      "expanding 28\n",
      "n. 28\n",
      "losing 28\n",
      "formerly 28\n",
      "oct. 28\n",
      "compare 28\n",
      "examined 28\n",
      "roosevelt 28\n",
      "yankees 28\n",
      "1956 28\n",
      "arise 28\n",
      "prize 28\n",
      "wound 28\n",
      "hal 28\n",
      "talents 28\n",
      "african 28\n",
      "santa 28\n",
      "dining 28\n",
      "journey 28\n",
      "ladies 28\n",
      "freight 28\n",
      "maintaining 28\n",
      "designs 28\n",
      "marks 28\n",
      "promptly 28\n",
      "witness 28\n",
      "fled 28\n",
      "cloud 28\n",
      "upstairs 28\n",
      "dawn 28\n",
      "commander 28\n",
      "communications 28\n",
      "quarters 28\n",
      "rendered 28\n",
      "convention 28\n",
      "mechanism 28\n",
      "surfaces 28\n",
      "satisfaction 28\n",
      "offering 28\n",
      "pound 28\n",
      "tons 28\n",
      "closing 28\n",
      "colony 28\n",
      "warmth 28\n",
      "shade 28\n",
      "discuss 28\n",
      "peculiar 28\n",
      "paused 28\n",
      "folklore 28\n",
      "tight 28\n",
      "sand 28\n",
      "happening 28\n",
      "textile 28\n",
      "mines 28\n",
      "libraries 28\n",
      "limitations 28\n",
      "advantages 28\n",
      "excuse 28\n",
      "sovereignty 28\n",
      "humanity 28\n",
      "prayer 28\n",
      "hanging 28\n",
      "cure 28\n",
      "consistent 28\n",
      "clarity 28\n",
      "verse 28\n",
      "gentleman 28\n",
      "committed 28\n",
      "passion 28\n",
      "pot 28\n",
      "laugh 28\n",
      "sensitivity 28\n",
      "worthy 28\n",
      "dried 28\n",
      "hated 28\n",
      "bullet 28\n",
      "stained 28\n",
      "drugs 28\n",
      "laughing 28\n",
      "powder 28\n",
      "sergeant 28\n",
      "optimal 28\n",
      "polynomial 28\n",
      "ramey 28\n",
      "operated 27\n",
      "weekend 27\n",
      "voted 27\n",
      "veteran 27\n",
      "davis 27\n",
      "pistol 27\n",
      "permits 27\n",
      "hughes 27\n",
      "requirement 27\n",
      "acquire 27\n",
      "marshall 27\n",
      "prefer 27\n",
      "prevention 27\n",
      "aids 27\n",
      "absolutely 27\n",
      "placing 27\n",
      "scattered 27\n",
      "profound 27\n",
      "wherever 27\n",
      "insist 27\n",
      "shopping 27\n",
      "exact 27\n",
      "women's 27\n",
      "surplus 27\n",
      "t. 27\n",
      "publicly 27\n",
      "combat 27\n",
      "reorganization 27\n",
      "builders 27\n",
      "victim 27\n",
      "ours 27\n",
      "surrounding 27\n",
      "flew 27\n",
      "injury 27\n",
      "magnificent 27\n",
      "passes 27\n",
      "dan 27\n",
      "permission 27\n",
      "eager 27\n",
      "rushed 27\n",
      "christmas 27\n",
      "publicity 27\n",
      "festival 27\n",
      "suite 27\n",
      "reputation 27\n",
      "delaware 27\n",
      "greenwich 27\n",
      "clayton 27\n",
      "suspicion 27\n",
      "fred 27\n",
      "approaching 27\n",
      "literally 27\n",
      "distributed 27\n",
      "jefferson 27\n",
      "company's 27\n",
      "newport 27\n",
      "enemies 27\n",
      "restrictions 27\n",
      "wings 27\n",
      "reserved 27\n",
      "upward 27\n",
      "dull 27\n",
      "grain 27\n",
      "ranch 27\n",
      "butter 27\n",
      "mirror 27\n",
      "marriages 27\n",
      "refer 27\n",
      "utterly 27\n",
      "cap 27\n",
      "consisting 27\n",
      "horizon 27\n",
      "define 27\n",
      "delicate 27\n",
      "scope 27\n",
      "seconds 27\n",
      "scholars 27\n",
      "friendship 27\n",
      "customer 27\n",
      "germans 27\n",
      "concepts 27\n",
      "judgments 27\n",
      "outdoor 27\n",
      "occurs 27\n",
      "imagined 27\n",
      "discipline 27\n",
      "supporting 27\n",
      "shoot 27\n",
      "conceived 27\n",
      "observation 27\n",
      "roots 27\n",
      "gentle 27\n",
      "prevented 27\n",
      "theological 27\n",
      "minimal 27\n",
      "frozen 27\n",
      "holder 27\n",
      "oral 27\n",
      "clinical 27\n",
      "shirt 27\n",
      "slept 27\n",
      "julia 27\n",
      "fiber 27\n",
      "pursuant 27\n",
      "v 27\n",
      "pulmonary 27\n",
      "myra 27\n",
      "shayne 27\n",
      "cady 27\n",
      "eliminate 26\n",
      "settlement 26\n",
      "sessions 26\n",
      "controversy 26\n",
      "intelligent 26\n",
      "paying 26\n",
      "retirement 26\n",
      "1953 26\n",
      "kennedy's 26\n",
      "mutual 26\n",
      "climate 26\n",
      "critics 26\n",
      "outcome 26\n",
      "ill 26\n",
      "establishing 26\n",
      "assist 26\n",
      "part-time 26\n",
      "released 26\n",
      "liberals 26\n",
      "handled 26\n",
      "sixth 26\n",
      "mitchell 26\n",
      "unhappy 26\n",
      "desperate 26\n",
      "pointing 26\n",
      "premier 26\n",
      "kingdom 26\n",
      "promotion 26\n",
      "revenues 26\n",
      "widow 26\n",
      "bridges 26\n",
      "threatening 26\n",
      "disaster 26\n",
      "frames 26\n",
      "contest 26\n",
      "stretch 26\n",
      "billy 26\n",
      "bears 26\n",
      "27 26\n",
      "quoted 26\n",
      "entry 26\n",
      "inherent 26\n",
      "recalled 26\n",
      "overcome 26\n",
      "concerts 26\n",
      "storm 26\n",
      "cellar 26\n",
      "bath 26\n",
      "temperatures 26\n",
      "eileen 26\n",
      "mount 26\n",
      "register 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submarine 26\n",
      "gear 26\n",
      "electricity 26\n",
      "meals 26\n",
      "treat 26\n",
      "planes 26\n",
      "stockholders 26\n",
      "landing 26\n",
      "card 26\n",
      "instruction 26\n",
      "justify 26\n",
      "o 26\n",
      "invited 26\n",
      "exceptions 26\n",
      "sophisticated 26\n",
      "charm 26\n",
      "appreciate 26\n",
      "lively 26\n",
      "hang 26\n",
      "instruments 26\n",
      "delightful 26\n",
      "acquired 26\n",
      "preferred 26\n",
      "anti-trust 26\n",
      "marginal 26\n",
      "southerners 26\n",
      "cents 26\n",
      "legend 26\n",
      "wars 26\n",
      "coolidge 26\n",
      "peaceful 26\n",
      "repeat 26\n",
      "trembling 26\n",
      "emerged 26\n",
      "disturbed 26\n",
      "feeding 26\n",
      "perspective 26\n",
      "philosophical 26\n",
      "marshal 26\n",
      "mysterious 26\n",
      "arlene 26\n",
      "sarah 26\n",
      "frightened 26\n",
      "switches 26\n",
      "identify 26\n",
      "phenomena 26\n",
      "beard 26\n",
      "zen 26\n",
      "jew 26\n",
      "aesthetic 26\n",
      "velocity 26\n",
      "staring 26\n",
      "traders 26\n",
      "cavalry 26\n",
      "palfrey 26\n",
      "variables 26\n",
      "patchen 26\n",
      "snakes 26\n",
      "tangent 26\n",
      "johnnie 26\n",
      "urethane 26\n",
      "gyro 26\n",
      "ekstrohm 26\n",
      "helva 26\n",
      "greg 26\n",
      "departments 25\n",
      "aug. 25\n",
      "allowances 25\n",
      "constitutional 25\n",
      "abandoned 25\n",
      "recommend 25\n",
      "corporations 25\n",
      "houston 25\n",
      "owen 25\n",
      "racial 25\n",
      "viewed 25\n",
      "composition 25\n",
      "ward 25\n",
      "300 25\n",
      "nato 25\n",
      "nixon 25\n",
      "intervals 25\n",
      "bearing 25\n",
      "cocktail 25\n",
      "jersey 25\n",
      "succession 25\n",
      "attracted 25\n",
      "accused 25\n",
      "parade 25\n",
      "probable 25\n",
      "dilemma 25\n",
      "prospect 25\n",
      "eugene 25\n",
      "torn 25\n",
      "critic 25\n",
      "mothers 25\n",
      "noon 25\n",
      "al 25\n",
      "inspired 25\n",
      "stadium 25\n",
      "delayed 25\n",
      "nick 25\n",
      "productive 25\n",
      "star 25\n",
      "pack 25\n",
      "aboard 25\n",
      "conductor 25\n",
      "reaches 25\n",
      "harm 25\n",
      "wally 25\n",
      "pittsburgh 25\n",
      "amateur 25\n",
      "breaking 25\n",
      "terror 25\n",
      "cancer 25\n",
      "shelters 25\n",
      "d.c. 25\n",
      "pressing 25\n",
      "exhibit 25\n",
      "suits 25\n",
      "partially 25\n",
      "blanche 25\n",
      "patrol 25\n",
      "trustees 25\n",
      "generous 25\n",
      "applications 25\n",
      "evidently 25\n",
      "attacked 25\n",
      "northwest 25\n",
      "magnetic 25\n",
      "tim 25\n",
      "adult 25\n",
      "demonstration 25\n",
      "hired 25\n",
      "attached 25\n",
      "faint 25\n",
      "trading 25\n",
      "roughly 25\n",
      "dealer 25\n",
      "coverage 25\n",
      "vivid 25\n",
      "woods 25\n",
      "pile 25\n",
      "flexible 25\n",
      "pulling 25\n",
      "grateful 25\n",
      "correspondence 25\n",
      "carolina 25\n",
      "conferences 25\n",
      "rational 25\n",
      "painful 25\n",
      "proceeded 25\n",
      "impressions 25\n",
      "fortune 25\n",
      "glanced 25\n",
      "ritual 25\n",
      "wildly 25\n",
      "vague 25\n",
      "responsibilities 25\n",
      "pupils 25\n",
      "chin 25\n",
      "approaches 25\n",
      "vein 25\n",
      "operational 25\n",
      "honey 25\n",
      "lonely 25\n",
      "fist 25\n",
      "component 25\n",
      "magazines 25\n",
      "continually 25\n",
      "observe 25\n",
      "destructive 25\n",
      "lands 25\n",
      "twenty-five 25\n",
      "exposure 25\n",
      "fog 25\n",
      "devil 25\n",
      "cigarette 25\n",
      "continuity 25\n",
      "yours 25\n",
      "disk 25\n",
      "subtle 25\n",
      "reflect 25\n",
      "transformed 25\n",
      "pond 25\n",
      "structural 25\n",
      "contacts 25\n",
      "saddle 25\n",
      "detergent 25\n",
      "exploration 25\n",
      "penny 25\n",
      "regiment 25\n",
      "o'banion 25\n",
      "bang-jensen 25\n",
      "sba 25\n",
      "yeah 25\n",
      "alec 25\n",
      "barton 25\n",
      "tilghman 25\n",
      "occupation 24\n",
      "enthusiastic 24\n",
      "entering 24\n",
      "contracts 24\n",
      "insure 24\n",
      "subjected 24\n",
      "absorbed 24\n",
      "recommendation 24\n",
      "criminal 24\n",
      "ruling 24\n",
      "70 24\n",
      "qualified 24\n",
      "backed 24\n",
      "rank 24\n",
      "realization 24\n",
      "neighboring 24\n",
      "advisory 24\n",
      "full-time 24\n",
      "undoubtedly 24\n",
      "cited 24\n",
      "draft 24\n",
      "clubs 24\n",
      "managers 24\n",
      "announcement 24\n",
      "democracy 24\n",
      "tractor 24\n",
      "explicit 24\n",
      "honored 24\n",
      "estimates 24\n",
      "biggest 24\n",
      "puerto 24\n",
      "preliminary 24\n",
      "portland 24\n",
      "workshop 24\n",
      "accomplish 24\n",
      "relieved 24\n",
      "coach 24\n",
      "promising 24\n",
      "swing 24\n",
      "academy 24\n",
      "moore 24\n",
      "chances 24\n",
      "ford 24\n",
      "masters 24\n",
      "bend 24\n",
      "broadway 24\n",
      "arrive 24\n",
      "mason 24\n",
      "jump 24\n",
      "civilian 24\n",
      "motel 24\n",
      "seated 24\n",
      "prospects 24\n",
      "manufacturing 24\n",
      "heating 24\n",
      "firing 24\n",
      "seized 24\n",
      "slid 24\n",
      "tribute 24\n",
      "expressing 24\n",
      "seventeen 24\n",
      "tail 24\n",
      "factories 24\n",
      "depression 24\n",
      "phases 24\n",
      "consisted 24\n",
      "weekly 24\n",
      "charming 24\n",
      "assembled 24\n",
      "functional 24\n",
      "mexican 24\n",
      "exclusively 24\n",
      "leather 24\n",
      "nearest 24\n",
      "tended 24\n",
      "employee 24\n",
      "aimed 24\n",
      "specimen 24\n",
      "forgive 24\n",
      "barrel 24\n",
      "declaration 24\n",
      "angels 24\n",
      "drivers 24\n",
      "scarcely 24\n",
      "access 24\n",
      "cheap 24\n",
      "wholly 24\n",
      "realism 24\n",
      "utopia 24\n",
      "meaningful 24\n",
      "bore 24\n",
      "nervous 24\n",
      "interpreted 24\n",
      "lock 24\n",
      "desires 24\n",
      "wishes 24\n",
      "brave 24\n",
      "automobiles 24\n",
      "accurately 24\n",
      "actor 24\n",
      "narrative 24\n",
      "cycle 24\n",
      "stupid 24\n",
      "categories 24\n",
      "astronomy 24\n",
      "mathematical 24\n",
      "peas 24\n",
      "rigid 24\n",
      "drug 24\n",
      "zero 24\n",
      "tubes 24\n",
      "norms 24\n",
      "sitter 24\n",
      "wines 24\n",
      "diffusion 24\n",
      "theresa 24\n",
      "registration 23\n",
      "registered 23\n",
      "amendment 23\n",
      "agriculture 23\n",
      "midnight 23\n",
      "anticipated 23\n",
      "savings 23\n",
      "thinks 23\n",
      "discrimination 23\n",
      "monthly 23\n",
      "originally 23\n",
      "children's 23\n",
      "attending 23\n",
      "regime 23\n",
      "channels 23\n",
      "encouraging 23\n",
      "gen. 23\n",
      "compete 23\n",
      "luncheon 23\n",
      "orange 23\n",
      "colleagues 23\n",
      "historic 23\n",
      "governmental 23\n",
      "settle 23\n",
      "dedicated 23\n",
      "douglas 23\n",
      "memorial 23\n",
      "29 23\n",
      "circuit 23\n",
      "beliefs 23\n",
      "stressed 23\n",
      "strategic 23\n",
      "eighth 23\n",
      "pete 23\n",
      "champion 23\n",
      "casey 23\n",
      "bobby 23\n",
      "bowl 23\n",
      "maryland 23\n",
      "controlling 23\n",
      "don 23\n",
      "bases 23\n",
      "hearts 23\n",
      "ruth 23\n",
      "crystal 23\n",
      "tie 23\n",
      "harder 23\n",
      "expectations 23\n",
      "heights 23\n",
      "westminster 23\n",
      "flower 23\n",
      "jean 23\n",
      "suggestions 23\n",
      "furnished 23\n",
      "adults 23\n",
      "chairs 23\n",
      "worn 23\n",
      "dances 23\n",
      "arrival 23\n",
      "burns 23\n",
      "resumed 23\n",
      "ultimately 23\n",
      "cleared 23\n",
      "lawyers 23\n",
      "sharing 23\n",
      "killing 23\n",
      "rifles 23\n",
      "category 23\n",
      "madison 23\n",
      "sheep 23\n",
      "assessment 23\n",
      "farmer 23\n",
      "insects 23\n",
      "incredible 23\n",
      "dive 23\n",
      "spare 23\n",
      "attempting 23\n",
      "gin 23\n",
      "manufacturer 23\n",
      "lift 23\n",
      "heels 23\n",
      "plates 23\n",
      "hollywood 23\n",
      "stern 23\n",
      "noble 23\n",
      "stuck 23\n",
      "musician 23\n",
      "select 23\n",
      "justified 23\n",
      "giant 23\n",
      "sink 23\n",
      "unexpected 23\n",
      "hungry 23\n",
      "illustration 23\n",
      "fraction 23\n",
      "protest 23\n",
      "variations 23\n",
      "cabin 23\n",
      "whisky 23\n",
      "generations 23\n",
      "wake 23\n",
      "craft 23\n",
      "plug 23\n",
      "continuously 23\n",
      "sentiment 23\n",
      "reflects 23\n",
      "civic 23\n",
      "searching 23\n",
      "cat 23\n",
      "grades 23\n",
      "brown's 23\n",
      "exercises 23\n",
      "trace 23\n",
      "lighting 23\n",
      "sweat 23\n",
      "publications 23\n",
      "victor 23\n",
      "refrigerator 23\n",
      "enable 23\n",
      "rocks 23\n",
      "substances 23\n",
      "relevant 23\n",
      "tennessee 23\n",
      "belly 23\n",
      "radar 23\n",
      "h 23\n",
      "deck 23\n",
      "genius 23\n",
      "curiosity 23\n",
      "boating 23\n",
      "degrees 23\n",
      "oxidation 23\n",
      "hurried 23\n",
      "assumptions 23\n",
      "empirical 23\n",
      "excited 23\n",
      "habit 23\n",
      "lengths 23\n",
      "imitation 23\n",
      "displacement 23\n",
      "plaster 23\n",
      "fibers 23\n",
      "inventory 23\n",
      "wounded 23\n",
      "whispered 23\n",
      "fogg 23\n",
      "anti-semitism 23\n",
      "pa 23\n",
      "happiness 23\n",
      "authors 23\n",
      "maggie 23\n",
      "quiney 23\n",
      "spencer 23\n",
      "substrate 23\n",
      "pip 23\n",
      "ambiguous 22\n",
      "recommendations 22\n",
      "servants 22\n",
      "warned 22\n",
      "traveled 22\n",
      "congressional 22\n",
      "miller 22\n",
      "obligations 22\n",
      "sponsor 22\n",
      "complained 22\n",
      "expects 22\n",
      "gulf 22\n",
      "physics 22\n",
      "relatives 22\n",
      "capitol 22\n",
      "carries 22\n",
      "rehabilitation 22\n",
      "voluntary 22\n",
      "troubles 22\n",
      "appreciation 22\n",
      "attacks 22\n",
      "suited 22\n",
      "earliest 22\n",
      "trucks 22\n",
      "retained 22\n",
      "strategy 22\n",
      "posts 22\n",
      "intentions 22\n",
      "bid 22\n",
      "conspiracy 22\n",
      "investigations 22\n",
      "uncertain 22\n",
      "overseas 22\n",
      "adding 22\n",
      "loyalty 22\n",
      "patience 22\n",
      "ralph 22\n",
      "empire 22\n",
      "rev. 22\n",
      "miami 22\n",
      "exhibition 22\n",
      "hits 22\n",
      "pitch 22\n",
      "plate 22\n",
      "palm 22\n",
      "triumph 22\n",
      "baltimore 22\n",
      "doubtful 22\n",
      "statistics 22\n",
      "spectacular 22\n",
      "sighed 22\n",
      "balanced 22\n",
      "respects 22\n",
      "nerves 22\n",
      "teams 22\n",
      "dealt 22\n",
      "shouldn't 22\n",
      "engagement 22\n",
      "ah 22\n",
      "woman's 22\n",
      "vienna 22\n",
      "merchants 22\n",
      "aunt 22\n",
      "altered 22\n",
      "valid 22\n",
      "ambassador 22\n",
      "s 22\n",
      "auto 22\n",
      "elaine 22\n",
      "blues 22\n",
      "convenient 22\n",
      "loaded 22\n",
      "di 22\n",
      "gang 22\n",
      "regularly 22\n",
      "autumn 22\n",
      "moderate 22\n",
      "surrender 22\n",
      "chiefly 22\n",
      "chart 22\n",
      "resist 22\n",
      "architect 22\n",
      "weren't 22\n",
      "america's 22\n",
      "eighteenth 22\n",
      "rhythm 22\n",
      "ownership 22\n",
      "participate 22\n",
      "totally 22\n",
      "tip 22\n",
      "belongs 22\n",
      "panic 22\n",
      "shell 22\n",
      "capabilities 22\n",
      "substitute 22\n",
      "wealth 22\n",
      "savage 22\n",
      "occasions 22\n",
      "racing 22\n",
      "describes 22\n",
      "mess 22\n",
      "0 22\n",
      "bombers 22\n",
      "successes 22\n",
      "grows 22\n",
      "sticks 22\n",
      "backward 22\n",
      "desperately 22\n",
      "hide 22\n",
      "implications 22\n",
      "fault 22\n",
      "lo 22\n",
      "casual 22\n",
      "sandburg 22\n",
      "freely 22\n",
      "laughter 22\n",
      "destiny 22\n",
      "drinks 22\n",
      "motive 22\n",
      "targets 22\n",
      "thrust 22\n",
      "sphere 22\n",
      "novels 22\n",
      "melting 22\n",
      "formulas 22\n",
      "unfortunate 22\n",
      "joke 22\n",
      "uneasy 22\n",
      "souls 22\n",
      "arbitrary 22\n",
      "reliable 22\n",
      "possessed 22\n",
      "eliminated 22\n",
      "fortunate 22\n",
      "meanings 22\n",
      "bother 22\n",
      "insight 22\n",
      "preparing 22\n",
      "steadily 22\n",
      "forests 22\n",
      "physiological 22\n",
      "planets 22\n",
      "alaska 22\n",
      "frequency 22\n",
      "tire 22\n",
      "dressing 22\n",
      "economical 22\n",
      "sixties 22\n",
      "herd 22\n",
      "anglo-saxon 22\n",
      "soap 22\n",
      "yelled 22\n",
      "middle-class 22\n",
      "alienation 22\n",
      "sampling 22\n",
      "refund 22\n",
      "hypothalamic 22\n",
      "foams 22\n",
      "bobbie 22\n",
      "deegan 22\n",
      "merger 21\n",
      "taxpayers 21\n",
      "jail 21\n",
      "witnesses 21\n",
      "saving 21\n",
      "delay 21\n",
      "springs 21\n",
      "associate 21\n",
      "startled 21\n",
      "port 21\n",
      "nov. 21\n",
      "morris 21\n",
      "questioning 21\n",
      "speeches 21\n",
      "inspection 21\n",
      "acceptable 21\n",
      "detroit 21\n",
      "displayed 21\n",
      "cope 21\n",
      "1948 21\n",
      "pertinent 21\n",
      "procurement 21\n",
      "colonial 21\n",
      "socialist 21\n",
      "screw 21\n",
      "johnston 21\n",
      "hesitated 21\n",
      "respond 21\n",
      "launched 21\n",
      "crises 21\n",
      "michigan 21\n",
      "1950 21\n",
      "dynamic 21\n",
      "wagner 21\n",
      "submitted 21\n",
      "forming 21\n",
      "rico 21\n",
      "surrounded 21\n",
      "keeps 21\n",
      "congressman 21\n",
      "dedication 21\n",
      "unlikely 21\n",
      "city's 21\n",
      "accepting 21\n",
      "behalf 21\n",
      "flash 21\n",
      "trends 21\n",
      "philip 21\n",
      "russ 21\n",
      "pitcher 21\n",
      "races 21\n",
      "crossing 21\n",
      "definitely 21\n",
      "loop 21\n",
      "fans 21\n",
      "masses 21\n",
      "giants 21\n",
      "vernon 21\n",
      "helpless 21\n",
      "replacement 21\n",
      "ben 21\n",
      "missouri 21\n",
      "despair 21\n",
      "warwick 21\n",
      "stiff 21\n",
      "anniversary 21\n",
      "francis 21\n",
      "luxury 21\n",
      "skirt 21\n",
      "beam 21\n",
      "kay 21\n",
      "colorful 21\n",
      "taylor 21\n",
      "28 21\n",
      "availability 21\n",
      "killer 21\n",
      "joyce 21\n",
      "drawings 21\n",
      "prairie 21\n",
      "suspected 21\n",
      "revolutionary 21\n",
      "prisoners 21\n",
      "governing 21\n",
      "prospective 21\n",
      "profits 21\n",
      "painter 21\n",
      "wheels 21\n",
      "conversion 21\n",
      "defend 21\n",
      "crack 21\n",
      "lucky 21\n",
      "characterized 21\n",
      "winds 21\n",
      "heritage 21\n",
      "computed 21\n",
      "inclined 21\n",
      "lowered 21\n",
      "dishes 21\n",
      "marble 21\n",
      "passengers 21\n",
      "addresses 21\n",
      "john's 21\n",
      "ideological 21\n",
      "monument 21\n",
      "fluid 21\n",
      "shaking 21\n",
      "vermont 21\n",
      "urgent 21\n",
      "pause 21\n",
      "competent 21\n",
      "commodities 21\n",
      "indirect 21\n",
      "34 21\n",
      "ugly 21\n",
      "gentlemen 21\n",
      "belgians 21\n",
      "obliged 21\n",
      "katanga 21\n",
      "respectable 21\n",
      "desert 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displays 21\n",
      "educated 21\n",
      "enjoyment 21\n",
      "theirs 21\n",
      "partisan 21\n",
      "urge 21\n",
      "bullets 21\n",
      "resolved 21\n",
      "diet 21\n",
      "shame 21\n",
      "bold 21\n",
      "certainty 21\n",
      "podger 21\n",
      "aristotle 21\n",
      "cholesterol 21\n",
      "wondering 21\n",
      "verbal 21\n",
      "classification 21\n",
      "wives 21\n",
      "sidewalk 21\n",
      "scared 21\n",
      "melody 21\n",
      "tales 21\n",
      "cromwell 21\n",
      "persuaded 21\n",
      "thorough 21\n",
      "sixty 21\n",
      "breathing 21\n",
      "theoretical 21\n",
      "tale 21\n",
      "envelope 21\n",
      "possession 21\n",
      "summary 21\n",
      "heroic 21\n",
      "shining 21\n",
      "intimate 21\n",
      "traditions 21\n",
      "habits 21\n",
      "dare 21\n",
      "neat 21\n",
      "hen 21\n",
      "milligrams 21\n",
      "protein 21\n",
      "punishment 21\n",
      "stumbled 21\n",
      "reverend 21\n",
      "mode 21\n",
      "glory 21\n",
      "reveals 21\n",
      "shu 21\n",
      "planet 21\n",
      "rent 21\n",
      "intermediate 21\n",
      "nuts 21\n",
      "circular 21\n",
      "particle 21\n",
      "garage 21\n",
      "linear 21\n",
      "patient's 21\n",
      "smart 21\n",
      "faulkner 21\n",
      "complement 21\n",
      "mate 21\n",
      "telegraph 21\n",
      "tsunami 21\n",
      "bridget 21\n",
      "planetary 21\n",
      "occurring 21\n",
      "keith 21\n",
      "mars 21\n",
      "utopian 21\n",
      "steele 21\n",
      "rang 21\n",
      "maude 21\n",
      "hoag 21\n",
      "b'dikkat 21\n",
      "allen 20\n",
      "voters 20\n",
      "legislators 20\n",
      "orderly 20\n",
      "receives 20\n",
      "jan. 20\n",
      "adjustments 20\n",
      "repair 20\n",
      "votes 20\n",
      "sheriff 20\n",
      "enforced 20\n",
      "el 20\n",
      "stake 20\n",
      "feb. 20\n",
      "border 20\n",
      "solve 20\n",
      "underlying 20\n",
      "observers 20\n",
      "quarrel 20\n",
      "cape 20\n",
      "judges 20\n",
      "grants 20\n",
      "illness 20\n",
      "hospitals 20\n",
      "confirmed 20\n",
      "treaty 20\n",
      "alliance 20\n",
      "submarines 20\n",
      "disposal 20\n",
      "dominated 20\n",
      "intervention 20\n",
      "negotiations 20\n",
      "sailing 20\n",
      "residents 20\n",
      "bet 20\n",
      "notte 20\n",
      "conditioned 20\n",
      "greeted 20\n",
      "basically 20\n",
      "expanded 20\n",
      "emphasize 20\n",
      "manhattan 20\n",
      "aroused 20\n",
      "temporarily 20\n",
      "mathematics 20\n",
      "explains 20\n",
      "puts 20\n",
      "tactics 20\n",
      "1946 20\n",
      "decent 20\n",
      "ranks 20\n",
      "trim 20\n",
      "donald 20\n",
      "inc. 20\n",
      "75 20\n",
      "hotels 20\n",
      "parks 20\n",
      "injured 20\n",
      "rush 20\n",
      "compromise 20\n",
      "pioneer 20\n",
      "ninth 20\n",
      "purchased 20\n",
      "strongest 20\n",
      "grabbed 20\n",
      "florida 20\n",
      "physically 20\n",
      "clock 20\n",
      "splendid 20\n",
      "strikes 20\n",
      "grip 20\n",
      "guys 20\n",
      "1951 20\n",
      "buck 20\n",
      "1949 20\n",
      "arnold 20\n",
      "tournament 20\n",
      "invitation 20\n",
      "loud 20\n",
      "fitted 20\n",
      "boss 20\n",
      "mercy 20\n",
      "chapel 20\n",
      "promises 20\n",
      "bundle 20\n",
      "clothing 20\n",
      "slim 20\n",
      "improvements 20\n",
      "secrets 20\n",
      "hidden 20\n",
      "converted 20\n",
      "gather 20\n",
      "crash 20\n",
      "retail 20\n",
      "guided 20\n",
      "assessors 20\n",
      "brushed 20\n",
      "crop 20\n",
      "newer 20\n",
      "warrant 20\n",
      "supplement 20\n",
      "thereafter 20\n",
      "foil 20\n",
      "notable 20\n",
      "pipe 20\n",
      "struggling 20\n",
      "cream 20\n",
      "indication 20\n",
      "tones 20\n",
      "cafe 20\n",
      "hasn't 20\n",
      "trap 20\n",
      "figured 20\n",
      "abel 20\n",
      "reactionary 20\n",
      "tent 20\n",
      "blonde 20\n",
      "lobby 20\n",
      "renaissance 20\n",
      "responded 20\n",
      "founded 20\n",
      "fantastic 20\n",
      "binding 20\n",
      "lean 20\n",
      "landscape 20\n",
      "amazing 20\n",
      "alike 20\n",
      "passages 20\n",
      "reporter 20\n",
      "cooperative 20\n",
      "alabama 20\n",
      "happily 20\n",
      "shadows 20\n",
      "rises 20\n",
      "fortunately 20\n",
      "vacuum 20\n",
      "endless 20\n",
      "minority 20\n",
      "overwhelming 20\n",
      "jungle 20\n",
      "convictions 20\n",
      "nest 20\n",
      "fascinating 20\n",
      "accordance 20\n",
      "motives 20\n",
      "listeners 20\n",
      "distinctive 20\n",
      "tooth 20\n",
      "attain 20\n",
      "styles 20\n",
      "bones 20\n",
      "wit 20\n",
      "solely 20\n",
      "socialism 20\n",
      "grains 20\n",
      "sixteen 20\n",
      "hatred 20\n",
      "creatures 20\n",
      "biological 20\n",
      "poverty 20\n",
      "twentieth 20\n",
      "historians 20\n",
      "manage 20\n",
      "authentic 20\n",
      "laura 20\n",
      "dolores 20\n",
      "leaped 20\n",
      "doc 20\n",
      "transformation 20\n",
      "theories 20\n",
      "worries 20\n",
      "merchant 20\n",
      "christ's 20\n",
      "b.c. 20\n",
      "anyhow 20\n",
      "relating 20\n",
      "probabilities 20\n",
      "impulse 20\n",
      "package 20\n",
      "pupil 20\n",
      "anticipation 20\n",
      "slide 20\n",
      "fractions 20\n",
      "cathy 20\n",
      "boots 20\n",
      "sauce 20\n",
      "mustard 20\n",
      "michelangelo 20\n",
      "invention 20\n",
      "cheek 20\n",
      "awake 20\n",
      "pursue 20\n",
      "peered 20\n",
      "crawled 20\n",
      "nude 20\n",
      "okay 20\n",
      "borden 20\n",
      "plato 20\n",
      "oedipus 20\n",
      "lungs 20\n",
      "input 20\n",
      "/ 20\n",
      "suitcase 20\n",
      "skyros 20\n",
      "freddy 20\n",
      "airport 19\n",
      "gov. 19\n",
      "rob 19\n",
      "consistently 19\n",
      "policeman 19\n",
      "underground 19\n",
      "remainder 19\n",
      "arrest 19\n",
      "whereby 19\n",
      "imposed 19\n",
      "discharge 19\n",
      "avoided 19\n",
      "cuban 19\n",
      "enforcement 19\n",
      "commissioner 19\n",
      "appeals 19\n",
      "supervision 19\n",
      "interviews 19\n",
      "tangible 19\n",
      "politicians 19\n",
      "elementary 19\n",
      "respective 19\n",
      "stresses 19\n",
      "directors 19\n",
      "continental 19\n",
      "filing 19\n",
      "males 19\n",
      "guards 19\n",
      "vincent 19\n",
      "salem 19\n",
      "lodge 19\n",
      "specialists 19\n",
      "wiped 19\n",
      "slender 19\n",
      "snapped 19\n",
      "string 19\n",
      "whip 19\n",
      "ray 19\n",
      "achievements 19\n",
      "span 19\n",
      "drank 19\n",
      "fathers 19\n",
      "stroke 19\n",
      "1927 19\n",
      "frederick 19\n",
      "addressed 19\n",
      "ethics 19\n",
      "toast 19\n",
      "lover 19\n",
      "calif. 19\n",
      "solved 19\n",
      "theology 19\n",
      "crown 19\n",
      "convenience 19\n",
      "men's 19\n",
      "victims 19\n",
      "arrested 19\n",
      "cottage 19\n",
      "lid 19\n",
      "packed 19\n",
      "lacked 19\n",
      "condemned 19\n",
      "documents 19\n",
      "corporate 19\n",
      "eve 19\n",
      "entries 19\n",
      "wildlife 19\n",
      "livestock 19\n",
      "businesses 19\n",
      "attract 19\n",
      "companion 19\n",
      "rid 19\n",
      "shipping 19\n",
      "earnings 19\n",
      "makers 19\n",
      "gains 19\n",
      "venture 19\n",
      "affects 19\n",
      "demanding 19\n",
      "delivery 19\n",
      "allows 19\n",
      "toes 19\n",
      "loves 19\n",
      "mexico 19\n",
      "likes 19\n",
      "ham 19\n",
      "label 19\n",
      "ladder 19\n",
      "dreamed 19\n",
      "resting 19\n",
      "guitar 19\n",
      "pamela 19\n",
      "traveling 19\n",
      "slip 19\n",
      "spell 19\n",
      "neatly 19\n",
      "people's 19\n",
      "decisive 19\n",
      "tensions 19\n",
      "caution 19\n",
      "dated 19\n",
      "fatal 19\n",
      "insistence 19\n",
      "midst 19\n",
      "devotion 19\n",
      "produces 19\n",
      "shocked 19\n",
      "flood 19\n",
      "ambition 19\n",
      "combinations 19\n",
      "mechanics 19\n",
      "deadly 19\n",
      "lap 19\n",
      "slope 19\n",
      "instantly 19\n",
      "rolling 19\n",
      "ridiculous 19\n",
      "startling 19\n",
      "exceed 19\n",
      "preserved 19\n",
      "canvas 19\n",
      "comprehensive 19\n",
      "servant 19\n",
      "assurance 19\n",
      "dairy 19\n",
      "infinite 19\n",
      "individually 19\n",
      "exceptional 19\n",
      "lesser 19\n",
      "shorter 19\n",
      "polish 19\n",
      "tobacco 19\n",
      "ignore 19\n",
      "applies 19\n",
      "relax 19\n",
      "brass 19\n",
      "curves 19\n",
      "sober 19\n",
      "naive 19\n",
      "depths 19\n",
      "diseases 19\n",
      "orthodox 19\n",
      "hostile 19\n",
      "blowing 19\n",
      "wisconsin 19\n",
      "rugged 19\n",
      "broader 19\n",
      "sketches 19\n",
      "paula 19\n",
      "fever 19\n",
      "realm 19\n",
      "emperor 19\n",
      "salesmen 19\n",
      "optical 19\n",
      "situated 19\n",
      "fats 19\n",
      "hypothalamus 19\n",
      "immortality 19\n",
      "assert 19\n",
      "numerical 19\n",
      "realtors 19\n",
      "daytime 19\n",
      "amen 19\n",
      "dim 19\n",
      "distances 19\n",
      "puzzled 19\n",
      "hay 19\n",
      "ma 19\n",
      "figs. 19\n",
      "twisted 19\n",
      "fury 19\n",
      "straightened 19\n",
      "timber 19\n",
      "glued 19\n",
      "movable 19\n",
      "essay 19\n",
      "distinguish 19\n",
      "patents 19\n",
      "lb. 19\n",
      "sec. 19\n",
      "therapist 19\n",
      "damned 19\n",
      "murderer 19\n",
      "plantation 19\n",
      "helion 19\n",
      "rousseau 19\n",
      "smelled 19\n",
      "reactivity 19\n",
      "tetrachloride 19\n",
      "sera 19\n",
      "nonspecific 19\n",
      "vector 19\n",
      "vertex 19\n",
      "rourke 19\n",
      "killpath 19\n",
      "haney 19\n",
      "letch 19\n",
      "commented 18\n",
      "ridge 18\n",
      "priority 18\n",
      "privilege 18\n",
      "formally 18\n",
      "austin 18\n",
      "stocks 18\n",
      "folks 18\n",
      "committees 18\n",
      "earned 18\n",
      "athletic 18\n",
      "stolen 18\n",
      "deliver 18\n",
      "proceedings 18\n",
      "repeatedly 18\n",
      "abuse 18\n",
      "35 18\n",
      "aged 18\n",
      "drain 18\n",
      "asks 18\n",
      "emerge 18\n",
      "proceed 18\n",
      "remarkably 18\n",
      "compelled 18\n",
      "faster 18\n",
      "arkansas 18\n",
      "juvenile 18\n",
      "assign 18\n",
      "arose 18\n",
      "chorus 18\n",
      "lip 18\n",
      "resentment 18\n",
      "talks 18\n",
      "hunter 18\n",
      "announce 18\n",
      "shrugged 18\n",
      "1945 18\n",
      "erected 18\n",
      "halfway 18\n",
      "columbia 18\n",
      "camps 18\n",
      "loyal 18\n",
      "squeezed 18\n",
      "ranged 18\n",
      "sue 18\n",
      "objection 18\n",
      "pronounced 18\n",
      "representation 18\n",
      "bargaining 18\n",
      "rebel 18\n",
      "dick 18\n",
      "squad 18\n",
      "yankee 18\n",
      "bat 18\n",
      "wing 18\n",
      "tokyo 18\n",
      "favored 18\n",
      "kicked 18\n",
      "lemon 18\n",
      "fan 18\n",
      "400 18\n",
      "ceremony 18\n",
      "fame 18\n",
      "ernie 18\n",
      "harvey 18\n",
      "stevens 18\n",
      "costumes 18\n",
      "tommy 18\n",
      "stein 18\n",
      "it'll 18\n",
      "knight 18\n",
      "angel 18\n",
      "twist 18\n",
      "birthday 18\n",
      "chase 18\n",
      "80 18\n",
      "meredith 18\n",
      "suburbs 18\n",
      "competence 18\n",
      "brick 18\n",
      "predicted 18\n",
      "buried 18\n",
      "overnight 18\n",
      "kowalski 18\n",
      "candle 18\n",
      "escaped 18\n",
      "pole 18\n",
      "battery 18\n",
      "crops 18\n",
      "emphasized 18\n",
      "youngsters 18\n",
      "stephen 18\n",
      "investigated 18\n",
      "acted 18\n",
      "officially 18\n",
      "ratios 18\n",
      "overhead 18\n",
      "classroom 18\n",
      "abrupt 18\n",
      "continuation 18\n",
      "sank 18\n",
      "bell 18\n",
      "50% 18\n",
      "disposed 18\n",
      "designer 18\n",
      "pen 18\n",
      "oxford 18\n",
      "lamp 18\n",
      "parlor 18\n",
      "assignments 18\n",
      "bishop 18\n",
      "undertaken 18\n",
      "attributed 18\n",
      "fountain 18\n",
      "informal 18\n",
      "initially 18\n",
      "mechanisms 18\n",
      "specialized 18\n",
      "manufacture 18\n",
      "fellows 18\n",
      "customs 18\n",
      "tanks 18\n",
      "drops 18\n",
      "terribly 18\n",
      "earnest 18\n",
      "nineteen 18\n",
      "cats 18\n",
      "religions 18\n",
      "autonomy 18\n",
      "copies 18\n",
      "advances 18\n",
      "applicable 18\n",
      "humble 18\n",
      "defended 18\n",
      "spectacle 18\n",
      "submit 18\n",
      "interrupted 18\n",
      "expecting 18\n",
      "rests 18\n",
      "partnership 18\n",
      "editors 18\n",
      "congregations 18\n",
      "drift 18\n",
      "rubbed 18\n",
      "plainly 18\n",
      "surprisingly 18\n",
      "consistency 18\n",
      "biblical 18\n",
      "phrases 18\n",
      "bathroom 18\n",
      "sung 18\n",
      "brains 18\n",
      "bitterness 18\n",
      "causing 18\n",
      "locations 18\n",
      "bulletin 18\n",
      "likewise 18\n",
      "referring 18\n",
      "sole 18\n",
      "neglected 18\n",
      "tap 18\n",
      "cosmic 18\n",
      "denial 18\n",
      "shifted 18\n",
      "selective 18\n",
      "corners 18\n",
      "faded 18\n",
      "print 18\n",
      "affection 18\n",
      "hypothesis 18\n",
      "refers 18\n",
      "transport 18\n",
      "speaks 18\n",
      "simpler 18\n",
      "who's 18\n",
      "beloved 18\n",
      "tray 18\n",
      "reverse 18\n",
      "differ 18\n",
      "consumption 18\n",
      "technological 18\n",
      "medieval 18\n",
      "rod 18\n",
      "proportions 18\n",
      "interval 18\n",
      "exercised 18\n",
      "brightness 18\n",
      "camping 18\n",
      "cylinder 18\n",
      "colt 18\n",
      "photograph 18\n",
      "aluminum 18\n",
      "bunk 18\n",
      "milling 18\n",
      "grams 18\n",
      "printing 18\n",
      "ammunition 18\n",
      "systematic 18\n",
      "roared 18\n",
      "epic 18\n",
      "subjective 18\n",
      "intuition 18\n",
      "marry 18\n",
      "upton 18\n",
      "forgot 18\n",
      "adolescence 18\n",
      "griffith 18\n",
      "doctor's 18\n",
      "stall 18\n",
      "saxon 18\n",
      "cobb 18\n",
      "spectra 18\n",
      "serum 18\n",
      "iodine 18\n",
      "tsh 18\n",
      "theorem 18\n",
      "abruptly 18\n",
      "effluent 18\n",
      "stevie 18\n",
      "andrei 18\n",
      "kitti 18\n",
      "madden 18\n",
      "langford 18\n",
      "barco 18\n",
      "fulton 17\n",
      "praise 17\n",
      "purchasing 17\n",
      "department's 17\n",
      "compensation 17\n",
      "employes 17\n",
      "consulted 17\n",
      "superintendent 17\n",
      "anonymous 17\n",
      "pockets 17\n",
      "violation 17\n",
      "counsel 17\n",
      "pierre 17\n",
      "nursing 17\n",
      "nurse 17\n",
      "combine 17\n",
      "persuade 17\n",
      "cabinet 17\n",
      "wider 17\n",
      "popularity 17\n",
      "friction 17\n",
      "geneva 17\n",
      "lao 17\n",
      "fleet 17\n",
      "useless 17\n",
      "performing 17\n",
      "vitality 17\n",
      "raymond 17\n",
      "noting 17\n",
      "deputy 17\n",
      "shops 17\n",
      "consent 17\n",
      "vicious 17\n",
      "fires 17\n",
      "ivory 17\n",
      "visits 17\n",
      "declined 17\n",
      "scientist 17\n",
      "missionary 17\n",
      "pays 17\n",
      "separation 17\n",
      "eighteen 17\n",
      "slowed 17\n",
      "assuming 17\n",
      "stripped 17\n",
      "weary 17\n",
      "alternatives 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "followers 17\n",
      "females 17\n",
      "k. 17\n",
      "tract 17\n",
      "designated 17\n",
      "apartments 17\n",
      "baptist 17\n",
      "explosive 17\n",
      "dec. 17\n",
      "grasp 17\n",
      "graduates 17\n",
      "motions 17\n",
      "catcher 17\n",
      "balls 17\n",
      "failing 17\n",
      "reporting 17\n",
      "defensive 17\n",
      "walker 17\n",
      "cleveland 17\n",
      "hank 17\n",
      "throwing 17\n",
      "checks 17\n",
      "cracked 17\n",
      "porter 17\n",
      "seasons 17\n",
      "hitting 17\n",
      "shaped 17\n",
      "awards 17\n",
      "departure 17\n",
      "barriers 17\n",
      "meadow 17\n",
      "sport 17\n",
      "admired 17\n",
      "holiday 17\n",
      "earth's 17\n",
      "awarded 17\n",
      "muttered 17\n",
      "import 17\n",
      "lessons 17\n",
      "milton 17\n",
      "wagons 17\n",
      "portrait 17\n",
      "detectives 17\n",
      "mortgage 17\n",
      "cooled 17\n",
      "steam 17\n",
      "codes 17\n",
      "simms 17\n",
      "confession 17\n",
      "rested 17\n",
      "spontaneous 17\n",
      "describing 17\n",
      "captured 17\n",
      "rebels 17\n",
      "renewed 17\n",
      "operators 17\n",
      "inability 17\n",
      "indifference 17\n",
      "intersection 17\n",
      "economics 17\n",
      "flowing 17\n",
      "anniston 17\n",
      "carroll 17\n",
      "preservation 17\n",
      "breed 17\n",
      "fuel 17\n",
      "suicide 17\n",
      "gambling 17\n",
      "tracing 17\n",
      "counted 17\n",
      "inquiries 17\n",
      "chip 17\n",
      "sooner 17\n",
      "pushing 17\n",
      "shake 17\n",
      "reflecting 17\n",
      "logic 17\n",
      "connecticut 17\n",
      "accompanying 17\n",
      "preaching 17\n",
      "darling 17\n",
      "acquisition 17\n",
      "stretching 17\n",
      "seemingly 17\n",
      "diplomacy 17\n",
      "lest 17\n",
      "aggressive 17\n",
      "embassy 17\n",
      "notions 17\n",
      "illustrate 17\n",
      "allocation 17\n",
      "supernatural 17\n",
      "brand 17\n",
      "titles 17\n",
      "destroying 17\n",
      "artificial 17\n",
      "sums 17\n",
      "employers 17\n",
      "chaos 17\n",
      "quote 17\n",
      "heroes 17\n",
      "possess 17\n",
      "frankie 17\n",
      "belonged 17\n",
      "vaguely 17\n",
      "continent 17\n",
      "hamilton 17\n",
      "engines 17\n",
      "capture 17\n",
      "loudly 17\n",
      "productivity 17\n",
      "remembering 17\n",
      "dome 17\n",
      "seal 17\n",
      "campaigns 17\n",
      "mistaken 17\n",
      "formidable 17\n",
      "acceleration 17\n",
      "guerrillas 17\n",
      "rivers 17\n",
      "pastor 17\n",
      "silently 17\n",
      "progressive 17\n",
      "neighborhoods 17\n",
      "enjoying 17\n",
      "staying 17\n",
      "significantly 17\n",
      "readings 17\n",
      "thee 17\n",
      "basket 17\n",
      "collar 17\n",
      "tumor 17\n",
      "gap 17\n",
      "lion 17\n",
      "christians 17\n",
      "morale 17\n",
      "husband's 17\n",
      "consist 17\n",
      "awful 17\n",
      "well-known 17\n",
      "statue 17\n",
      "burma 17\n",
      "prevailing 17\n",
      "abandon 17\n",
      "clever 17\n",
      "shifts 17\n",
      "obscure 17\n",
      "uncertainty 17\n",
      "horror 17\n",
      "interaction 17\n",
      "bunch 17\n",
      "infectious 17\n",
      "imaginary 17\n",
      "fitting 17\n",
      "summers 17\n",
      "flame 17\n",
      "benson 17\n",
      "absurd 17\n",
      "implied 17\n",
      "present-day 17\n",
      "inquiry 17\n",
      "hunger 17\n",
      "and/or 17\n",
      "indirectly 17\n",
      "comparative 17\n",
      "gonna 17\n",
      "packing 17\n",
      "ballistic 17\n",
      "bunks 17\n",
      "whiskey 17\n",
      "sunlight 17\n",
      "lit 17\n",
      "frankfurter 17\n",
      "transom 17\n",
      "respiratory 17\n",
      "drug's 17\n",
      "formulation 17\n",
      "cereal 17\n",
      "dandy 17\n",
      "screaming 17\n",
      "screamed 17\n",
      "voyage 17\n",
      "cops 17\n",
      "giffen 17\n",
      "lizzie 17\n",
      "piazza 17\n",
      "kiss 17\n",
      "murmured 17\n",
      "molecular 17\n",
      "hiding 17\n",
      "sensed 17\n",
      "trevelyan 17\n",
      "krim 17\n",
      "graph 17\n",
      "geometric 17\n",
      "thyroglobulin 17\n",
      "corridor 17\n",
      "odd-lot 17\n",
      "bod 17\n",
      "accelerometer 17\n",
      "ada 17\n",
      "angie 17\n",
      "lalaurie 17\n",
      "nadine 17\n",
      "moreland 17\n",
      "deserves 16\n",
      "revised 16\n",
      "improving 16\n",
      "farms 16\n",
      "highways 16\n",
      "raises 16\n",
      "appearing 16\n",
      "locate 16\n",
      "circulation 16\n",
      "scott 16\n",
      "pro 16\n",
      "fee 16\n",
      "sites 16\n",
      "costly 16\n",
      "gotten 16\n",
      "boy's 16\n",
      "chemistry 16\n",
      "indicating 16\n",
      "inquired 16\n",
      "thru 16\n",
      "financed 16\n",
      "payroll 16\n",
      "unnecessary 16\n",
      "ambitious 16\n",
      "requiring 16\n",
      "explosion 16\n",
      "geographical 16\n",
      "viet 16\n",
      "invasion 16\n",
      "observer 16\n",
      "professors 16\n",
      "pin 16\n",
      "disturbing 16\n",
      "severely 16\n",
      "1910 16\n",
      "500 16\n",
      "unemployment 16\n",
      "glow 16\n",
      "ticket 16\n",
      "dropping 16\n",
      "successor 16\n",
      "confirm 16\n",
      "taxi 16\n",
      "viewpoint 16\n",
      "government's 16\n",
      "chapters 16\n",
      "commitments 16\n",
      "quest 16\n",
      "shortage 16\n",
      "clearing 16\n",
      "legitimate 16\n",
      "transit 16\n",
      "adequately 16\n",
      "montgomery 16\n",
      "greece 16\n",
      "fighters 16\n",
      "42 16\n",
      "instructed 16\n",
      "delegates 16\n",
      "strengthen 16\n",
      "reads 16\n",
      "missions 16\n",
      "arranging 16\n",
      "indications 16\n",
      "homer 16\n",
      "brooks 16\n",
      "chores 16\n",
      "bounced 16\n",
      "slammed 16\n",
      "flavor 16\n",
      "pitching 16\n",
      "allowance 16\n",
      "claiming 16\n",
      "southwest 16\n",
      "statistical 16\n",
      "nelson 16\n",
      "spotted 16\n",
      "m 16\n",
      "denver 16\n",
      "buffalo 16\n",
      "cherished 16\n",
      "waved 16\n",
      "i. 16\n",
      "stuart 16\n",
      "commit 16\n",
      "strings 16\n",
      "cows 16\n",
      "they've 16\n",
      "differently 16\n",
      "inherited 16\n",
      "earn 16\n",
      "oriental 16\n",
      "gown 16\n",
      "byron 16\n",
      "trains 16\n",
      "goodness 16\n",
      "candy 16\n",
      "confident 16\n",
      "collect 16\n",
      "telephoned 16\n",
      "opens 16\n",
      "staged 16\n",
      "link 16\n",
      "tile 16\n",
      "troop 16\n",
      "proceeds 16\n",
      "commissioners 16\n",
      "broadcast 16\n",
      "flashed 16\n",
      "asserted 16\n",
      "pursuit 16\n",
      "spun 16\n",
      "northeast 16\n",
      "mailed 16\n",
      "priests 16\n",
      "obligation 16\n",
      "producer 16\n",
      "cranston 16\n",
      "rider 16\n",
      "mistakes 16\n",
      "cockpit 16\n",
      "beautifully 16\n",
      "adjust 16\n",
      "monk 16\n",
      "investors 16\n",
      "wanting 16\n",
      "translated 16\n",
      "fancy 16\n",
      "grab 16\n",
      "fringe 16\n",
      "peak 16\n",
      "pan 16\n",
      "dangers 16\n",
      "railroads 16\n",
      "shipments 16\n",
      "vessel 16\n",
      "simplicity 16\n",
      "composite 16\n",
      "matched 16\n",
      "henri 16\n",
      "anthony 16\n",
      "l 16\n",
      "outfit 16\n",
      "bass 16\n",
      "dish 16\n",
      "lecture 16\n",
      "finest 16\n",
      "persistent 16\n",
      "sins 16\n",
      "swinging 16\n",
      "channel 16\n",
      "delighted 16\n",
      "discussing 16\n",
      "confrontation 16\n",
      "aims 16\n",
      "satisfy 16\n",
      "miracle 16\n",
      "orientation 16\n",
      "qualifications 16\n",
      "secular 16\n",
      "partners 16\n",
      "foolish 16\n",
      "devised 16\n",
      "reasoning 16\n",
      "nineteenth-century 16\n",
      "parliament 16\n",
      "vs. 16\n",
      "stating 16\n",
      "afterward 16\n",
      "whites 16\n",
      "excluding 16\n",
      "switched 16\n",
      "muscular 16\n",
      "endurance 16\n",
      "purchases 16\n",
      "philosopher 16\n",
      "notably 16\n",
      "hammarskjold 16\n",
      "soup 16\n",
      "toll 16\n",
      "gradual 16\n",
      "rail 16\n",
      "ample 16\n",
      "maintains 16\n",
      "capita 16\n",
      "founding 16\n",
      "bulk 16\n",
      "translate 16\n",
      "conjunction 16\n",
      "hers 16\n",
      "blockade 16\n",
      "sturdy 16\n",
      "audiences 16\n",
      "piled 16\n",
      "spreading 16\n",
      "contents 16\n",
      "flexibility 16\n",
      "lined 16\n",
      "disastrous 16\n",
      "rows 16\n",
      "specialist 16\n",
      "stride 16\n",
      "confined 16\n",
      "influenced 16\n",
      "rage 16\n",
      "vegetables 16\n",
      "implies 16\n",
      "unto 16\n",
      "o' 16\n",
      "resume 16\n",
      "ashamed 16\n",
      "injustice 16\n",
      "typically 16\n",
      "heated 16\n",
      "prolonged 16\n",
      "messages 16\n",
      "translation 16\n",
      "cleaned 16\n",
      "vertical 16\n",
      "ruined 16\n",
      "compatible 16\n",
      "alarm 16\n",
      "compounds 16\n",
      "transmission 16\n",
      "veterans 16\n",
      "flag 16\n",
      "refuse 16\n",
      "buffer 16\n",
      "proves 16\n",
      "minimize 16\n",
      "ideals 16\n",
      "orbit 16\n",
      "boundary 16\n",
      "conclude 16\n",
      "historically 16\n",
      "insights 16\n",
      "bleeding 16\n",
      "linked 16\n",
      "alien 16\n",
      "angular 16\n",
      "doubts 16\n",
      "damp 16\n",
      "jeep 16\n",
      "trails 16\n",
      "proposition 16\n",
      "differential 16\n",
      "networks 16\n",
      "curriculum 16\n",
      "glorious 16\n",
      "glimpse 16\n",
      "photographs 16\n",
      "bitterly 16\n",
      "cowboy 16\n",
      "inserted 16\n",
      "ignorance 16\n",
      "sketch 16\n",
      "forehead 16\n",
      "exhibits 16\n",
      "occupy 16\n",
      "wasted 16\n",
      "gavin 16\n",
      "stirring 16\n",
      "references 16\n",
      "humorous 16\n",
      "arteries 16\n",
      "liver 16\n",
      "saint 16\n",
      "priest 16\n",
      "justification 16\n",
      "downward 16\n",
      "ate 16\n",
      "thereof 16\n",
      "weighed 16\n",
      "judicial 16\n",
      "attic 16\n",
      "conformity 16\n",
      "metaphysical 16\n",
      "spray 16\n",
      "1/2 16\n",
      "mare 16\n",
      "jaw 16\n",
      "fences 16\n",
      "hawaii 16\n",
      "tourist 16\n",
      "pottery 16\n",
      "jar 16\n",
      "infantry 16\n",
      "closet 16\n",
      "synthesis 16\n",
      "lung 16\n",
      "correlation 16\n",
      "installations 16\n",
      "farming 16\n",
      "claire 16\n",
      "hudson's 16\n",
      "g 16\n",
      "isolation 16\n",
      "dusty 16\n",
      "crouched 16\n",
      "dickens 16\n",
      "analytic 16\n",
      "solar 16\n",
      "abstraction 16\n",
      "optimum 16\n",
      "opium 16\n",
      "voltage 16\n",
      "clover 16\n",
      "secants 16\n",
      "roleplaying 16\n",
      "operand 16\n",
      "leveling 16\n",
      "payne 16\n",
      "styka 16\n",
      "gilborn 16\n",
      "mae 16\n",
      "handley 16\n",
      "nicolas 16\n",
      "hanford 16\n",
      "willis 16\n",
      "foster 15\n",
      "petition 15\n",
      "wife's 15\n",
      "validity 15\n",
      "defeated 15\n",
      "bankers 15\n",
      "termed 15\n",
      "oppose 15\n",
      "mentally 15\n",
      "eliminating 15\n",
      "succeed 15\n",
      "decrease 15\n",
      "policemen 15\n",
      "client 15\n",
      "devote 15\n",
      "contempt 15\n",
      "employer 15\n",
      "boost 15\n",
      "indispensable 15\n",
      "hastily 15\n",
      "administrator 15\n",
      "selections 15\n",
      "disappointment 15\n",
      "tentative 15\n",
      "essence 15\n",
      "threshold 15\n",
      "optimism 15\n",
      "participating 15\n",
      "morton 15\n",
      "coalition 15\n",
      "domination 15\n",
      "constructive 15\n",
      "react 15\n",
      "hire 15\n",
      "rescue 15\n",
      "gaining 15\n",
      "federation 15\n",
      "stature 15\n",
      "1944 15\n",
      "attraction 15\n",
      "country's 15\n",
      "optimistic 15\n",
      "seats 15\n",
      "marching 15\n",
      "operates 15\n",
      "contributing 15\n",
      "unpleasant 15\n",
      "competing 15\n",
      "berger 15\n",
      "cambridge 15\n",
      "backs 15\n",
      "agreements 15\n",
      "tore 15\n",
      "blast 15\n",
      "landed 15\n",
      "reward 15\n",
      "practicing 15\n",
      "coordination 15\n",
      "norman 15\n",
      "oak 15\n",
      "loses 15\n",
      "powell 15\n",
      "verdict 15\n",
      "jerry 15\n",
      "batting 15\n",
      "apprentice 15\n",
      "physician 15\n",
      "kick 15\n",
      "scored 15\n",
      "jay 15\n",
      "meaningless 15\n",
      "trick 15\n",
      "ace 15\n",
      "smashed 15\n",
      "newest 15\n",
      "advocate 15\n",
      "quit 15\n",
      "graham 15\n",
      "personalities 15\n",
      "roger 15\n",
      "brooding 15\n",
      "potato 15\n",
      "alongside 15\n",
      "creature 15\n",
      "cardinal 15\n",
      "dragging 15\n",
      "beaten 15\n",
      "honors 15\n",
      "knock 15\n",
      "sights 15\n",
      "lawn 15\n",
      "tennis 15\n",
      "swim 15\n",
      "preceded 15\n",
      "pools 15\n",
      "loving 15\n",
      "originated 15\n",
      "bow 15\n",
      "underwater 15\n",
      "forbidden 15\n",
      "cemetery 15\n",
      "stove 15\n",
      "havana 15\n",
      "segregated 15\n",
      "intend 15\n",
      "excellence 15\n",
      "waddell 15\n",
      "cafeteria 15\n",
      "assault 15\n",
      "daylight 15\n",
      "arguments 15\n",
      "judged 15\n",
      "day's 15\n",
      "judging 15\n",
      "pains 15\n",
      "rental 15\n",
      "scores 15\n",
      "connections 15\n",
      "da 15\n",
      "cubic 15\n",
      "celebration 15\n",
      "qualify 15\n",
      "1912 15\n",
      "sizable 15\n",
      "comparatively 15\n",
      "alter 15\n",
      "businessmen 15\n",
      "supports 15\n",
      "owed 15\n",
      "province 15\n",
      "parent 15\n",
      "madame 15\n",
      "apt 15\n",
      "potatoes 15\n",
      "onion 15\n",
      "texture 15\n",
      "rounded 15\n",
      "heavier 15\n",
      "carla 15\n",
      "elizabeth 15\n",
      "drums 15\n",
      "picnic 15\n",
      "actors 15\n",
      "silly 15\n",
      "mileage 15\n",
      "associates 15\n",
      "discouraged 15\n",
      "evenings 15\n",
      "realities 15\n",
      "penetration 15\n",
      "disapproval 15\n",
      "dragged 15\n",
      "cease 15\n",
      "dimension 15\n",
      "scholar 15\n",
      "tightly 15\n",
      "expressions 15\n",
      "stimulus 15\n",
      "restricted 15\n",
      "deemed 15\n",
      "stirred 15\n",
      "readiness 15\n",
      "gates 15\n",
      "tense 15\n",
      "deliberate 15\n",
      "dutch 15\n",
      "ties 15\n",
      "dug 15\n",
      "1940 15\n",
      "elder 15\n",
      "stevenson 15\n",
      "virtues 15\n",
      "reluctant 15\n",
      "nut 15\n",
      "turnpike 15\n",
      "textiles 15\n",
      "imports 15\n",
      "clue 15\n",
      "feasible 15\n",
      "joining 15\n",
      "governed 15\n",
      "drag 15\n",
      "intensive 15\n",
      "emerging 15\n",
      "crude 15\n",
      "clearer 15\n",
      "divisions 15\n",
      "selecting 15\n",
      "forgiveness 15\n",
      "sweep 15\n",
      "dad 15\n",
      "accustomed 15\n",
      "fabric 15\n",
      "considers 15\n",
      "rubber 15\n",
      "writings 15\n",
      "straw 15\n",
      "razor 15\n",
      "shower 15\n",
      "guessed 15\n",
      "sheer 15\n",
      "senses 15\n",
      "explanations 15\n",
      "opponent 15\n",
      "cruel 15\n",
      "exhausted 15\n",
      "misery 15\n",
      "distress 15\n",
      "freezing 15\n",
      "sincere 15\n",
      "israel 15\n",
      "horrible 15\n",
      "remind 15\n",
      "stalin 15\n",
      "plunged 15\n",
      "execution 15\n",
      "refusal 15\n",
      "commonplace 15\n",
      "preventive 15\n",
      "liberalism 15\n",
      "lectures 15\n",
      "placement 15\n",
      "memories 15\n",
      "sentimental 15\n",
      "commands 15\n",
      "lyrics 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amy 15\n",
      "cop 15\n",
      "thread 15\n",
      "circumstance 15\n",
      "achieving 15\n",
      "touching 15\n",
      "crying 15\n",
      "contours 15\n",
      "deals 15\n",
      "burn 15\n",
      "anchor 15\n",
      "novelist 15\n",
      "vanished 15\n",
      "ambitions 15\n",
      "sloan 15\n",
      "manners 15\n",
      "sustained 15\n",
      "englishman 15\n",
      "expedition 15\n",
      "arriving 15\n",
      "literal 15\n",
      "denominations 15\n",
      "distinctions 15\n",
      "socially 15\n",
      "pill 15\n",
      "hey 15\n",
      "dissolved 15\n",
      "cared 15\n",
      "steinberg 15\n",
      "industry's 15\n",
      "tappet 15\n",
      "substituted 15\n",
      "cracking 15\n",
      "needle 15\n",
      "battens 15\n",
      "armies 15\n",
      "bathing 15\n",
      "bottles 15\n",
      "profile 15\n",
      "swiftly 15\n",
      "preparations 15\n",
      "molding 15\n",
      "fluids 15\n",
      "pont's 15\n",
      "doorway 15\n",
      "soils 15\n",
      "x 15\n",
      "commanded 15\n",
      "disappointed 15\n",
      "ryan 15\n",
      "self-help 15\n",
      "rope 15\n",
      "kissed 15\n",
      "oils 15\n",
      "ulyate 15\n",
      "leaning 15\n",
      "rancher 15\n",
      "selden 15\n",
      "bees 15\n",
      "fromm 15\n",
      "deserted 15\n",
      "katie 15\n",
      "aegean 15\n",
      "blackman 15\n",
      "folded 15\n",
      "sociology 15\n",
      "helium 15\n",
      "lublin 15\n",
      "homeric 15\n",
      "burton 15\n",
      "collage 15\n",
      "fromm's 15\n",
      "hardy's 15\n",
      "lagoon 15\n",
      "juanita 15\n",
      "eugenia 15\n",
      "influences 14\n",
      "encouragement 14\n",
      "governor's 14\n",
      "picking 14\n",
      "approve 14\n",
      "bush 14\n",
      "threats 14\n",
      "feared 14\n",
      "daniel 14\n",
      "treasurer 14\n",
      "amended 14\n",
      "penalty 14\n",
      "pending 14\n",
      "russia's 14\n",
      "dismissed 14\n",
      "requests 14\n",
      "criticized 14\n",
      "customary 14\n",
      "reforms 14\n",
      "arises 14\n",
      "vice-president 14\n",
      "kentucky 14\n",
      "disclosed 14\n",
      "organize 14\n",
      "eligible 14\n",
      "fails 14\n",
      "lever 14\n",
      "town's 14\n",
      "affixed 14\n",
      "plains 14\n",
      "applause 14\n",
      "passenger 14\n",
      "preferably 14\n",
      "debut 14\n",
      "carved 14\n",
      "believing 14\n",
      "corruption 14\n",
      "amid 14\n",
      "publishing 14\n",
      "ceremonies 14\n",
      "odds 14\n",
      "$1 14\n",
      "annually 14\n",
      "driveway 14\n",
      "ap 14\n",
      "paths 14\n",
      "speakers 14\n",
      "grove 14\n",
      "theodore 14\n",
      "revelation 14\n",
      "10,000 14\n",
      "sensation 14\n",
      "engage 14\n",
      "purse 14\n",
      "columbus 14\n",
      "bothered 14\n",
      "45 14\n",
      "upset 14\n",
      "exclaimed 14\n",
      "countless 14\n",
      "chuck 14\n",
      "examiner 14\n",
      "incidentally 14\n",
      "hodges 14\n",
      "merits 14\n",
      "bull 14\n",
      "careers 14\n",
      "gum 14\n",
      "joan 14\n",
      "egypt 14\n",
      "invitations 14\n",
      "farewell 14\n",
      "appearances 14\n",
      "entertain 14\n",
      "marty 14\n",
      "touches 14\n",
      "frightening 14\n",
      "boxes 14\n",
      "gods 14\n",
      "tickets 14\n",
      "elegant 14\n",
      "hats 14\n",
      "husbands 14\n",
      "celebrated 14\n",
      "auditorium 14\n",
      "1947 14\n",
      "prosperity 14\n",
      "complexity 14\n",
      "fastened 14\n",
      "convicted 14\n",
      "neighbor 14\n",
      "surviving 14\n",
      "pursued 14\n",
      "executed 14\n",
      "emory 14\n",
      "daughters 14\n",
      "uniforms 14\n",
      "counts 14\n",
      "flames 14\n",
      "alice 14\n",
      "valued 14\n",
      "beverly 14\n",
      "interstate 14\n",
      "stopping 14\n",
      "complaint 14\n",
      "flights 14\n",
      "scrutiny 14\n",
      "swiss 14\n",
      "pickup 14\n",
      "sunset 14\n",
      "momentum 14\n",
      "hemisphere 14\n",
      "short-term 14\n",
      "one-third 14\n",
      "producers 14\n",
      "creator 14\n",
      "shoe 14\n",
      "designers 14\n",
      "arrow 14\n",
      "yarn 14\n",
      "appealing 14\n",
      "pianist 14\n",
      "le 14\n",
      "scenery 14\n",
      "lend 14\n",
      "bored 14\n",
      "adventures 14\n",
      "feathers 14\n",
      "polished 14\n",
      "presidents 14\n",
      "khrushchev's 14\n",
      "organs 14\n",
      "prone 14\n",
      "protective 14\n",
      "framed 14\n",
      "structured 14\n",
      "regulation 14\n",
      "enterprises 14\n",
      "monopoly 14\n",
      "capitalism 14\n",
      "impersonal 14\n",
      "15th 14\n",
      "afterwards 14\n",
      "intact 14\n",
      "pine 14\n",
      "borrowed 14\n",
      "divide 14\n",
      "classified 14\n",
      "sweater 14\n",
      "oldest 14\n",
      "picasso 14\n",
      "chancellor 14\n",
      "boundaries 14\n",
      "belgian 14\n",
      "administered 14\n",
      "ordinarily 14\n",
      "anaconda 14\n",
      "twenty-four 14\n",
      "flung 14\n",
      "vigor 14\n",
      "nassau 14\n",
      "psychology 14\n",
      "centered 14\n",
      "penetrating 14\n",
      "dominican 14\n",
      "habitat 14\n",
      "hopeless 14\n",
      "ruin 14\n",
      "paragraphs 14\n",
      "spectrum 14\n",
      "draws 14\n",
      "amusing 14\n",
      "intent 14\n",
      "accommodate 14\n",
      "irrelevant 14\n",
      "cannery 14\n",
      "breeze 14\n",
      "treats 14\n",
      "deviation 14\n",
      "uncomfortable 14\n",
      "relaxed 14\n",
      "crimes 14\n",
      "sensible 14\n",
      "thou 14\n",
      "answering 14\n",
      "congregational 14\n",
      "brace 14\n",
      "analyzed 14\n",
      "seeming 14\n",
      "kent 14\n",
      "prescribed 14\n",
      "heater 14\n",
      "capability 14\n",
      "vulnerable 14\n",
      "vocal 14\n",
      "mild 14\n",
      "influential 14\n",
      "usage 14\n",
      "custom 14\n",
      "leap 14\n",
      "superiority 14\n",
      "protestants 14\n",
      "manpower 14\n",
      "shallow 14\n",
      "speeds 14\n",
      "painfully 14\n",
      "gloom 14\n",
      "profitable 14\n",
      "restored 14\n",
      "symptoms 14\n",
      "decay 14\n",
      "shells 14\n",
      "superb 14\n",
      "weep 14\n",
      "comedie 14\n",
      "immense 14\n",
      "nearer 14\n",
      "retreat 14\n",
      "upright 14\n",
      "lightning 14\n",
      "territorial 14\n",
      "yalta 14\n",
      "pity 14\n",
      "stare 14\n",
      "fists 14\n",
      "pit 14\n",
      "dared 14\n",
      "thunder 14\n",
      "evolution 14\n",
      "projected 14\n",
      "thirty-five 14\n",
      "prose 14\n",
      "solitary 14\n",
      "fantasy 14\n",
      "king's 14\n",
      "emancipation 14\n",
      "heap 14\n",
      "instinct 14\n",
      "mist 14\n",
      "sustain 14\n",
      "minerals 14\n",
      "proteins 14\n",
      "irenaeus 14\n",
      "foundations 14\n",
      "fruits 14\n",
      "odor 14\n",
      "survived 14\n",
      "pairs 14\n",
      "climax 14\n",
      "grim 14\n",
      "conceive 14\n",
      "attach 14\n",
      "proclamation 14\n",
      "width 14\n",
      "ripe 14\n",
      "favorably 14\n",
      "resonance 14\n",
      "coordinated 14\n",
      "revolver 14\n",
      "coupled 14\n",
      "creek 14\n",
      "fork 14\n",
      "pie 14\n",
      "beaches 14\n",
      "adventure 14\n",
      "illuminated 14\n",
      "cups 14\n",
      "charcoal 14\n",
      "fix 14\n",
      "strips 14\n",
      "bubbles 14\n",
      "wax 14\n",
      "nails 14\n",
      "screwed 14\n",
      "confederate 14\n",
      "insect 14\n",
      "conditioning 14\n",
      "thoughtfully 14\n",
      "x-ray 14\n",
      "ultraviolet 14\n",
      "generator 14\n",
      "bacterial 14\n",
      "pasture 14\n",
      "wrapped 14\n",
      "determines 14\n",
      "neon 14\n",
      "bari 14\n",
      "gradient 14\n",
      "bark 14\n",
      "diane 14\n",
      "eichmann 14\n",
      "dislike 14\n",
      "swore 14\n",
      "plato's 14\n",
      "garryowen 14\n",
      "clung 14\n",
      "singular 14\n",
      "harlem 14\n",
      "blank 14\n",
      "discretion 14\n",
      "bastards 14\n",
      "dice 14\n",
      "garth 14\n",
      "chill 14\n",
      "greville 14\n",
      "rupees 14\n",
      "export-import 14\n",
      "decomposition 14\n",
      "aqueous 14\n",
      "proportional 14\n",
      "woke 14\n",
      "python 14\n",
      "conjugates 14\n",
      "diagonalizable 14\n",
      "iliad 14\n",
      "hawk 14\n",
      "beowulf 14\n",
      "jessica 14\n",
      "jess's 14\n",
      "edythe 14\n",
      "welch 14\n",
      "handful 13\n",
      "remedy 13\n",
      "enabling 13\n",
      "incorporated 13\n",
      "undue 13\n",
      "deputies 13\n",
      "pension 13\n",
      "praised 13\n",
      "cruelty 13\n",
      "gop 13\n",
      "tower 13\n",
      "rep. 13\n",
      "relieve 13\n",
      "65 13\n",
      "opponents 13\n",
      "oklahoma 13\n",
      "consulting 13\n",
      "breakdown 13\n",
      "spokesman 13\n",
      "unfair 13\n",
      "privately 13\n",
      "expand 13\n",
      "guaranteed 13\n",
      "propose 13\n",
      "ill. 13\n",
      "emotionally 13\n",
      "un 13\n",
      "globe 13\n",
      "nam 13\n",
      "modified 13\n",
      "spokesmen 13\n",
      "restrained 13\n",
      "reviewed 13\n",
      "adopt 13\n",
      "disposition 13\n",
      "beating 13\n",
      "witnessed 13\n",
      "ave. 13\n",
      "fits 13\n",
      "conservation 13\n",
      "resolve 13\n",
      "visitor 13\n",
      "democrat 13\n",
      "protested 13\n",
      "inhabitants 13\n",
      "standpoint 13\n",
      "spectators 13\n",
      "lafayette 13\n",
      "observing 13\n",
      "restless 13\n",
      "solidarity 13\n",
      "convey 13\n",
      "acute 13\n",
      "$600 13\n",
      "objected 13\n",
      "absorb 13\n",
      "statewide 13\n",
      "800 13\n",
      "ed 13\n",
      "buddy 13\n",
      "gentile 13\n",
      "n.y. 13\n",
      "completing 13\n",
      "accelerated 13\n",
      "helen 13\n",
      "sox 13\n",
      "tries 13\n",
      "incomplete 13\n",
      "defending 13\n",
      "idle 13\n",
      "minnesota 13\n",
      "mays 13\n",
      "elderly 13\n",
      "belonging 13\n",
      "lou 13\n",
      "afternoons 13\n",
      "night's 13\n",
      "matches 13\n",
      "monstrous 13\n",
      "par 13\n",
      "slice 13\n",
      "handy 13\n",
      "grin 13\n",
      "tumbled 13\n",
      "correctly 13\n",
      "youngest 13\n",
      "singers 13\n",
      "portable 13\n",
      "sunny 13\n",
      "lucille 13\n",
      "dough 13\n",
      "fur 13\n",
      "tribune 13\n",
      "colorado 13\n",
      "grandma 13\n",
      "methodist 13\n",
      "lester 13\n",
      "anderson 13\n",
      "graduated 13\n",
      "resident 13\n",
      "outset 13\n",
      "cumulative 13\n",
      "hull 13\n",
      "dot 13\n",
      "indiana 13\n",
      "appealed 13\n",
      "adjoining 13\n",
      "suburb 13\n",
      "supplying 13\n",
      "mcclellan 13\n",
      "safely 13\n",
      "exaggerated 13\n",
      "statutory 13\n",
      "commitment 13\n",
      "survivors 13\n",
      "sisters 13\n",
      "handicapped 13\n",
      "familiarity 13\n",
      "collapsed 13\n",
      "omitted 13\n",
      "barbecue 13\n",
      "acid 13\n",
      "scotland 13\n",
      "birmingham 13\n",
      "imply 13\n",
      "nazi 13\n",
      "explaining 13\n",
      "abilities 13\n",
      "sells 13\n",
      "1900 13\n",
      "sweeping 13\n",
      "stimulation 13\n",
      "undertake 13\n",
      "lowest 13\n",
      "inventories 13\n",
      "assets 13\n",
      "taxed 13\n",
      "steep 13\n",
      "ireland 13\n",
      "truman 13\n",
      "pepper 13\n",
      "utter 13\n",
      "lasting 13\n",
      "shades 13\n",
      "copper 13\n",
      "carpet 13\n",
      "couples 13\n",
      "streetcar 13\n",
      "walks 13\n",
      "nonsense 13\n",
      "understandable 13\n",
      "performers 13\n",
      "map 13\n",
      "prayers 13\n",
      "conducting 13\n",
      "imperial 13\n",
      "brutality 13\n",
      "smallest 13\n",
      "miserable 13\n",
      "separately 13\n",
      "rely 13\n",
      "satisfying 13\n",
      "owns 13\n",
      "depreciation 13\n",
      "derive 13\n",
      "implicit 13\n",
      "ideology 13\n",
      "dictatorship 13\n",
      "sentences 13\n",
      "suspicious 13\n",
      "rebellion 13\n",
      "purple 13\n",
      "electoral 13\n",
      "rounds 13\n",
      "roar 13\n",
      "gifted 13\n",
      "aloud 13\n",
      "dumb 13\n",
      "doubtless 13\n",
      "pentagon 13\n",
      "greene 13\n",
      "collecting 13\n",
      "journalism 13\n",
      "investigators 13\n",
      "ethnic 13\n",
      "elite 13\n",
      "consumed 13\n",
      "planners 13\n",
      "fiat 13\n",
      "modernization 13\n",
      "forcing 13\n",
      "ton 13\n",
      "biography 13\n",
      "evaluate 13\n",
      "vigorously 13\n",
      "stability 13\n",
      "creates 13\n",
      "frankly 13\n",
      "sector 13\n",
      "specimens 13\n",
      "prohibition 13\n",
      "tempted 13\n",
      "ironic 13\n",
      "politician 13\n",
      "earnestly 13\n",
      "progressed 13\n",
      "unlimited 13\n",
      "objections 13\n",
      "fearful 13\n",
      "cake 13\n",
      "posture 13\n",
      "shy 13\n",
      "document 13\n",
      "contradiction 13\n",
      "shattered 13\n",
      "blessed 13\n",
      "bursting 13\n",
      "autistic 13\n",
      "diagnosis 13\n",
      "nursery 13\n",
      "rug 13\n",
      "curtain 13\n",
      "invented 13\n",
      "rockets 13\n",
      "32 13\n",
      "yale 13\n",
      "repetition 13\n",
      "experimentation 13\n",
      "slightest 13\n",
      "adapted 13\n",
      "ego 13\n",
      "embrace 13\n",
      "remembers 13\n",
      "erect 13\n",
      "suggesting 13\n",
      "promoting 13\n",
      "statute 13\n",
      "hart 13\n",
      "inquirer 13\n",
      "inspector 13\n",
      "drainage 13\n",
      "u 13\n",
      "media 13\n",
      "induced 13\n",
      "virus 13\n",
      "defects 13\n",
      "fusion 13\n",
      "doses 13\n",
      "radically 13\n",
      "perceive 13\n",
      "insane 13\n",
      "equilibrium 13\n",
      "tips 13\n",
      "painters 13\n",
      "participated 13\n",
      "foreigners 13\n",
      "analogy 13\n",
      "communicate 13\n",
      "deer 13\n",
      "patch 13\n",
      "mix 13\n",
      "composers 13\n",
      "deepest 13\n",
      "enters 13\n",
      "resemblance 13\n",
      "sincerity 13\n",
      "charlotte 13\n",
      "differed 13\n",
      "diverse 13\n",
      "permanently 13\n",
      "boot 13\n",
      "imaginative 13\n",
      "casually 13\n",
      "static 13\n",
      "romance 13\n",
      "curled 13\n",
      "behaved 13\n",
      "programing 13\n",
      "spur 13\n",
      "gospel 13\n",
      "illustrations 13\n",
      "abundance 13\n",
      "rocking 13\n",
      "poet's 13\n",
      "fictional 13\n",
      "eagerly 13\n",
      "buzz 13\n",
      "cone 13\n",
      "whoever 13\n",
      "averaged 13\n",
      "refrigeration 13\n",
      "alcohol 13\n",
      "integral 13\n",
      "mythological 13\n",
      "mutually 13\n",
      "ministry 13\n",
      "debt 13\n",
      "sprang 13\n",
      "opposing 13\n",
      "leveled 13\n",
      "exerted 13\n",
      "maps 13\n",
      "clarify 13\n",
      "involvement 13\n",
      "gossip 13\n",
      "ordering 13\n",
      "dwelling 13\n",
      "blade 13\n",
      "waving 13\n",
      "pansies 13\n",
      "behave 13\n",
      "tappets 13\n",
      "insert 13\n",
      "files 13\n",
      "lantern 13\n",
      "chewing 13\n",
      "fond 13\n",
      "lime 13\n",
      "pint 13\n",
      "squares 13\n",
      "transparent 13\n",
      "toilet 13\n",
      "sealed 13\n",
      "arch 13\n",
      "beams 13\n",
      "conditioner 13\n",
      "compass 13\n",
      "computer 13\n",
      "antenna 13\n",
      "receiver 13\n",
      "detection 13\n",
      "tub 13\n",
      "oersted 13\n",
      "necessities 13\n",
      "champagne 13\n",
      "linguist 13\n",
      "therapeutic 13\n",
      "depot 13\n",
      "officer's 13\n",
      "wires 13\n",
      "begged 13\n",
      "hymen 13\n",
      "scream 13\n",
      "vocabulary 13\n",
      "foundation's 13\n",
      "chickens 13\n",
      "hut 13\n",
      "thayer 13\n",
      "sinister 13\n",
      "packard 13\n",
      "proclaim 13\n",
      "litigation 13\n",
      "a.l.a.m. 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 13\n",
      "tory 13\n",
      "copernicus 13\n",
      "unaware 13\n",
      "manifold 13\n",
      "curb 13\n",
      "null 13\n",
      "piepsam 13\n",
      "monitoring 13\n",
      "incest 13\n",
      "frieze 13\n",
      "rabbi 13\n",
      "hormone 13\n",
      "arlen 13\n",
      "diversity 13\n",
      "garibaldi 13\n",
      "moonlight 13\n",
      "malraux 13\n",
      "analyses 13\n",
      "sally 13\n",
      "subsection 13\n",
      "paramagnetic 13\n",
      "plasma 13\n",
      "nighttime 13\n",
      "solids 13\n",
      "nuclei 13\n",
      "tissues 13\n",
      "skeletal 13\n",
      "wtv 13\n",
      "invariant 13\n",
      "casework 13\n",
      "kohnstamm-positive 13\n",
      "kohnstamm 13\n",
      "compulsivity 13\n",
      "cheeks 13\n",
      "athabascan 13\n",
      "pip's 13\n",
      "phosphor 13\n",
      "ludie 13\n",
      "argiento 13\n",
      "hino 13\n",
      "glendora 13\n",
      "effected 12\n",
      "1913 12\n",
      "ballot 12\n",
      "$100 12\n",
      "expended 12\n",
      "deficit 12\n",
      "deaf 12\n",
      "yearly 12\n",
      "whipped 12\n",
      "meantime 12\n",
      "two-thirds 12\n",
      "paradise 12\n",
      "patrolman 12\n",
      "ribbon 12\n",
      "mass. 12\n",
      "advisers 12\n",
      "dental 12\n",
      "1.5 12\n",
      "staggered 12\n",
      "enacted 12\n",
      "wayne 12\n",
      "ministers 12\n",
      "harsh 12\n",
      "ad 12\n",
      "strengthening 12\n",
      "correspondent 12\n",
      "feeds 12\n",
      "pro-western 12\n",
      "spark 12\n",
      "urgency 12\n",
      "outline 12\n",
      "contended 12\n",
      "controversial 12\n",
      "nomination 12\n",
      "essex 12\n",
      "credited 12\n",
      "co-operation 12\n",
      "rival 12\n",
      "choices 12\n",
      "episode 12\n",
      "hopeful 12\n",
      "parochial 12\n",
      "technicians 12\n",
      "premium 12\n",
      "welcomed 12\n",
      "dwight 12\n",
      "tracks 12\n",
      "andrew 12\n",
      "guam 12\n",
      "focused 12\n",
      "crossroads 12\n",
      "terminate 12\n",
      "acknowledge 12\n",
      "blocked 12\n",
      "joints 12\n",
      "$500 12\n",
      "flies 12\n",
      "mining 12\n",
      "turkish 12\n",
      "dependence 12\n",
      "surveys 12\n",
      "babies 12\n",
      "inning 12\n",
      "lighter 12\n",
      "breaks 12\n",
      "crowds 12\n",
      "nearing 12\n",
      "kicking 12\n",
      "undergoing 12\n",
      "organ 12\n",
      "twins 12\n",
      "pops 12\n",
      "loosely 12\n",
      "richardson 12\n",
      "pirates 12\n",
      "strokes 12\n",
      "honestly 12\n",
      "enabled 12\n",
      "bubble 12\n",
      "monroe 12\n",
      "holidays 12\n",
      "housed 12\n",
      "russell 12\n",
      "nerve 12\n",
      "herbert 12\n",
      "peasants 12\n",
      "dies 12\n",
      "decides 12\n",
      "restaurants 12\n",
      "poised 12\n",
      "tourists 12\n",
      "winchester 12\n",
      "ken 12\n",
      "earl 12\n",
      "entertaining 12\n",
      "richmond 12\n",
      "silk 12\n",
      "maids 12\n",
      "cab 12\n",
      "tires 12\n",
      "plow 12\n",
      "blades 12\n",
      "layer 12\n",
      "benjamin 12\n",
      "navy's 12\n",
      "presumed 12\n",
      "currency 12\n",
      "indictment 12\n",
      "wealthy 12\n",
      "icy 12\n",
      "salesman 12\n",
      "grandfather 12\n",
      "youthful 12\n",
      "fulfillment 12\n",
      "bloom 12\n",
      "scratching 12\n",
      "bryan 12\n",
      "consultant 12\n",
      "employ 12\n",
      "theatrical 12\n",
      "attendance 12\n",
      "skies 12\n",
      "acknowledged 12\n",
      "beds 12\n",
      "riders 12\n",
      "freed 12\n",
      "cane 12\n",
      "crashed 12\n",
      "manned 12\n",
      "halted 12\n",
      "challenging 12\n",
      "switzerland 12\n",
      "lately 12\n",
      "realizing 12\n",
      "world-wide 12\n",
      "maker 12\n",
      "boil 12\n",
      "durable 12\n",
      "extends 12\n",
      "gasoline 12\n",
      "incentive 12\n",
      "mineral 12\n",
      "deduction 12\n",
      "liquidation 12\n",
      "harvest 12\n",
      "1943 12\n",
      "bake 12\n",
      "floors 12\n",
      "railway 12\n",
      "foliage 12\n",
      "compact 12\n",
      "cooper 12\n",
      "hazard 12\n",
      "supposedly 12\n",
      "distinctly 12\n",
      "pray 12\n",
      "titled 12\n",
      "tank 12\n",
      "downstairs 12\n",
      "laying 12\n",
      "churchill 12\n",
      "summit 12\n",
      "temper 12\n",
      "stereotype 12\n",
      "tenure 12\n",
      "equality 12\n",
      "accounting 12\n",
      "convert 12\n",
      "reacted 12\n",
      "sorts 12\n",
      "ominous 12\n",
      "turmoil 12\n",
      "counting 12\n",
      "guerrilla 12\n",
      "roberts' 12\n",
      "pearson 12\n",
      "borders 12\n",
      "successive 12\n",
      "couch 12\n",
      "recalls 12\n",
      "daring 12\n",
      "statesman 12\n",
      "interviewed 12\n",
      "lasted 12\n",
      "gripped 12\n",
      "tribes 12\n",
      "congolese 12\n",
      "tin 12\n",
      "instituted 12\n",
      "lumumba 12\n",
      "provinces 12\n",
      "mercenaries 12\n",
      "resort 12\n",
      "raced 12\n",
      "requested 12\n",
      "ceased 12\n",
      "populated 12\n",
      "everyday 12\n",
      "teen-agers 12\n",
      "soviets 12\n",
      "deserve 12\n",
      "trujillo 12\n",
      "deserved 12\n",
      "maria 12\n",
      "stray 12\n",
      "settlers 12\n",
      "nod 12\n",
      "suspension 12\n",
      "yielded 12\n",
      "compounded 12\n",
      "grey 12\n",
      "debts 12\n",
      "prayed 12\n",
      "clergy 12\n",
      "floating 12\n",
      "smoothly 12\n",
      "adjacent 12\n",
      "furiously 12\n",
      "favorites 12\n",
      "sizes 12\n",
      "korea 12\n",
      "baptized 12\n",
      "sermon 12\n",
      "stereo 12\n",
      "fifties 12\n",
      "louder 12\n",
      "hairs 12\n",
      "lousy 12\n",
      "actively 12\n",
      "rockefeller 12\n",
      "faithful 12\n",
      "proceeding 12\n",
      "stubborn 12\n",
      "ardent 12\n",
      "temptation 12\n",
      "dialogue 12\n",
      "functioning 12\n",
      "purity 12\n",
      "snap 12\n",
      "foremost 12\n",
      "nowadays 12\n",
      "absorption 12\n",
      "adolescent 12\n",
      "20th 12\n",
      "swallowed 12\n",
      "solemn 12\n",
      "coincide 12\n",
      "paragraph 12\n",
      "rhythms 12\n",
      "garry 12\n",
      "poles 12\n",
      "resultant 12\n",
      "traced 12\n",
      "ignorant 12\n",
      "lyric 12\n",
      "shelley 12\n",
      "promoted 12\n",
      "passionate 12\n",
      "sadly 12\n",
      "acquainted 12\n",
      "nazis 12\n",
      "fox 12\n",
      "joel 12\n",
      "shelf 12\n",
      "collaboration 12\n",
      "beethoven 12\n",
      "attributes 12\n",
      "impulses 12\n",
      "sixteenth 12\n",
      "fashionable 12\n",
      "consciously 12\n",
      "sail 12\n",
      "neglect 12\n",
      "saloon 12\n",
      "interpretations 12\n",
      "logically 12\n",
      "contraction 12\n",
      "intellectuals 12\n",
      "sansom 12\n",
      "explore 12\n",
      "monsieur 12\n",
      "cooler 12\n",
      "trailers 12\n",
      "eaten 12\n",
      "insoluble 12\n",
      "quaint 12\n",
      "irony 12\n",
      "blindness 12\n",
      "evidenced 12\n",
      "someday 12\n",
      "thy 12\n",
      "honorable 12\n",
      "bits 12\n",
      "passions 12\n",
      "lord's 12\n",
      "defenses 12\n",
      "pleading 12\n",
      "yang 12\n",
      "initiated 12\n",
      "cultures 12\n",
      "calculation 12\n",
      "affirm 12\n",
      "aspirations 12\n",
      "privacy 12\n",
      "luminous 12\n",
      "anti-slavery 12\n",
      "magical 12\n",
      "perceived 12\n",
      "whisper 12\n",
      "landlord 12\n",
      "porous 12\n",
      "leaf 12\n",
      "egg 12\n",
      "vessels 12\n",
      "hardened 12\n",
      "atlas 12\n",
      "infrared 12\n",
      "ninety 12\n",
      "combustion 12\n",
      "tar 12\n",
      "trot 12\n",
      "2:36 12\n",
      "trigger 12\n",
      "gauge 12\n",
      "kid's 12\n",
      "inspect 12\n",
      "grill 12\n",
      "villages 12\n",
      "canyon 12\n",
      "meats 12\n",
      "cavity 12\n",
      "antique 12\n",
      "construct 12\n",
      "butt 12\n",
      "tilted 12\n",
      "installation 12\n",
      "comfortably 12\n",
      "pretending 12\n",
      "overall 12\n",
      "stones 12\n",
      "max 12\n",
      "independently 12\n",
      "watercolor 12\n",
      "climb 12\n",
      "microorganisms 12\n",
      "nitrogen 12\n",
      "cm. 12\n",
      "detected 12\n",
      "lens 12\n",
      "vapor 12\n",
      "sodium 12\n",
      "elevator 12\n",
      "synthetic 12\n",
      "carryover 12\n",
      "tri-state 12\n",
      "discount 12\n",
      "molded 12\n",
      "therapy 12\n",
      "gaze 12\n",
      "dolls 12\n",
      "phony 12\n",
      "dentist 12\n",
      "retention 12\n",
      "deciding 12\n",
      "selkirk 12\n",
      "stockade 12\n",
      "torrio 12\n",
      "murders 12\n",
      "jerked 12\n",
      "crest 12\n",
      "swollen 12\n",
      "7th 12\n",
      "violently 12\n",
      "coatings 12\n",
      "unlocked 12\n",
      "processed 12\n",
      "dill 12\n",
      "blew 12\n",
      "unwed 12\n",
      "palazzo 12\n",
      "detached 12\n",
      "persians 12\n",
      "attendant 12\n",
      "ptolemaic 12\n",
      "metaphysics 12\n",
      "mimesis 12\n",
      "individualism 12\n",
      "gabriel 12\n",
      "cigarettes 12\n",
      "steele's 12\n",
      "hetman 12\n",
      "wrinkled 12\n",
      "trevelyan's 12\n",
      "demographic 12\n",
      "howe 12\n",
      "meltzer 12\n",
      "state-owned 12\n",
      "cm 12\n",
      "terminal 12\n",
      "unadjusted 12\n",
      "hollow 12\n",
      "dimly 12\n",
      "104 12\n",
      "deduct 12\n",
      "antibody 12\n",
      "dialysis 12\n",
      "fluorescence 12\n",
      "antigen 12\n",
      "nilpotent 12\n",
      "intersections 12\n",
      "secant 12\n",
      "unstructured 12\n",
      "transferor 12\n",
      "mexicans 12\n",
      "formulaic 12\n",
      "frowning 12\n",
      "bastard 12\n",
      "chris 12\n",
      "nigger 12\n",
      "limp 12\n",
      "purdew 12\n",
      "andrus 12\n",
      "mcfeeley 12\n",
      "brassnose 12\n",
      "delphine 12\n",
      "feathertop 12\n",
      "cappy 12\n",
      "investigate 11\n",
      "interim 11\n",
      "criticisms 11\n",
      "equitable 11\n",
      "chambers 11\n",
      "reconstruction 11\n",
      "delegation 11\n",
      "protests 11\n",
      "rejection 11\n",
      "taxation 11\n",
      "unanimously 11\n",
      "paso 11\n",
      "gifts 11\n",
      "$1,000 11\n",
      "adverse 11\n",
      "250 11\n",
      "teaches 11\n",
      "portions 11\n",
      "constituted 11\n",
      "90 11\n",
      "eisenhower's 11\n",
      "noticeable 11\n",
      "preoccupied 11\n",
      "fulfilled 11\n",
      "concentrate 11\n",
      "respected 11\n",
      "kremlin 11\n",
      "strenuous 11\n",
      "aided 11\n",
      "doubled 11\n",
      "locally 11\n",
      "declares 11\n",
      "squeeze 11\n",
      "misunderstanding 11\n",
      "michael 11\n",
      "regretted 11\n",
      "proven 11\n",
      "graduation 11\n",
      "plowing 11\n",
      "adoption 11\n",
      "edwin 11\n",
      "contrasting 11\n",
      "sometime 11\n",
      "conceded 11\n",
      "41 11\n",
      "1942 11\n",
      "1920 11\n",
      "marines 11\n",
      "zone 11\n",
      "script 11\n",
      "ross 11\n",
      "barnett 11\n",
      "honeymoon 11\n",
      "exert 11\n",
      "elevated 11\n",
      "architecture 11\n",
      "cooperate 11\n",
      "disagreement 11\n",
      "repaired 11\n",
      "1000 11\n",
      "corp. 11\n",
      "economically 11\n",
      "plea 11\n",
      "oregon 11\n",
      "assemblies 11\n",
      "scriptures 11\n",
      "forthcoming 11\n",
      "1,000 11\n",
      "orioles 11\n",
      "mound 11\n",
      "albany 11\n",
      "posted 11\n",
      "enlisted 11\n",
      "swelling 11\n",
      "consult 11\n",
      "tony 11\n",
      "moritz 11\n",
      "timing 11\n",
      "nailed 11\n",
      "shaken 11\n",
      "cliff 11\n",
      "compartment 11\n",
      "willie 11\n",
      "injuries 11\n",
      "retain 11\n",
      "hogan 11\n",
      "perfection 11\n",
      "thirteen 11\n",
      "ye 11\n",
      "lengthy 11\n",
      "blond 11\n",
      "happier 11\n",
      "hong 11\n",
      "kong 11\n",
      "drum 11\n",
      "towels 11\n",
      "proprietor 11\n",
      "presbyterian 11\n",
      "mortar 11\n",
      "freeman 11\n",
      "walnut 11\n",
      "sculpture 11\n",
      "entertained 11\n",
      "easter 11\n",
      "array 11\n",
      "drastic 11\n",
      "loading 11\n",
      "necessitated 11\n",
      "38 11\n",
      "poorly 11\n",
      "dodge 11\n",
      "choosing 11\n",
      "hardware 11\n",
      "comprise 11\n",
      "testified 11\n",
      "dots 11\n",
      "grimly 11\n",
      "youths 11\n",
      "pohl 11\n",
      "subdivision 11\n",
      "del 11\n",
      "criteria 11\n",
      "integrated 11\n",
      "theaters 11\n",
      "transfers 11\n",
      "treating 11\n",
      "claude 11\n",
      "unusually 11\n",
      "jimmy 11\n",
      "hillsboro 11\n",
      "showmanship 11\n",
      "rabbit 11\n",
      "poultry 11\n",
      "mill 11\n",
      "tide 11\n",
      "ribs 11\n",
      "drifting 11\n",
      "alternate 11\n",
      "incidents 11\n",
      "wilderness 11\n",
      "disarmament 11\n",
      "sanitation 11\n",
      "coincidence 11\n",
      "1930 11\n",
      "prompt 11\n",
      "threaten 11\n",
      "dillon 11\n",
      "satisfactorily 11\n",
      "discrepancy 11\n",
      "climbing 11\n",
      "paradoxically 11\n",
      "acreage 11\n",
      "planted 11\n",
      "deductions 11\n",
      "taxpayer 11\n",
      "warn 11\n",
      "entities 11\n",
      "descent 11\n",
      "prejudice 11\n",
      "archaeology 11\n",
      "buys 11\n",
      "breast 11\n",
      "juice 11\n",
      "arched 11\n",
      "wells 11\n",
      "glaze 11\n",
      "midwest 11\n",
      "violin 11\n",
      "here's 11\n",
      "wired 11\n",
      "appreciated 11\n",
      "phillips 11\n",
      "frantic 11\n",
      "reunion 11\n",
      "marking 11\n",
      "wonderfully 11\n",
      "fixing 11\n",
      "toys 11\n",
      "towering 11\n",
      "mounting 11\n",
      "dozens 11\n",
      "tomb 11\n",
      "catastrophe 11\n",
      "shifting 11\n",
      "smiles 11\n",
      "eventual 11\n",
      "willingness 11\n",
      "expectation 11\n",
      "interpret 11\n",
      "unified 11\n",
      "parish 11\n",
      "generated 11\n",
      "trivial 11\n",
      "usefulness 11\n",
      "nominal 11\n",
      "unprecedented 11\n",
      "squarely 11\n",
      "manufactured 11\n",
      "artillery 11\n",
      "carriers 11\n",
      "embarrassing 11\n",
      "restrict 11\n",
      "presidency 11\n",
      "recover 11\n",
      "moist 11\n",
      "unquestionably 11\n",
      "settling 11\n",
      "dash 11\n",
      "australia 11\n",
      "accumulation 11\n",
      "undergraduate 11\n",
      "trusted 11\n",
      "infant 11\n",
      "quo 11\n",
      "airplane 11\n",
      "subsequently 11\n",
      "streams 11\n",
      "deficiency 11\n",
      "exports 11\n",
      "limiting 11\n",
      "cheaper 11\n",
      "twisting 11\n",
      "contrasts 11\n",
      "nucleus 11\n",
      "develops 11\n",
      "restraint 11\n",
      "render 11\n",
      "tyranny 11\n",
      "symbolized 11\n",
      "awkward 11\n",
      "sanction 11\n",
      "incapable 11\n",
      "commanding 11\n",
      "secured 11\n",
      "coexistence 11\n",
      "overt 11\n",
      "hardest 11\n",
      "compilation 11\n",
      "politically 11\n",
      "tropical 11\n",
      "underneath 11\n",
      "commissions 11\n",
      "restoration 11\n",
      "assistants 11\n",
      "sliding 11\n",
      "spaces 11\n",
      "preserves 11\n",
      "depressed 11\n",
      "vacant 11\n",
      "confess 11\n",
      "haven 11\n",
      "dominance 11\n",
      "maxwell 11\n",
      "beneficial 11\n",
      "blankets 11\n",
      "publishers 11\n",
      "calf 11\n",
      "1941 11\n",
      "sprawled 11\n",
      "beg 11\n",
      "frighten 11\n",
      "rhythmic 11\n",
      "shaft 11\n",
      "eloquent 11\n",
      "natives 11\n",
      "conceivable 11\n",
      "bronze 11\n",
      "korean 11\n",
      "instrumental 11\n",
      "chronic 11\n",
      "recordings 11\n",
      "memorable 11\n",
      "anticipate 11\n",
      "oriented 11\n",
      "hampshire 11\n",
      "facility 11\n",
      "frustration 11\n",
      "distorted 11\n",
      "agrees 11\n",
      "appropriated 11\n",
      "pollen 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afforded 11\n",
      "leisure 11\n",
      "strained 11\n",
      "wholesome 11\n",
      "ghost 11\n",
      "hiroshima 11\n",
      "greatness 11\n",
      "rake 11\n",
      "cult 11\n",
      "civilized 11\n",
      "census 11\n",
      "curse 11\n",
      "dose 11\n",
      "pose 11\n",
      "expenditure 11\n",
      "mainland 11\n",
      "invite 11\n",
      "introduce 11\n",
      "constitutes 11\n",
      "intelligible 11\n",
      "curiously 11\n",
      "muffled 11\n",
      "rotation 11\n",
      "rejects 11\n",
      "criterion 11\n",
      "convincing 11\n",
      "preacher 11\n",
      "concerto 11\n",
      "marvelous 11\n",
      "venus 11\n",
      "rendering 11\n",
      "ensemble 11\n",
      "disciplined 11\n",
      "bands 11\n",
      "sleeve 11\n",
      "baroque 11\n",
      "amazed 11\n",
      "philosophic 11\n",
      "herald 11\n",
      "waking 11\n",
      "childish 11\n",
      "narrator 11\n",
      "partial 11\n",
      "keen 11\n",
      "agreeable 11\n",
      "explored 11\n",
      "energetic 11\n",
      "quantities 11\n",
      "fatigue 11\n",
      "sociological 11\n",
      "ash 11\n",
      "electrons 11\n",
      "shakespeare's 11\n",
      "barbed 11\n",
      "sigh 11\n",
      "shed 11\n",
      "appetite 11\n",
      "varies 11\n",
      "schoolhouse 11\n",
      "possessions 11\n",
      "tender 11\n",
      "strict 11\n",
      "passive 11\n",
      "tear 11\n",
      "pituitary 11\n",
      "boredom 11\n",
      "calcium 11\n",
      "framework 11\n",
      "epidemic 11\n",
      "elemental 11\n",
      "grasped 11\n",
      "apprehension 11\n",
      "formulations 11\n",
      "indifferent 11\n",
      "anglican 11\n",
      "communion 11\n",
      "rolls 11\n",
      "protestantism 11\n",
      "endured 11\n",
      "formulated 11\n",
      "conclusive 11\n",
      "histories 11\n",
      "disguised 11\n",
      "shivering 11\n",
      "enclosed 11\n",
      "uniformity 11\n",
      "finite 11\n",
      "thoughtful 11\n",
      "psychologists 11\n",
      "arising 11\n",
      "burial 11\n",
      "mahayana 11\n",
      "unexpectedly 11\n",
      "cursed 11\n",
      "spit 11\n",
      "weights 11\n",
      "improves 11\n",
      "duration 11\n",
      "locating 11\n",
      "trailer 11\n",
      "infancy 11\n",
      "commercially 11\n",
      "angles 11\n",
      "flush 11\n",
      "rotating 11\n",
      "computing 11\n",
      "paced 11\n",
      "duke 11\n",
      "magnum 11\n",
      "wart 11\n",
      "pump 11\n",
      "complain 11\n",
      "photographic 11\n",
      "ferry 11\n",
      "seventeenth 11\n",
      "vases 11\n",
      "needless 11\n",
      "cement 11\n",
      "specify 11\n",
      "obtainable 11\n",
      "compound 11\n",
      "adjusting 11\n",
      "dimensional 11\n",
      "timothy 11\n",
      "eccentric 11\n",
      "dreaming 11\n",
      "pavement 11\n",
      "furnace 11\n",
      "eighty 11\n",
      "gymnastics 11\n",
      "meters 11\n",
      "disappear 11\n",
      "enzymes 11\n",
      "evaluated 11\n",
      "occupational 11\n",
      "treatments 11\n",
      "adopting 11\n",
      "acrylic 11\n",
      "linguists 11\n",
      "revealing 11\n",
      "ambiguity 11\n",
      "hideous 11\n",
      "snatched 11\n",
      "postwar 11\n",
      "pulley 11\n",
      "foregoing 11\n",
      "individual's 11\n",
      "invested 11\n",
      "surgeon 11\n",
      "avoiding 11\n",
      "cohesive 11\n",
      "disliked 11\n",
      "liking 11\n",
      "spinning 11\n",
      "counterparts 11\n",
      "sexes 11\n",
      "rubbing 11\n",
      "bee 11\n",
      "waist 11\n",
      "acquiring 11\n",
      "calmly 11\n",
      "mckinley 11\n",
      "log 11\n",
      "winslow 11\n",
      "spelman 11\n",
      "motivation 11\n",
      "reef 11\n",
      "thereto 11\n",
      "ghetto 11\n",
      "posse 11\n",
      "crawl 11\n",
      "militia 11\n",
      "crept 11\n",
      "energies 11\n",
      "approximate 11\n",
      "boulevard 11\n",
      "dylan 11\n",
      "hub 11\n",
      "szold 11\n",
      "jastrow 11\n",
      "taliesin 11\n",
      "jack's 11\n",
      "outcomes 11\n",
      "kehl 11\n",
      "plymouth 11\n",
      "carriage 11\n",
      "calendars 11\n",
      "cylindrical 11\n",
      "ml 11\n",
      "inorganic 11\n",
      "mg. 11\n",
      "autonomic 11\n",
      "congruence 11\n",
      "cubism 11\n",
      "brumidi 11\n",
      "odyssey 11\n",
      "aerator 11\n",
      "scotty's 11\n",
      "goddamn 11\n",
      "vince 11\n",
      "rev 11\n",
      "mahzeer 11\n",
      "hesperus 11\n",
      "mcbride 11\n",
      "quint 11\n",
      "vivian 11\n",
      "viola 11\n",
      "crombie 11\n",
      "topics 10\n",
      "71 10\n",
      "1937 10\n",
      "rally 10\n",
      "allotted 10\n",
      "congressmen 10\n",
      "polls 10\n",
      "senators 10\n",
      "miscellaneous 10\n",
      "alleged 10\n",
      "semester 10\n",
      "chester 10\n",
      "subsistence 10\n",
      "preserving 10\n",
      "defendants 10\n",
      "goldberg 10\n",
      "diagnostic 10\n",
      "prediction 10\n",
      "consultation 10\n",
      "underdeveloped 10\n",
      "aggression 10\n",
      "tougher 10\n",
      "restrain 10\n",
      "bloc 10\n",
      "asian 10\n",
      "pathet 10\n",
      "multiply 10\n",
      "coordinate 10\n",
      "hawksley 10\n",
      "riverside 10\n",
      "nationally 10\n",
      "rotary 10\n",
      "favors 10\n",
      "counseling 10\n",
      "sewer 10\n",
      "simplest 10\n",
      "dumont 10\n",
      "forum 10\n",
      "decency 10\n",
      "36 10\n",
      "elephants 10\n",
      "mates 10\n",
      "slate 10\n",
      "queens 10\n",
      "revulsion 10\n",
      "norm 10\n",
      "waterfront 10\n",
      "gordon 10\n",
      "unite 10\n",
      "princess 10\n",
      "viewing 10\n",
      "representations 10\n",
      "patronage 10\n",
      "baton 10\n",
      "segregation 10\n",
      "frustrated 10\n",
      "token 10\n",
      "confronting 10\n",
      "reviewing 10\n",
      "pratt 10\n",
      "48 10\n",
      "leonard 10\n",
      "replies 10\n",
      "hazards 10\n",
      "acre 10\n",
      "sectors 10\n",
      "seeks 10\n",
      "barbara 10\n",
      "consecutive 10\n",
      "brandt 10\n",
      "denying 10\n",
      "fla. 10\n",
      "deadlock 10\n",
      "stole 10\n",
      "singled 10\n",
      "flock 10\n",
      "streak 10\n",
      "hansen 10\n",
      "champions 10\n",
      "richards 10\n",
      "halfback 10\n",
      "hip 10\n",
      "slaughter 10\n",
      "rushing 10\n",
      "insists 10\n",
      "meek 10\n",
      "tackle 10\n",
      "ramsey 10\n",
      "averages 10\n",
      "warmed 10\n",
      "masterpiece 10\n",
      "rocky 10\n",
      "lifetime 10\n",
      "privileged 10\n",
      "slashed 10\n",
      "skorich 10\n",
      "shea 10\n",
      "lease 10\n",
      "sands 10\n",
      "abandonment 10\n",
      "margin 10\n",
      "majestic 10\n",
      "bounds 10\n",
      "competitors 10\n",
      "sore 10\n",
      "compiled 10\n",
      "professionals 10\n",
      "adds 10\n",
      "ringing 10\n",
      "bushes 10\n",
      "whirling 10\n",
      "obstacle 10\n",
      "chicago's 10\n",
      "wheeled 10\n",
      "typewriter 10\n",
      "she'll 10\n",
      "schedules 10\n",
      "bleak 10\n",
      "reared 10\n",
      "tastes 10\n",
      "ellen 10\n",
      "conrad 10\n",
      "deposited 10\n",
      "feminine 10\n",
      "newman 10\n",
      "arrange 10\n",
      "sponsors 10\n",
      "evans 10\n",
      "drexel 10\n",
      "latter's 10\n",
      "margaret 10\n",
      "eleanor 10\n",
      "principally 10\n",
      "robbery 10\n",
      "cites 10\n",
      "summoned 10\n",
      "deficiencies 10\n",
      "chains 10\n",
      "lloyd 10\n",
      "university's 10\n",
      "guiding 10\n",
      "implication 10\n",
      "magnification 10\n",
      "clients 10\n",
      "polaris 10\n",
      "robbed 10\n",
      "1935 10\n",
      "misfortune 10\n",
      "fleeing 10\n",
      "privileges 10\n",
      "se 10\n",
      "likelihood 10\n",
      "negotiate 10\n",
      "72 10\n",
      "emma 10\n",
      "disturbance 10\n",
      "expressway 10\n",
      "scottish 10\n",
      "exhibited 10\n",
      "cooked 10\n",
      "theft 10\n",
      "persian 10\n",
      "sewing 10\n",
      "founder 10\n",
      "girl's 10\n",
      "urging 10\n",
      "halt 10\n",
      "serene 10\n",
      "mob 10\n",
      "radioactive 10\n",
      "huff 10\n",
      "export 10\n",
      "stacy 10\n",
      "segment 10\n",
      "mercury 10\n",
      "thesis 10\n",
      "declaring 10\n",
      "convertible 10\n",
      "merge 10\n",
      "forecast 10\n",
      "distributions 10\n",
      "glowing 10\n",
      "sunrise 10\n",
      "steak 10\n",
      "monks 10\n",
      "tops 10\n",
      "wool 10\n",
      "finals 10\n",
      "1917 10\n",
      "rides 10\n",
      "tune 10\n",
      "undergone 10\n",
      "mouse 10\n",
      "abolition 10\n",
      "isaac 10\n",
      "romans 10\n",
      "unfamiliar 10\n",
      "politely 10\n",
      "hail 10\n",
      "tours 10\n",
      "85 10\n",
      "1859 10\n",
      "journalist 10\n",
      "ideally 10\n",
      "veto 10\n",
      "conversations 10\n",
      "adaptation 10\n",
      "owe 10\n",
      "uniquely 10\n",
      "mortal 10\n",
      "spreads 10\n",
      "knit 10\n",
      "reservoir 10\n",
      "begging 10\n",
      "accuse 10\n",
      "unconsciously 10\n",
      "gigantic 10\n",
      "bags 10\n",
      "combining 10\n",
      "pious 10\n",
      "assuring 10\n",
      "rusk 10\n",
      "cautious 10\n",
      "constituents 10\n",
      "mister 10\n",
      "dreadful 10\n",
      "pork 10\n",
      "traditionally 10\n",
      "presiding 10\n",
      "licked 10\n",
      "bite 10\n",
      "amazement 10\n",
      "herman 10\n",
      "enjoys 10\n",
      "sandwich 10\n",
      "crushed 10\n",
      "zinc 10\n",
      "prostitution 10\n",
      "hunt 10\n",
      "57 10\n",
      "recruit 10\n",
      "ranges 10\n",
      "ration 10\n",
      "puzzle 10\n",
      "sweden 10\n",
      "secede 10\n",
      "immigration 10\n",
      "luggage 10\n",
      "slug 10\n",
      "simmons 10\n",
      "turnpikes 10\n",
      "risen 10\n",
      "wales 10\n",
      "lovers 10\n",
      "recognizes 10\n",
      "specialization 10\n",
      "balloon 10\n",
      "echo 10\n",
      "presenting 10\n",
      "recognizing 10\n",
      "seas 10\n",
      "commercials 10\n",
      "distilled 10\n",
      "ads 10\n",
      "occurrences 10\n",
      "singer 10\n",
      "patriotic 10\n",
      "layers 10\n",
      "applicants 10\n",
      "disturb 10\n",
      "johnson's 10\n",
      "integrity 10\n",
      "arguing 10\n",
      "diminished 10\n",
      "admiration 10\n",
      "clues 10\n",
      "conscientious 10\n",
      "loads 10\n",
      "honesty 10\n",
      "conform 10\n",
      "pleasantly 10\n",
      "impatience 10\n",
      "thumb 10\n",
      "extensively 10\n",
      "limitation 10\n",
      "tremendously 10\n",
      "compulsive 10\n",
      "gazette 10\n",
      "looming 10\n",
      "warsaw 10\n",
      "brother's 10\n",
      "hello 10\n",
      "managerial 10\n",
      "descending 10\n",
      "blessing 10\n",
      "discoveries 10\n",
      "hebrew 10\n",
      "buttons 10\n",
      "paste 10\n",
      "bull's-eye 10\n",
      "perfume 10\n",
      "akin 10\n",
      "admire 10\n",
      "editorials 10\n",
      "disabled 10\n",
      "va 10\n",
      "drastically 10\n",
      "misleading 10\n",
      "entity 10\n",
      "17th 10\n",
      "folly 10\n",
      "refusing 10\n",
      "exceeds 10\n",
      "prevents 10\n",
      "solidly 10\n",
      "utilize 10\n",
      "departing 10\n",
      "tremble 10\n",
      "megatons 10\n",
      "vengeance 10\n",
      "nonetheless 10\n",
      "imperative 10\n",
      "indignant 10\n",
      "projections 10\n",
      "cultivated 10\n",
      "generals 10\n",
      "scars 10\n",
      "dresses 10\n",
      "bolt 10\n",
      "genuinely 10\n",
      "mm. 10\n",
      "conservatism 10\n",
      "rust 10\n",
      "alas 10\n",
      "analyze 10\n",
      "larkin 10\n",
      "longing 10\n",
      "bearded 10\n",
      "vegetable 10\n",
      "museums 10\n",
      "pleasing 10\n",
      "mastery 10\n",
      "dramatically 10\n",
      "twenties 10\n",
      "nickname 10\n",
      "philharmonic 10\n",
      "costume 10\n",
      "admirable 10\n",
      "witty 10\n",
      "awfully 10\n",
      "intensely 10\n",
      "cheerful 10\n",
      "intricate 10\n",
      "reject 10\n",
      "indebted 10\n",
      "dialect 10\n",
      "persisted 10\n",
      "vain 10\n",
      "sings 10\n",
      "hysterical 10\n",
      "oily 10\n",
      "elegance 10\n",
      "editions 10\n",
      "twentieth-century 10\n",
      "fragile 10\n",
      "compositions 10\n",
      "julie 10\n",
      "sensations 10\n",
      "pony 10\n",
      "retrieved 10\n",
      "melodies 10\n",
      "marital 10\n",
      "systematically 10\n",
      "grief 10\n",
      "coarse 10\n",
      "embodiment 10\n",
      "rag 10\n",
      "neurotic 10\n",
      "argues 10\n",
      "danced 10\n",
      "accomplishments 10\n",
      "listener 10\n",
      "prints 10\n",
      "herr 10\n",
      "earthy 10\n",
      "huddled 10\n",
      "devoting 10\n",
      "coin 10\n",
      "framing 10\n",
      "void 10\n",
      "conceivably 10\n",
      "patches 10\n",
      "discourse 10\n",
      "preventing 10\n",
      "vitamins 10\n",
      "k 10\n",
      "undesirable 10\n",
      "enduring 10\n",
      "mornings 10\n",
      "clergyman 10\n",
      "mary's 10\n",
      "immigrants 10\n",
      "poison 10\n",
      "a.d. 10\n",
      "vent 10\n",
      "hints 10\n",
      "diagram 10\n",
      "hesitate 10\n",
      "missionaries 10\n",
      "swallow 10\n",
      "seminary 10\n",
      "patriot 10\n",
      "outward 10\n",
      "contributes 10\n",
      "marina 10\n",
      "blooming 10\n",
      "dig 10\n",
      "spade 10\n",
      "moisture 10\n",
      "avocado 10\n",
      "tasted 10\n",
      "detect 10\n",
      "employing 10\n",
      "doomed 10\n",
      "differs 10\n",
      "non 10\n",
      "wipe 10\n",
      "launch 10\n",
      "rating 10\n",
      "screws 10\n",
      "drilling 10\n",
      "equations 10\n",
      "harness 10\n",
      "muzzle 10\n",
      "boulder 10\n",
      "roast 10\n",
      "boiled 10\n",
      "ernest 10\n",
      "button 10\n",
      "cigar 10\n",
      "forge 10\n",
      "graceful 10\n",
      "mosque 10\n",
      "thermometer 10\n",
      "stains 10\n",
      "coats 10\n",
      "reinforce 10\n",
      "container 10\n",
      "keelson 10\n",
      "mixing 10\n",
      "ingenious 10\n",
      "oxen 10\n",
      "raid 10\n",
      "paints 10\n",
      "screens 10\n",
      "insulation 10\n",
      "negligible 10\n",
      "vastly 10\n",
      "transducer 10\n",
      "gram 10\n",
      "creep 10\n",
      "defining 10\n",
      "educator 10\n",
      "deceased 10\n",
      "tents 10\n",
      "advancement 10\n",
      "cellulose 10\n",
      "three-dimensional 10\n",
      "linguistic 10\n",
      "sailed 10\n",
      "anchored 10\n",
      "weird 10\n",
      "vue 10\n",
      "imagery 10\n",
      "psychologist 10\n",
      "caring 10\n",
      "fertility 10\n",
      "concord 10\n",
      "airplanes 10\n",
      "messenger 10\n",
      "doll 10\n",
      "vagina 10\n",
      "learns 10\n",
      "accumulated 10\n",
      "steichen 10\n",
      "bacon 10\n",
      "impatient 10\n",
      "fake 10\n",
      "jaws 10\n",
      "brodie 10\n",
      "utilized 10\n",
      "duly 10\n",
      "mice 10\n",
      "refrain 10\n",
      "comrades 10\n",
      "legends 10\n",
      "waiter 10\n",
      "calculations 10\n",
      "strode 10\n",
      "scar 10\n",
      "relevance 10\n",
      "identities 10\n",
      "instinctively 10\n",
      "non-violent 10\n",
      "fragments 10\n",
      "swear 10\n",
      "tunnel 10\n",
      "spatial 10\n",
      "segments 10\n",
      "calhoun 10\n",
      "cooperatives 10\n",
      "marijuana 10\n",
      "purified 10\n",
      "scanned 10\n",
      "plotted 10\n",
      "muddy 10\n",
      "lunar 10\n",
      "descriptions 10\n",
      "wrist 10\n",
      "specificity 10\n",
      "assessing 10\n",
      "littlepage 10\n",
      "papa's 10\n",
      "lilly 10\n",
      "ditch 10\n",
      "fosdick 10\n",
      "sturley 10\n",
      "viscosity 10\n",
      "commuter 10\n",
      "allotments 10\n",
      "appendix 10\n",
      "regulus 10\n",
      "cathode 10\n",
      "hard-surface 10\n",
      "anionic 10\n",
      "albumin 10\n",
      "bumblebees 10\n",
      "elbow 10\n",
      "gm. 10\n",
      "conjugate 10\n",
      "polynomials 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commute 10\n",
      "subsystems 10\n",
      "text-form 10\n",
      "phonologic 10\n",
      "istiqlal 10\n",
      "aia 10\n",
      "boris 10\n",
      "kennings 10\n",
      "autocoder 10\n",
      "mg/l 10\n",
      "irradiation 10\n",
      "trig 10\n",
      "majdanek 10\n",
      "pedersen 10\n",
      "rustling 10\n",
      "alex's 10\n",
      "lauren 10\n",
      "holster 10\n",
      "lucien 10\n",
      "meeker 10\n",
      "montero 10\n",
      "brenner 10\n",
      "carmer 10\n",
      "todman 10\n",
      "greg's 10\n",
      "eromonga 10\n",
      "georgia's 9\n",
      "clerical 9\n",
      "proportionate 9\n",
      "periodic 9\n",
      "pearl 9\n",
      "berry 9\n",
      "attorneys 9\n",
      "mayor's 9\n",
      "savannah 9\n",
      "resigned 9\n",
      "garland 9\n",
      "byrd 9\n",
      "marvin 9\n",
      "outright 9\n",
      "reportedly 9\n",
      "contention 9\n",
      "scholastic 9\n",
      "permitting 9\n",
      "eminent 9\n",
      "domain 9\n",
      "poll 9\n",
      "texans 9\n",
      "150 9\n",
      "retire 9\n",
      "basketball 9\n",
      "100,000 9\n",
      "precipitated 9\n",
      "prosecution 9\n",
      "fundamentally 9\n",
      "illegal 9\n",
      "18th 9\n",
      "precedent 9\n",
      "discourage 9\n",
      "47 9\n",
      "enforce 9\n",
      "nightmare 9\n",
      "unchanged 9\n",
      "manifestations 9\n",
      "hastened 9\n",
      "states' 9\n",
      "solemnly 9\n",
      "surveyed 9\n",
      "secretary's 9\n",
      "timely 9\n",
      "ally 9\n",
      "resumption 9\n",
      "centralized 9\n",
      "assemble 9\n",
      "revisions 9\n",
      "grocery 9\n",
      "obtaining 9\n",
      "ordinance 9\n",
      "addressing 9\n",
      "party's 9\n",
      "harriet 9\n",
      "10-year 9\n",
      "1925 9\n",
      "inn 9\n",
      "municipalities 9\n",
      "voluntarily 9\n",
      "stepping 9\n",
      "induce 9\n",
      "controller 9\n",
      "bronx 9\n",
      "indonesia 9\n",
      "territories 9\n",
      "toss 9\n",
      "bounded 9\n",
      "turkey 9\n",
      "barnes 9\n",
      "specifications 9\n",
      "salter 9\n",
      "athletics 9\n",
      "pennant 9\n",
      "yielding 9\n",
      "rookie 9\n",
      "glove 9\n",
      "milwaukee 9\n",
      "spectator 9\n",
      "skinny 9\n",
      "bulky 9\n",
      "workable 9\n",
      "catching 9\n",
      "filly 9\n",
      "$10,000 9\n",
      "timed 9\n",
      "thigh 9\n",
      "55 9\n",
      "gene 9\n",
      "bud 9\n",
      "flooded 9\n",
      "46 9\n",
      "rulers 9\n",
      "dissatisfaction 9\n",
      "cincinnati 9\n",
      "1924 9\n",
      "shaw 9\n",
      "depended 9\n",
      "bernard 9\n",
      "athlete 9\n",
      "motivated 9\n",
      "delivering 9\n",
      "guarding 9\n",
      "minneapolis 9\n",
      "slower 9\n",
      "vinegar 9\n",
      "larry 9\n",
      "finishing 9\n",
      "club's 9\n",
      "other's 9\n",
      "trio 9\n",
      "doubted 9\n",
      "fifteenth 9\n",
      "towne 9\n",
      "phoenix 9\n",
      "comic 9\n",
      "garson 9\n",
      "deposit 9\n",
      "esther 9\n",
      "sporting 9\n",
      "chef 9\n",
      "scenic 9\n",
      "dinners 9\n",
      "chairmen 9\n",
      "oscar 9\n",
      "pa. 9\n",
      "bermuda 9\n",
      "carnival 9\n",
      "jenkins 9\n",
      "corrected 9\n",
      "manual 9\n",
      "murdered 9\n",
      "terrace 9\n",
      "spy 9\n",
      "designing 9\n",
      "potent 9\n",
      "oddly 9\n",
      "av. 9\n",
      "moses 9\n",
      "reckless 9\n",
      "sped 9\n",
      "hengesbach 9\n",
      "else's 9\n",
      "sidney 9\n",
      "expresses 9\n",
      "businessman 9\n",
      "taxing 9\n",
      "challenged 9\n",
      "paramount 9\n",
      "derives 9\n",
      "occupants 9\n",
      "picket 9\n",
      "attacking 9\n",
      "menace 9\n",
      "humans 9\n",
      "cameras 9\n",
      "fulfill 9\n",
      "handles 9\n",
      "collaborated 9\n",
      "quartet 9\n",
      "thieves 9\n",
      "coins 9\n",
      "tearing 9\n",
      "discharged 9\n",
      "heel 9\n",
      "hose 9\n",
      "escort 9\n",
      "wyoming 9\n",
      "rogers 9\n",
      "sanctuary 9\n",
      "fiedler 9\n",
      "pulse 9\n",
      "roaring 9\n",
      "steering 9\n",
      "steer 9\n",
      "schweitzer 9\n",
      "dread 9\n",
      "acquaintance 9\n",
      "boiling 9\n",
      "executives 9\n",
      "employs 9\n",
      "moss 9\n",
      "cleaner 9\n",
      "3,000 9\n",
      "appropriations 9\n",
      "questionable 9\n",
      "monetary 9\n",
      "consolidation 9\n",
      "narrowed 9\n",
      "battered 9\n",
      "dense 9\n",
      "averaging 9\n",
      "incurred 9\n",
      "tax-free 9\n",
      "2% 9\n",
      "surge 9\n",
      "patrons 9\n",
      "cadillac 9\n",
      "melted 9\n",
      "breasts 9\n",
      "one-half 9\n",
      "facets 9\n",
      "ceramic 9\n",
      "chocolate 9\n",
      "reservations 9\n",
      "hungarian 9\n",
      "hardship 9\n",
      "pilgrimage 9\n",
      "notices 9\n",
      "publisher 9\n",
      "accent 9\n",
      "accord 9\n",
      "rated 9\n",
      "lounge 9\n",
      "pursuing 9\n",
      "format 9\n",
      "retains 9\n",
      "slack 9\n",
      "cologne 9\n",
      "kirov 9\n",
      "spacious 9\n",
      "luis 9\n",
      "unhappily 9\n",
      "furnishings 9\n",
      "miniature 9\n",
      "simultaneous 9\n",
      "confederacy 9\n",
      "aide 9\n",
      "warnings 9\n",
      "exploit 9\n",
      "resembles 9\n",
      "emerges 9\n",
      "attainment 9\n",
      "paradox 9\n",
      "grips 9\n",
      "guarantee 9\n",
      "earning 9\n",
      "indignation 9\n",
      "pp. 9\n",
      "1933 9\n",
      "tortured 9\n",
      "wheat 9\n",
      "antagonism 9\n",
      "consumers 9\n",
      "blunt 9\n",
      "dulles 9\n",
      "numbered 9\n",
      "choke 9\n",
      "scrambled 9\n",
      "closest 9\n",
      "spontaneously 9\n",
      "destined 9\n",
      "locker 9\n",
      "anecdote 9\n",
      "ladies' 9\n",
      "island's 9\n",
      "lazy 9\n",
      "trustee 9\n",
      "groupings 9\n",
      "exploited 9\n",
      "chien 9\n",
      "provincial 9\n",
      "departed 9\n",
      "interfere 9\n",
      "awoke 9\n",
      "volunteer 9\n",
      "labeled 9\n",
      "b-52 9\n",
      "maine 9\n",
      "enables 9\n",
      "industrialized 9\n",
      "pour 9\n",
      "seams 9\n",
      "proportionately 9\n",
      "resource 9\n",
      "classics 9\n",
      "vitally 9\n",
      "await 9\n",
      "spared 9\n",
      "proudly 9\n",
      "coupling 9\n",
      "abundant 9\n",
      "replacing 9\n",
      "recovered 9\n",
      "conquest 9\n",
      "zoo 9\n",
      "ills 9\n",
      "australian 9\n",
      "persuasion 9\n",
      "patiently 9\n",
      "fighter 9\n",
      "conventions 9\n",
      "conflicts 9\n",
      "graves 9\n",
      "unification 9\n",
      "preference 9\n",
      "pouring 9\n",
      "restore 9\n",
      "subordinates 9\n",
      "forecasting 9\n",
      "presses 9\n",
      "unimportant 9\n",
      "stimulating 9\n",
      "gratitude 9\n",
      "beans 9\n",
      "heavenly 9\n",
      "enchanting 9\n",
      "jokes 9\n",
      "conceptions 9\n",
      "woven 9\n",
      "drifted 9\n",
      "withdrew 9\n",
      "tolerant 9\n",
      "cage 9\n",
      "son's 9\n",
      "versions 9\n",
      "winding 9\n",
      "salad 9\n",
      "pineapple 9\n",
      "hierarchy 9\n",
      "freud 9\n",
      "currents 9\n",
      "dazzling 9\n",
      "forbes 9\n",
      "broadening 9\n",
      "plumbing 9\n",
      "verses 9\n",
      "heavens 9\n",
      "toe 9\n",
      "stacked 9\n",
      "tailored 9\n",
      "blend 9\n",
      "cork 9\n",
      "tan 9\n",
      "laboratories 9\n",
      "scratch 9\n",
      "gland 9\n",
      "guides 9\n",
      "arizona 9\n",
      "breathed 9\n",
      "melancholy 9\n",
      "undergraduates 9\n",
      "pants 9\n",
      "panting 9\n",
      "agony 9\n",
      "barrier 9\n",
      "repeating 9\n",
      "additions 9\n",
      "bless 9\n",
      "immensely 9\n",
      "manifest 9\n",
      "sundays 9\n",
      "regret 9\n",
      "advise 9\n",
      "heartily 9\n",
      "well-being 9\n",
      "assessed 9\n",
      "impose 9\n",
      "trades 9\n",
      "adherence 9\n",
      "versus 9\n",
      "inspiration 9\n",
      "hymn 9\n",
      "dictates 9\n",
      "destination 9\n",
      "annoyance 9\n",
      "anatomy 9\n",
      "sweetheart 9\n",
      "augmented 9\n",
      "sixty-five 9\n",
      "guardian 9\n",
      "karl 9\n",
      "40,000 9\n",
      "cynical 9\n",
      "irresponsible 9\n",
      "atmospheric 9\n",
      "wishful 9\n",
      "theologians 9\n",
      "efficacy 9\n",
      "cousins 9\n",
      "engendered 9\n",
      "technically 9\n",
      "superbly 9\n",
      "malaise 9\n",
      "von 9\n",
      "summarized 9\n",
      "comparing 9\n",
      "soloist 9\n",
      "satire 9\n",
      "gracious 9\n",
      "conveyed 9\n",
      "grotesque 9\n",
      "brilliantly 9\n",
      "sonata 9\n",
      "nicely 9\n",
      "marched 9\n",
      "settings 9\n",
      "unsuccessful 9\n",
      "projection 9\n",
      "positively 9\n",
      "colleague 9\n",
      "hint 9\n",
      "cave 9\n",
      "wrath 9\n",
      "eden 9\n",
      "disadvantages 9\n",
      "flashes 9\n",
      "chromatic 9\n",
      "resisted 9\n",
      "tonal 9\n",
      "wicked 9\n",
      "traces 9\n",
      "picturesque 9\n",
      "des 9\n",
      "extremes 9\n",
      "cares 9\n",
      "stack 9\n",
      "indies 9\n",
      "attractions 9\n",
      "loneliness 9\n",
      "blown 9\n",
      "sterile 9\n",
      "skillful 9\n",
      "stretches 9\n",
      "broadcasting 9\n",
      "advertised 9\n",
      "preoccupation 9\n",
      "charts 9\n",
      "wept 9\n",
      "smells 9\n",
      "trifle 9\n",
      "outlet 9\n",
      "introducing 9\n",
      "analytical 9\n",
      "peninsula 9\n",
      "shores 9\n",
      "inward 9\n",
      "nagging 9\n",
      "molecules 9\n",
      "vividly 9\n",
      "demon 9\n",
      "existential 9\n",
      "latent 9\n",
      "unpaid 9\n",
      "formulate 9\n",
      "ecclesiastical 9\n",
      "symbolize 9\n",
      "mortality 9\n",
      "obedience 9\n",
      "warming 9\n",
      "proclaimed 9\n",
      "compressed 9\n",
      "norton 9\n",
      "saviour 9\n",
      "han 9\n",
      "philosophers 9\n",
      "conversely 9\n",
      "evangelism 9\n",
      "postponed 9\n",
      "topic 9\n",
      "denominational 9\n",
      "appalling 9\n",
      "pathological 9\n",
      "buddhism 9\n",
      "tolerance 9\n",
      "toynbee 9\n",
      "accidental 9\n",
      "evils 9\n",
      "alumni 9\n",
      "declining 9\n",
      "metallic 9\n",
      "orbits 9\n",
      "lessened 9\n",
      "disclose 9\n",
      "enrolled 9\n",
      "worldly 9\n",
      "stillness 9\n",
      "haste 9\n",
      "sorrow 9\n",
      "ingredients 9\n",
      "push-pull 9\n",
      "enormously 9\n",
      "weighing 9\n",
      "reconnaissance 9\n",
      "dual 9\n",
      "concentrations 9\n",
      "hauled 9\n",
      "plywood 9\n",
      "carefree 9\n",
      "interlocking 9\n",
      "pulls 9\n",
      "rails 9\n",
      "simplified 9\n",
      "constants 9\n",
      "simplify 9\n",
      "radius 9\n",
      "brood 9\n",
      "hoot 9\n",
      "2:35 9\n",
      "thor 9\n",
      "rack 9\n",
      "smoked 9\n",
      "resin 9\n",
      "draped 9\n",
      "amused 9\n",
      "reins 9\n",
      "wright's 9\n",
      "apple 9\n",
      "grease 9\n",
      "cheese 9\n",
      "1/4 9\n",
      "seam 9\n",
      "bin 9\n",
      "pigment 9\n",
      "quill 9\n",
      "horizontal 9\n",
      "coombs 9\n",
      "ratings 9\n",
      "filter 9\n",
      "offset 9\n",
      "extracted 9\n",
      "etcetera 9\n",
      "swayed 9\n",
      "shaved 9\n",
      "persistence 9\n",
      "beginnings 9\n",
      "illumination 9\n",
      "anatomical 9\n",
      "specially 9\n",
      "borrow 9\n",
      "ritter 9\n",
      "similarity 9\n",
      "electrostatic 9\n",
      "0.1 9\n",
      "0.5 9\n",
      "utilization 9\n",
      "quantitative 9\n",
      "institutional 9\n",
      "vacations 9\n",
      "assigning 9\n",
      "campers 9\n",
      "secretaries 9\n",
      "tenants 9\n",
      "clause 9\n",
      "extruded 9\n",
      "butyrate 9\n",
      "ft. 9\n",
      "carrier 9\n",
      "untouched 9\n",
      "supplementary 9\n",
      "indulge 9\n",
      "indicators 9\n",
      "secrecy 9\n",
      "irregular 9\n",
      "slab 9\n",
      "sensory 9\n",
      "counterpart 9\n",
      "gravel 9\n",
      "hillside 9\n",
      "overcast 9\n",
      "how's 9\n",
      "scrub 9\n",
      "creativity 9\n",
      "intercourse 9\n",
      "anxiously 9\n",
      "searched 9\n",
      "quack 9\n",
      "quacks 9\n",
      "orthodontist 9\n",
      "orthodontic 9\n",
      "admissible 9\n",
      "sullen 9\n",
      "sanctions 9\n",
      "peter's 9\n",
      "homely 9\n",
      "guts 9\n",
      "o'banion's 9\n",
      "earthquake 9\n",
      "earthquakes 9\n",
      "mask 9\n",
      "bouncing 9\n",
      "elders 9\n",
      "ivy 9\n",
      "sticky 9\n",
      "soaking 9\n",
      "palfrey's 9\n",
      "walton 9\n",
      "gavin's 9\n",
      "kirby 9\n",
      "geometry 9\n",
      "denoted 9\n",
      "circled 9\n",
      "yell 9\n",
      "kearton 9\n",
      "peering 9\n",
      "developmental 9\n",
      "grandmother 9\n",
      "condensed 9\n",
      "co-optation 9\n",
      "pelts 9\n",
      "bienville 9\n",
      "puzzling 9\n",
      "therein 9\n",
      "wisman 9\n",
      "pamphlets 9\n",
      "suggestive 9\n",
      "cautiously 9\n",
      "ragged 9\n",
      "mann's 9\n",
      "hammer 9\n",
      "proprietorship 9\n",
      "faulkner's 9\n",
      "badness 9\n",
      "nephew 9\n",
      "generalized 9\n",
      "shann 9\n",
      "reviews 9\n",
      "monkey 9\n",
      "grunted 9\n",
      "perceptions 9\n",
      "predispositions 9\n",
      "symposium 9\n",
      "elimination 9\n",
      "snapping 9\n",
      "redcoats 9\n",
      "othon 9\n",
      "germanic 9\n",
      "invasions 9\n",
      "voltaire 9\n",
      "duck 9\n",
      "ions 9\n",
      "christiana 9\n",
      "rankin 9\n",
      "gonzales 9\n",
      "jupiter 9\n",
      "generators 9\n",
      "douglass 9\n",
      "impurities 9\n",
      "alveolar 9\n",
      "antibodies 9\n",
      "chromatography 9\n",
      "deae-cellulose 9\n",
      "willow 9\n",
      "distal 9\n",
      "bronchioles 9\n",
      "pbs 9\n",
      "discharges 9\n",
      "f{t} 9\n",
      "involution 9\n",
      "kohnstamm-negative 9\n",
      "grammatical 9\n",
      "syllables 9\n",
      "declarative 9\n",
      "consonantal 9\n",
      "morphophonemics 9\n",
      "wagner-peyser 9\n",
      "apportionment 9\n",
      "braque 9\n",
      "aerated 9\n",
      "palatability 9\n",
      "radiopasteurization 9\n",
      "foamed 9\n",
      "suds 9\n",
      "stiffly 9\n",
      "jumping 9\n",
      "prevot 9\n",
      "handkerchief 9\n",
      "steeple 9\n",
      "dumped 9\n",
      "dusk 9\n",
      "randolph 9\n",
      "who'd 9\n",
      "kayabashi 9\n",
      "impatiently 9\n",
      "skiff 9\n",
      "spat 9\n",
      "pastern 9\n",
      "eyebrows 9\n",
      "grosse 9\n",
      "holden 9\n",
      "hohlbein 9\n",
      "docherty 9\n",
      "muller 9\n",
      "dogtown 9\n",
      "jubal 9\n",
      "half-man 9\n",
      "hague 9\n",
      "gran 9\n",
      "roebuck 9\n",
      "schaffner 9\n",
      "elec 9\n",
      "biwa 9\n",
      "partlow 9\n",
      "blatz 9\n",
      "irregularities 8\n",
      "implementation 8\n",
      "outgoing 8\n",
      "court's 8\n",
      "featured 8\n",
      "petitions 8\n",
      "requesting 8\n",
      "lt. 8\n",
      "barber 8\n",
      "undermine 8\n",
      "revision 8\n",
      "clarence 8\n",
      "instructor 8\n",
      "adc 8\n",
      "bellows 8\n",
      "admitting 8\n",
      "prosecutor 8\n",
      "hearings 8\n",
      "precinct 8\n",
      "4th 8\n",
      "scholarships 8\n",
      "1963 8\n",
      "180 8\n",
      "tactical 8\n",
      "deterrent 8\n",
      "inclination 8\n",
      "coping 8\n",
      "revolt 8\n",
      "neutralist 8\n",
      "setup 8\n",
      "cd 8\n",
      "urges 8\n",
      "inspiring 8\n",
      "offense 8\n",
      "resent 8\n",
      "newark 8\n",
      "fbi 8\n",
      "1921 8\n",
      "elect 8\n",
      "feeble 8\n",
      "tangle 8\n",
      "retiring 8\n",
      "corrupt 8\n",
      "selfish 8\n",
      "petty 8\n",
      "rayburn's 8\n",
      "slums 8\n",
      "overly 8\n",
      "succeeds 8\n",
      "merchandising 8\n",
      "inaugural 8\n",
      "monuments 8\n",
      "hoover 8\n",
      "statues 8\n",
      "gazing 8\n",
      "guessing 8\n",
      "predict 8\n",
      "wreck 8\n",
      "la. 8\n",
      "revive 8\n",
      "hemphill 8\n",
      "contracted 8\n",
      "investigating 8\n",
      "managing 8\n",
      "bankruptcy 8\n",
      "travelers 8\n",
      "repairs 8\n",
      "$2 8\n",
      "renewal 8\n",
      "spends 8\n",
      "bites 8\n",
      "julian 8\n",
      "years' 8\n",
      "barred 8\n",
      "mo. 8\n",
      "teamsters 8\n",
      "organizing 8\n",
      "350 8\n",
      "honoring 8\n",
      "hugh 8\n",
      "denomination 8\n",
      "authoritative 8\n",
      "miracles 8\n",
      "super 8\n",
      "church's 8\n",
      "engaging 8\n",
      "fraud 8\n",
      "sullivan 8\n",
      "blows 8\n",
      "pitchers 8\n",
      "barker 8\n",
      "freshman 8\n",
      "sheldon 8\n",
      "colts 8\n",
      "mating 8\n",
      "rosy 8\n",
      "knights 8\n",
      "butcher 8\n",
      "undergo 8\n",
      "week's 8\n",
      "stops 8\n",
      "aerial 8\n",
      "ankle 8\n",
      "tech 8\n",
      "exceptionally 8\n",
      "comforting 8\n",
      "leagues 8\n",
      "charley 8\n",
      "clicked 8\n",
      "traveler 8\n",
      "herb 8\n",
      "slump 8\n",
      "19th 8\n",
      "cardinals 8\n",
      "championship 8\n",
      "babe 8\n",
      "prominently 8\n",
      "winner 8\n",
      "charity 8\n",
      "trophy 8\n",
      "echoes 8\n",
      "heed 8\n",
      "gibson 8\n",
      "flu 8\n",
      "pitched 8\n",
      "contented 8\n",
      "peaks 8\n",
      "resented 8\n",
      "preached 8\n",
      "inviting 8\n",
      "ruth's 8\n",
      "wendell 8\n",
      "mccormick 8\n",
      "niece 8\n",
      "memphis 8\n",
      "carnegie 8\n",
      "generously 8\n",
      "cheer 8\n",
      "stag 8\n",
      "decorations 8\n",
      "testify 8\n",
      "scots 8\n",
      "accommodations 8\n",
      "boast 8\n",
      "rhodes 8\n",
      "veil 8\n",
      "angelo 8\n",
      "festivities 8\n",
      "oysters 8\n",
      "shelves 8\n",
      "wins 8\n",
      "jacques 8\n",
      "ballroom 8\n",
      "hostess 8\n",
      "crimson 8\n",
      "two-story 8\n",
      "divorced 8\n",
      "manuscript 8\n",
      "sentenced 8\n",
      "rumor 8\n",
      "perry 8\n",
      "nominated 8\n",
      "pardon 8\n",
      "offensive 8\n",
      "creed 8\n",
      "bruises 8\n",
      "n.c. 8\n",
      "subdued 8\n",
      "cursing 8\n",
      "murray 8\n",
      "rite 8\n",
      "99 8\n",
      "diamond 8\n",
      "deed 8\n",
      "collections 8\n",
      "shotgun 8\n",
      "decoration 8\n",
      "spraying 8\n",
      "bomber 8\n",
      "middle-aged 8\n",
      "pilots 8\n",
      "abolish 8\n",
      "stupidity 8\n",
      "charging 8\n",
      "1896 8\n",
      "cleaners 8\n",
      "bluntly 8\n",
      "unsatisfactory 8\n",
      "traded 8\n",
      "peterson 8\n",
      "securities 8\n",
      "100% 8\n",
      "dividends 8\n",
      "precarious 8\n",
      "borrowing 8\n",
      "efficiently 8\n",
      "appliances 8\n",
      "sioux 8\n",
      "trailed 8\n",
      "taxable 8\n",
      "termination 8\n",
      "pad 8\n",
      "declare 8\n",
      "seasonal 8\n",
      "cerebral 8\n",
      "wardrobe 8\n",
      "noel 8\n",
      "coward 8\n",
      "supporters 8\n",
      "flour 8\n",
      "danish 8\n",
      "varieties 8\n",
      "decorative 8\n",
      "rooted 8\n",
      "comprised 8\n",
      "baked 8\n",
      "recipe 8\n",
      "practiced 8\n",
      "budapest 8\n",
      "entails 8\n",
      "dismal 8\n",
      "judy 8\n",
      "shrewd 8\n",
      "rusty 8\n",
      "appraisal 8\n",
      "sunshine 8\n",
      "francesca 8\n",
      "kern 8\n",
      "crusade 8\n",
      "anti-communist 8\n",
      "preach 8\n",
      "engagements 8\n",
      "contests 8\n",
      "discarded 8\n",
      "pillow 8\n",
      "headlights 8\n",
      "uncommon 8\n",
      "mansion 8\n",
      "banner 8\n",
      "cathedral 8\n",
      "benches 8\n",
      "wreath 8\n",
      "monotonous 8\n",
      "solving 8\n",
      "outsiders 8\n",
      "encounters 8\n",
      "inauguration 8\n",
      "negotiating 8\n",
      "brethren 8\n",
      "zeal 8\n",
      "duplication 8\n",
      "resembled 8\n",
      "delegate 8\n",
      "caliber 8\n",
      "boycott 8\n",
      "exaggerate 8\n",
      "salaries 8\n",
      "restriction 8\n",
      "disputes 8\n",
      "irrational 8\n",
      "exemption 8\n",
      "1914 8\n",
      "1922 8\n",
      "staffs 8\n",
      "endure 8\n",
      "southward 8\n",
      "confront 8\n",
      "gait 8\n",
      "southerner 8\n",
      "raged 8\n",
      "notorious 8\n",
      "hopefully 8\n",
      "midway 8\n",
      "boldly 8\n",
      "conspicuously 8\n",
      "dominate 8\n",
      "peril 8\n",
      "1938 8\n",
      "concede 8\n",
      "strikingly 8\n",
      "fierce 8\n",
      "weaker 8\n",
      "vines 8\n",
      "confided 8\n",
      "boom 8\n",
      "applicant 8\n",
      "statesmen 8\n",
      "$300 8\n",
      "1908 8\n",
      "commencing 8\n",
      "indefinite 8\n",
      "withdraw 8\n",
      "recruits 8\n",
      "eased 8\n",
      "toll-road 8\n",
      "speculative 8\n",
      "slumped 8\n",
      "extensions 8\n",
      "mouths 8\n",
      "broaden 8\n",
      "catalogue 8\n",
      "borne 8\n",
      "multiplying 8\n",
      "fuller 8\n",
      "governors 8\n",
      "bluff 8\n",
      "deaths 8\n",
      "dispatch 8\n",
      "westward 8\n",
      "bourbon 8\n",
      "sipping 8\n",
      "2,000 8\n",
      "terrain 8\n",
      "militant 8\n",
      "backing 8\n",
      "forceful 8\n",
      "blindly 8\n",
      "humidity 8\n",
      "experimentally 8\n",
      "domes 8\n",
      "momentous 8\n",
      "bloody 8\n",
      "worthwhile 8\n",
      "complaints 8\n",
      "astonishing 8\n",
      "devise 8\n",
      "guarantees 8\n",
      "predictable 8\n",
      "hurricane 8\n",
      "dunes 8\n",
      "impartial 8\n",
      "unstable 8\n",
      "heretofore 8\n",
      "recommending 8\n",
      "straightforward 8\n",
      "junk 8\n",
      "warmly 8\n",
      "wonders 8\n",
      "turtle 8\n",
      "pam 8\n",
      "counters 8\n",
      "establishments 8\n",
      "sprung 8\n",
      "jammed 8\n",
      "bounce 8\n",
      "depicted 8\n",
      "foe 8\n",
      "revival 8\n",
      "essays 8\n",
      "busily 8\n",
      "nostalgia 8\n",
      "youngster 8\n",
      "usable 8\n",
      "oval 8\n",
      "crisp 8\n",
      "brighter 8\n",
      "releases 8\n",
      "financially 8\n",
      "reassurance 8\n",
      "precaution 8\n",
      "sleeves 8\n",
      "wisely 8\n",
      "populations 8\n",
      "sack 8\n",
      "articulate 8\n",
      "thief 8\n",
      "granting 8\n",
      "utilizing 8\n",
      "coldly 8\n",
      "grinding 8\n",
      "clarified 8\n",
      "premises 8\n",
      "slum 8\n",
      "apportioned 8\n",
      "equals 8\n",
      "connally 8\n",
      "reservation 8\n",
      "perilous 8\n",
      "expose 8\n",
      "arithmetic 8\n",
      "alley 8\n",
      "everlasting 8\n",
      "withheld 8\n",
      "focal 8\n",
      "install 8\n",
      "directional 8\n",
      "prohibited 8\n",
      "sentiments 8\n",
      "stamp 8\n",
      "scandal 8\n",
      "exploded 8\n",
      "footsteps 8\n",
      "marx 8\n",
      "qualification 8\n",
      "resemble 8\n",
      "accompaniment 8\n",
      "reminder 8\n",
      "credo 8\n",
      "inescapable 8\n",
      "sane 8\n",
      "catholicism 8\n",
      "archbishop 8\n",
      "irresistible 8\n",
      "spirited 8\n",
      "interpreter 8\n",
      "sympathies 8\n",
      "discontent 8\n",
      "ensure 8\n",
      "residue 8\n",
      "concealed 8\n",
      "removing 8\n",
      "kills 8\n",
      "imported 8\n",
      "merry 8\n",
      "saloons 8\n",
      "ionic 8\n",
      "ancestry 8\n",
      "sailors 8\n",
      "evolved 8\n",
      "strangely 8\n",
      "pathetic 8\n",
      "plots 8\n",
      "maneuvers 8\n",
      "chooses 8\n",
      "reciprocal 8\n",
      "wounds 8\n",
      "twenty-two 8\n",
      "compelling 8\n",
      "sophistication 8\n",
      "half-hour 8\n",
      "absurdity 8\n",
      "minus 8\n",
      "cushion 8\n",
      "lucia 8\n",
      "cooks 8\n",
      "relish 8\n",
      "1920's 8\n",
      "interruption 8\n",
      "moods 8\n",
      "potentialities 8\n",
      "authenticity 8\n",
      "architectural 8\n",
      "recital 8\n",
      "op. 8\n",
      "careless 8\n",
      "choir 8\n",
      "impassioned 8\n",
      "rousing 8\n",
      "sensibility 8\n",
      "importantly 8\n",
      "seriousness 8\n",
      "scholarly 8\n",
      "concluding 8\n",
      "hitler 8\n",
      "themes 8\n",
      "characterization 8\n",
      "possesses 8\n",
      "architects 8\n",
      "dashed 8\n",
      "enemy's 8\n",
      "glancing 8\n",
      "deeds 8\n",
      "equivalents 8\n",
      "wander 8\n",
      "deprived 8\n",
      "ballad 8\n",
      "sticking 8\n",
      "belts 8\n",
      "peers 8\n",
      "haunting 8\n",
      "reminds 8\n",
      "thrusting 8\n",
      "observes 8\n",
      "tricks 8\n",
      "eh 8\n",
      "lone 8\n",
      "profoundly 8\n",
      "marches 8\n",
      "600 8\n",
      "assimilation 8\n",
      "supplements 8\n",
      "canvases 8\n",
      "penetrated 8\n",
      "kindly 8\n",
      "flair 8\n",
      "satellite 8\n",
      "peer 8\n",
      "dwell 8\n",
      "mingled 8\n",
      "motif 8\n",
      "foreseen 8\n",
      "weeping 8\n",
      "creeping 8\n",
      "gracefully 8\n",
      "stravinsky 8\n",
      "commentary 8\n",
      "pills 8\n",
      "logs 8\n",
      "lakes 8\n",
      "liberties 8\n",
      "faulty 8\n",
      "stumbling 8\n",
      "relied 8\n",
      "spoilage 8\n",
      "compulsion 8\n",
      "downright 8\n",
      "distaste 8\n",
      "accidents 8\n",
      "emptied 8\n",
      "superstition 8\n",
      "dispelled 8\n",
      "witches 8\n",
      "horns 8\n",
      "excluded 8\n",
      "analogous 8\n",
      "1861 8\n",
      "englishmen 8\n",
      "embarrassment 8\n",
      "bells 8\n",
      "spiral 8\n",
      "dominion 8\n",
      "obey 8\n",
      "unitarian 8\n",
      "thanksgiving 8\n",
      "emerson 8\n",
      "accusing 8\n",
      "descended 8\n",
      "snarled 8\n",
      "bosom 8\n",
      "alarmed 8\n",
      "radiant 8\n",
      "mounts 8\n",
      "perpetual 8\n",
      "curtains 8\n",
      "symbolism 8\n",
      "diagrams 8\n",
      "yin 8\n",
      "odors 8\n",
      "upside 8\n",
      "examinations 8\n",
      "unconcerned 8\n",
      "recreational 8\n",
      "differentiation 8\n",
      "mentioning 8\n",
      "analyzing 8\n",
      "realistically 8\n",
      "anguish 8\n",
      "repel 8\n",
      "avoidance 8\n",
      "insistent 8\n",
      "shaping 8\n",
      "dependable 8\n",
      "strains 8\n",
      "interdependent 8\n",
      "attained 8\n",
      "buddha 8\n",
      "cf. 8\n",
      "yourselves 8\n",
      "mock 8\n",
      "guideposts 8\n",
      "post-war 8\n",
      "thornburg 8\n",
      "mailing 8\n",
      "shield 8\n",
      "super-set 8\n",
      "compost 8\n",
      "mobility 8\n",
      "exceedingly 8\n",
      "one-shot 8\n",
      "sac 8\n",
      "fidelity 8\n",
      "sensing 8\n",
      "dock 8\n",
      "1/4'' 8\n",
      "1/2'' 8\n",
      "2-56 8\n",
      "lifting 8\n",
      "centimeters 8\n",
      "compression 8\n",
      "trader 8\n",
      "rocked 8\n",
      "skipped 8\n",
      "stormy 8\n",
      "barrels 8\n",
      "cruz 8\n",
      "sherry 8\n",
      "metropolis 8\n",
      "surroundings 8\n",
      "holland 8\n",
      "constantine 8\n",
      "courtyard 8\n",
      "coals 8\n",
      "buns 8\n",
      "franks 8\n",
      "pretend 8\n",
      "spaced 8\n",
      "mahogany 8\n",
      "glue 8\n",
      "planking 8\n",
      "jig 8\n",
      "clamped 8\n",
      "exterior 8\n",
      "bottoms 8\n",
      "spindle 8\n",
      "scrap 8\n",
      "hereby 8\n",
      "relates 8\n",
      "ph 8\n",
      "debris 8\n",
      "pumping 8\n",
      "geological 8\n",
      "gaiety 8\n",
      "umbrella 8\n",
      "1931 8\n",
      "fitness 8\n",
      "palms 8\n",
      "ultrasonic 8\n",
      "bacteria 8\n",
      "microscopic 8\n",
      "microscope 8\n",
      "transmitted 8\n",
      "indicator 8\n",
      "celestial 8\n",
      "magnetism 8\n",
      "assumes 8\n",
      "bloat 8\n",
      "rot 8\n",
      "infection 8\n",
      "intensification 8\n",
      "frowned 8\n",
      "rigidly 8\n",
      "overboard 8\n",
      "bugs 8\n",
      "aggregate 8\n",
      "fabrication 8\n",
      "lb 8\n",
      "kilometer 8\n",
      "twenty-one 8\n",
      "discernible 8\n",
      "actuality 8\n",
      "stunned 8\n",
      "vecchio 8\n",
      "flashlight 8\n",
      "invisible 8\n",
      "peculiarly 8\n",
      "nutrition 8\n",
      "struggled 8\n",
      "jenny 8\n",
      "wiry 8\n",
      "wolfe 8\n",
      "telegram 8\n",
      "clocks 8\n",
      "slips 8\n",
      "victorian 8\n",
      "erotic 8\n",
      "reversed 8\n",
      "tumors 8\n",
      "sucking 8\n",
      "embarrassed 8\n",
      "mourning 8\n",
      "pig 8\n",
      "shout 8\n",
      "propagation 8\n",
      "unnatural 8\n",
      "furious 8\n",
      "juet 8\n",
      "aborigines 8\n",
      "modes 8\n",
      "companions 8\n",
      "illiterate 8\n",
      "homogeneous 8\n",
      "collector 8\n",
      "bias 8\n",
      "detectable 8\n",
      "mel 8\n",
      "frenchman 8\n",
      "stiffened 8\n",
      "pet 8\n",
      "peripheral 8\n",
      "knot 8\n",
      "afflicted 8\n",
      "scraped 8\n",
      "victoria 8\n",
      "alibi 8\n",
      "brazil 8\n",
      "goin' 8\n",
      "aperture 8\n",
      "triumphantly 8\n",
      "knuckles 8\n",
      "pregnant 8\n",
      "automotive 8\n",
      "conflicting 8\n",
      "withholding 8\n",
      "staircase 8\n",
      "diminishing 8\n",
      "lower-class 8\n",
      "drawer 8\n",
      "civilizational 8\n",
      "frail 8\n",
      "crawling 8\n",
      "wandered 8\n",
      "potters 8\n",
      "caravan 8\n",
      "strangers 8\n",
      "urbanization 8\n",
      "agrarian 8\n",
      "disappearance 8\n",
      "pollock 8\n",
      "knelt 8\n",
      "liberated 8\n",
      "betrayed 8\n",
      "churchyard 8\n",
      "cyclist 8\n",
      "poetics 8\n",
      "immediacy 8\n",
      "isolate 8\n",
      "prosperous 8\n",
      "martyr 8\n",
      "ontological 8\n",
      "ptolemy 8\n",
      "spherical 8\n",
      "copernican 8\n",
      "mamma 8\n",
      "weston 8\n",
      "taut 8\n",
      "scout 8\n",
      "parliamentary 8\n",
      "mercer's 8\n",
      "hetman's 8\n",
      "accompany 8\n",
      "claimant 8\n",
      "multiplicity 8\n",
      "garibaldi's 8\n",
      "strasbourg 8\n",
      "traps 8\n",
      "athens 8\n",
      "ruins 8\n",
      "o.k. 8\n",
      "newt 8\n",
      "hips 8\n",
      "archaeological 8\n",
      "retreated 8\n",
      "haunted 8\n",
      "farmhouse 8\n",
      "smoking 8\n",
      "hypothetical 8\n",
      "dystopias 8\n",
      "science-fiction 8\n",
      "malraux's 8\n",
      "fresco 8\n",
      "krim's 8\n",
      "surveying 8\n",
      "follow-up 8\n",
      "whereof 8\n",
      "sixty-one 8\n",
      "germanium 8\n",
      "notify 8\n",
      "decreases 8\n",
      "1040 8\n",
      "arcs 8\n",
      "kinetic 8\n",
      "parameters 8\n",
      "decreased 8\n",
      "**yc 8\n",
      "capillary 8\n",
      "actives 8\n",
      "greasy 8\n",
      "sorbed 8\n",
      "iodide 8\n",
      "micrometeorite 8\n",
      "meteorites 8\n",
      "interlobular 8\n",
      "thyroxine 8\n",
      "inhibit 8\n",
      "mg 8\n",
      "parasympathetic 8\n",
      "c-plane 8\n",
      "equate 8\n",
      "ruanda-urundi 8\n",
      "intonation 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phonemic 8\n",
      "swadesh 8\n",
      "public-limit 8\n",
      "respondents 8\n",
      "respondent 8\n",
      "referrals 8\n",
      "social-class 8\n",
      "tactual 8\n",
      "barney 8\n",
      "dorset 8\n",
      "flatness 8\n",
      "frantically 8\n",
      "diocs 8\n",
      "24-hr. 8\n",
      "crystals 8\n",
      "mutton 8\n",
      "**yf 8\n",
      "knitted 8\n",
      "restorative 8\n",
      "accelerometers 8\n",
      "Vocab size: 10001\n"
     ]
    }
   ],
   "source": [
    "sentences, word2idx = get_sentences_with_word2idx_limit_vocab(10000)\n",
    "\n",
    "# vocab size\n",
    "V = len(word2idx)\n",
    "print(\"Vocab size:\", V)\n",
    "\n",
    "# we will also treat beginning of sentence and end of sentence as bigrams\n",
    "# START -> first word\n",
    "# last word -> END\n",
    "start_idx = word2idx['START']\n",
    "end_idx = word2idx['END']\n",
    "\n",
    "# a matrix where:\n",
    "# row = last word\n",
    "# col = current word\n",
    "# value at [row, col] = p(current word | last word)\n",
    "bigram_probs = get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0030300068419509335, 1.4090639447308758e-06)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_probs[word2idx['orange'], word2idx['juice']], bigram_probs[word2idx['the'], word2idx['the']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11816469038889489, 0.00010004354007589217)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_probs[word2idx['START'], word2idx['the']], bigram_probs[word2idx['the'], word2idx['END']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the probability of getting \"juice\" after \"orange\" is 0.3%, the probability of getting \"the\" as the first word of the  sentence is 11.8%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get log of probability of a sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(sentence):\n",
    "    score = 0\n",
    "    for i in range(len(sentence)):\n",
    "        if i == 0:\n",
    "            # beginning word\n",
    "            score += np.log(bigram_probs[start_idx, sentence[i]])\n",
    "        else:\n",
    "            # middle word\n",
    "            score += np.log(bigram_probs[sentence[i-1], sentence[i]])\n",
    "    # final word\n",
    "    score += np.log(bigram_probs[sentence[-1], end_idx])\n",
    "\n",
    "    # normalize the score\n",
    "    return score / (len(sentence) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.644836746857917"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score([word2idx[\"this\"], word2idx[\"is\"], word2idx[\"orange\"], word2idx[\"juice\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score of a real and fake sentence of a language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL: he promptly went to communist east germany . SCORE: -4.910446462923176\n",
      "FAKE: evils theater interlobular intention liberated occupants what scout SCORE: -9.79431698257847\n",
      "Enter your own sentence:\n",
      "this is a cat\n",
      "SCORE: -5.330933791486226\n",
      "Continue? [Y/n]Y\n",
      "REAL: they had protested that that wasn't any surprise . SCORE: -5.316102857397317\n",
      "FAKE: finals duck makers jazz eugene 0.5 nut candidate consultant SCORE: -9.639098730168612\n",
      "Enter your own sentence:\n",
      "the the nice a due\n",
      "SCORE: -9.617722312683222\n",
      "Continue? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "idx2word = dict((v, k) for k, v in iteritems(word2idx))\n",
    "def get_words(sentence):\n",
    "    return ' '.join(idx2word[i] for i in sentence)\n",
    "\n",
    "sample_probs = np.ones(V)\n",
    "sample_probs[start_idx] = 0\n",
    "sample_probs[end_idx] = 0\n",
    "sample_probs /= sample_probs.sum()\n",
    "\n",
    "# test our model on real and fake sentences\n",
    "while True:\n",
    "    # real sentence\n",
    "    real_idx = np.random.choice(len(sentences))\n",
    "    real = sentences[real_idx]\n",
    "\n",
    "    # fake sentence\n",
    "    fake = np.random.choice(V, size=len(real), p=sample_probs)\n",
    "\n",
    "    print(\"REAL:\", get_words(real), \"SCORE:\", get_score(real))\n",
    "    print(\"FAKE:\", get_words(fake), \"SCORE:\", get_score(fake))\n",
    "\n",
    "    # input your own sentence\n",
    "    custom = input(\"Enter your own sentence:\\n\")\n",
    "    custom = custom.lower().split()\n",
    "\n",
    "    # check that all tokens exist in word2idx (otherwise, we can't get score)\n",
    "    bad_sentence = False\n",
    "    for token in custom:\n",
    "        if token not in word2idx:\n",
    "            bad_sentence = True\n",
    "\n",
    "    if bad_sentence:\n",
    "        print(\"Sorry, you entered words that are not in the vocabulary\")\n",
    "    else:\n",
    "        # convert sentence into list of indexes\n",
    "        custom = [word2idx[token] for token in custom]\n",
    "        print(\"SCORE:\", get_score(custom))\n",
    "\n",
    "\n",
    "    cont = input(\"Continue? [Y/n]\")\n",
    "    if cont and cont.lower() in ('N', 'n'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Bigram Model with Classification Models\n",
    "\n",
    "### 3.2.1 (Multiclass) Softmax Regression\n",
    "Rather than counting, we can represent words as one-hot vectors and find some parametrized algorithm to modelize $\\mathbf P(B|A)$. In this section we will write $x, y$ instead of $A, B$, as we treat words like vectors.\n",
    "\n",
    "One example can be logistic regression\n",
    "$$\n",
    "\\mathbf P(y|x) = \\mathrm{softmax}(W_y x, V) = \\frac{\\exp (W_{y} x)}{\\sum_{z \\in V} W_{z}x}\n",
    "$$\n",
    "\n",
    "where $V$ denotes the vocabulary.\n",
    "\n",
    "Note that instead of treating the problem as a classical classification problem with one category for the output, we can give the output probability form as the count frequency calculated in section 3.1. i.e., instead of having output like this\n",
    "```\n",
    "orange -> juice\n",
    "```\n",
    "\n",
    "we can use probability form like\n",
    "```\n",
    "orange -> juice (0.3%), tree(0.2%), etc...\n",
    "```\n",
    "\n",
    "The cost function is still\n",
    "$$\n",
    "J = -\\sum_{n=1}^N \\sum_{k=1}^V y_{n,k} \\log(\\hat y(y_{n,k}|x_n))\n",
    "$$\n",
    "\n",
    "where $\\hat y(y_{n,k}|x_n) = \\mathbf P(y_{n,k}|x_n)$ is the predicted output, $V$ the size of vocabulary, $N$ number of examples. Note that now $y_{n,k}$ is not 0/1 but a real number between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START inf\n",
      "END inf\n",
      "man inf\n",
      "paris inf\n",
      "britain inf\n",
      "england inf\n",
      "king inf\n",
      "woman inf\n",
      "rome inf\n",
      "london inf\n",
      "queen inf\n",
      "italy inf\n",
      "france inf\n",
      "the 69971\n",
      ", 58334\n",
      ". 49346\n",
      "of 36412\n",
      "and 28853\n",
      "to 26158\n",
      "a 23195\n",
      "in 21337\n",
      "that 10594\n",
      "is 10109\n",
      "was 9815\n",
      "he 9548\n",
      "for 9489\n",
      "`` 8837\n",
      "'' 8789\n",
      "it 8760\n",
      "with 7289\n",
      "as 7253\n",
      "his 6996\n",
      "on 6741\n",
      "be 6377\n",
      "; 5566\n",
      "at 5372\n",
      "by 5306\n",
      "i 5164\n",
      "this 5145\n",
      "had 5133\n",
      "? 4693\n",
      "not 4610\n",
      "are 4394\n",
      "but 4381\n",
      "from 4370\n",
      "or 4206\n",
      "have 3942\n",
      "an 3740\n",
      "they 3620\n",
      "which 3561\n",
      "-- 3432\n",
      "one 3292\n",
      "you 3286\n",
      "were 3284\n",
      "her 3036\n",
      "all 3001\n",
      "she 2860\n",
      "there 2728\n",
      "would 2714\n",
      "their 2669\n",
      "we 2652\n",
      "him 2619\n",
      "been 2472\n",
      ") 2466\n",
      "has 2437\n",
      "( 2435\n",
      "when 2331\n",
      "who 2252\n",
      "will 2245\n",
      "more 2215\n",
      "if 2198\n",
      "no 2139\n",
      "out 2097\n",
      "so 1985\n",
      "said 1961\n",
      "what 1908\n",
      "up 1890\n",
      "its 1858\n",
      "about 1815\n",
      ": 1795\n",
      "into 1791\n",
      "than 1790\n",
      "them 1788\n",
      "can 1772\n",
      "only 1748\n",
      "other 1702\n",
      "new 1635\n",
      "some 1618\n",
      "could 1601\n",
      "time 1598\n",
      "! 1596\n",
      "these 1573\n",
      "two 1412\n",
      "may 1402\n",
      "then 1380\n",
      "do 1363\n",
      "first 1361\n",
      "any 1344\n",
      "my 1318\n",
      "now 1314\n",
      "such 1303\n",
      "like 1292\n",
      "our 1252\n",
      "over 1236\n",
      "me 1181\n",
      "even 1170\n",
      "most 1159\n",
      "made 1125\n",
      "also 1069\n",
      "after 1069\n",
      "did 1044\n",
      "many 1030\n",
      "before 1016\n",
      "must 1013\n",
      "af 996\n",
      "through 971\n",
      "back 966\n",
      "years 950\n",
      "where 937\n",
      "much 937\n",
      "your 923\n",
      "way 908\n",
      "well 897\n",
      "down 895\n",
      "should 888\n",
      "because 883\n",
      "each 877\n",
      "just 872\n",
      "those 850\n",
      "people 847\n",
      "mr. 844\n",
      "too 834\n",
      "how 834\n",
      "little 831\n",
      "state 807\n",
      "good 806\n",
      "very 796\n",
      "make 794\n",
      "world 787\n",
      "still 782\n",
      "see 772\n",
      "own 772\n",
      "men 763\n",
      "work 762\n",
      "long 752\n",
      "here 750\n",
      "get 749\n",
      "both 730\n",
      "between 730\n",
      "life 715\n",
      "being 712\n",
      "under 707\n",
      "never 697\n",
      "day 687\n",
      "same 686\n",
      "another 684\n",
      "know 683\n",
      "while 680\n",
      "last 676\n",
      "us 675\n",
      "might 672\n",
      "great 665\n",
      "old 661\n",
      "year 658\n",
      "off 639\n",
      "come 630\n",
      "since 628\n",
      "against 627\n",
      "go 626\n",
      "came 622\n",
      "right 613\n",
      "used 611\n",
      "take 610\n",
      "three 610\n",
      "himself 603\n",
      "states 603\n",
      "few 601\n",
      "house 591\n",
      "use 591\n",
      "during 585\n",
      "without 583\n",
      "again 577\n",
      "place 570\n",
      "american 569\n",
      "around 562\n",
      "however 552\n",
      "home 547\n",
      "small 542\n",
      "found 536\n",
      "mrs. 534\n",
      "1 527\n",
      "thought 517\n",
      "went 507\n",
      "say 504\n",
      "part 500\n",
      "once 499\n",
      "general 498\n",
      "high 497\n",
      "upon 495\n",
      "school 493\n",
      "every 491\n",
      "don't 489\n",
      "does 485\n",
      "got 482\n",
      "united 482\n",
      "left 480\n",
      "number 472\n",
      "course 465\n",
      "war 464\n",
      "until 461\n",
      "always 458\n",
      "away 456\n",
      "something 450\n",
      "fact 447\n",
      "2 446\n",
      "water 445\n",
      "though 440\n",
      "public 438\n",
      "less 437\n",
      "put 437\n",
      "think 433\n",
      "almost 432\n",
      "hand 431\n",
      "enough 430\n",
      "took 426\n",
      "far 426\n",
      "head 424\n",
      "yet 419\n",
      "government 418\n",
      "system 416\n",
      "set 414\n",
      "better 414\n",
      "told 413\n",
      "night 411\n",
      "nothing 411\n",
      "end 409\n",
      "why 404\n",
      "didn't 401\n",
      "called 401\n",
      "eyes 401\n",
      "find 400\n",
      "going 399\n",
      "look 399\n",
      "asked 398\n",
      "later 397\n",
      "knew 395\n",
      "point 395\n",
      "next 394\n",
      "program 394\n",
      "city 393\n",
      "business 393\n",
      "group 390\n",
      "give 389\n",
      "toward 386\n",
      "young 385\n",
      "let 384\n",
      "days 384\n",
      "room 384\n",
      "president 382\n",
      "side 381\n",
      "social 380\n",
      "present 377\n",
      "given 377\n",
      "several 377\n",
      "order 376\n",
      "national 375\n",
      "possible 374\n",
      "rather 373\n",
      "second 373\n",
      "face 371\n",
      "per 371\n",
      "among 370\n",
      "form 370\n",
      "often 369\n",
      "important 369\n",
      "things 368\n",
      "looked 367\n",
      "early 366\n",
      "white 365\n",
      "john 362\n",
      "case 362\n",
      "large 361\n",
      "four 360\n",
      "need 360\n",
      "big 360\n",
      "become 359\n",
      "within 359\n",
      "felt 357\n",
      "children 355\n",
      "along 355\n",
      "saw 352\n",
      "best 351\n",
      "church 348\n",
      "ever 344\n",
      "least 343\n",
      "power 342\n",
      "development 334\n",
      "seemed 333\n",
      "thing 333\n",
      "light 333\n",
      "family 331\n",
      "interest 330\n",
      "want 328\n",
      "members 325\n",
      "mind 325\n",
      "area 324\n",
      "country 324\n",
      "others 323\n",
      "although 321\n",
      "turned 320\n",
      "done 319\n",
      "open 318\n",
      "' 317\n",
      "god 316\n",
      "service 315\n",
      "problem 313\n",
      "certain 313\n",
      "kind 313\n",
      "different 312\n",
      "thus 312\n",
      "began 312\n",
      "door 312\n",
      "help 311\n",
      "sense 311\n",
      "means 310\n",
      "whole 309\n",
      "matter 308\n",
      "perhaps 307\n",
      "itself 304\n",
      "york 302\n",
      "it's 302\n",
      "times 300\n",
      "law 299\n",
      "human 299\n",
      "line 298\n",
      "above 296\n",
      "name 294\n",
      "example 292\n",
      "action 291\n",
      "company 290\n",
      "hands 289\n",
      "local 288\n",
      "show 288\n",
      "3 287\n",
      "whether 286\n",
      "five 286\n",
      "history 286\n",
      "gave 285\n",
      "today 284\n",
      "either 284\n",
      "act 283\n",
      "feet 283\n",
      "across 282\n",
      "taken 281\n",
      "past 281\n",
      "quite 281\n",
      "anything 280\n",
      "seen 279\n",
      "having 279\n",
      "death 277\n",
      "experience 276\n",
      "body 276\n",
      "week 275\n",
      "half 275\n",
      "really 275\n",
      "word 274\n",
      "field 274\n",
      "car 274\n",
      "words 274\n",
      "already 273\n",
      "themselves 270\n",
      "i'm 269\n",
      "information 269\n",
      "tell 268\n",
      "shall 268\n",
      "together 267\n",
      "college 267\n",
      "money 265\n",
      "period 265\n",
      "held 264\n",
      "keep 264\n",
      "sure 263\n",
      "probably 261\n",
      "free 259\n",
      "seems 259\n",
      "political 258\n",
      "real 258\n",
      "cannot 258\n",
      "behind 258\n",
      "question 257\n",
      "air 257\n",
      "office 255\n",
      "making 255\n",
      "brought 253\n",
      "miss 253\n",
      "whose 251\n",
      "special 250\n",
      "major 247\n",
      "heard 247\n",
      "problems 247\n",
      "federal 246\n",
      "became 246\n",
      "study 246\n",
      "ago 246\n",
      "moment 246\n",
      "available 245\n",
      "known 245\n",
      "result 244\n",
      "street 244\n",
      "economic 243\n",
      "boy 242\n",
      "position 241\n",
      "reason 241\n",
      "change 240\n",
      "south 240\n",
      "board 239\n",
      "individual 239\n",
      "job 238\n",
      "am 237\n",
      "society 237\n",
      "areas 236\n",
      "west 235\n",
      "close 234\n",
      "turn 233\n",
      "community 231\n",
      "true 231\n",
      "love 231\n",
      "court 230\n",
      "force 230\n",
      "full 230\n",
      "cost 229\n",
      "seem 229\n",
      "wife 228\n",
      "future 227\n",
      "age 227\n",
      "wanted 226\n",
      "voice 226\n",
      "department 225\n",
      "center 224\n",
      "control 223\n",
      "common 223\n",
      "policy 222\n",
      "necessary 222\n",
      "following 221\n",
      "front 221\n",
      "sometimes 221\n",
      "six 220\n",
      "girl 220\n",
      "clear 219\n",
      "further 218\n",
      "land 218\n",
      "provide 216\n",
      "feel 216\n",
      "party 216\n",
      "able 216\n",
      "mother 216\n",
      "music 216\n",
      "education 214\n",
      "university 214\n",
      "child 213\n",
      "effect 213\n",
      "students 213\n",
      "level 213\n",
      "run 212\n",
      "stood 212\n",
      "military 212\n",
      "town 212\n",
      "short 212\n",
      "morning 211\n",
      "total 211\n",
      "outside 210\n",
      "rate 209\n",
      "figure 209\n",
      "art 208\n",
      "century 207\n",
      "class 207\n",
      "washington 206\n",
      "4 206\n",
      "north 206\n",
      "usually 206\n",
      "plan 205\n",
      "leave 205\n",
      "therefore 205\n",
      "evidence 204\n",
      "top 204\n",
      "million 204\n",
      "sound 204\n",
      "black 203\n",
      "strong 202\n",
      "hard 202\n",
      "tax 201\n",
      "various 201\n",
      "says 200\n",
      "believe 200\n",
      "type 200\n",
      "value 200\n",
      "play 200\n",
      "surface 200\n",
      "soon 199\n",
      "mean 199\n",
      "near 198\n",
      "lines 198\n",
      "table 198\n",
      "peace 198\n",
      "modern 198\n",
      "road 197\n",
      "red 197\n",
      "book 197\n",
      "personal 196\n",
      "process 196\n",
      "situation 196\n",
      "minutes 196\n",
      "increase 195\n",
      "schools 195\n",
      "idea 195\n",
      "english 195\n",
      "alone 195\n",
      "women 195\n",
      "gone 195\n",
      "nor 195\n",
      "living 194\n",
      "america 194\n",
      "started 194\n",
      "longer 193\n",
      "dr. 192\n",
      "cut 192\n",
      "finally 191\n",
      "secretary 191\n",
      "nature 191\n",
      "private 191\n",
      "third 190\n",
      "months 189\n",
      "section 189\n",
      "greater 188\n",
      "call 188\n",
      "fire 187\n",
      "expected 187\n",
      "needed 187\n",
      "that's 187\n",
      "kept 186\n",
      "ground 186\n",
      "view 186\n",
      "values 186\n",
      "everything 185\n",
      "pressure 185\n",
      "dark 185\n",
      "basis 184\n",
      "space 184\n",
      "east 183\n",
      "father 183\n",
      "required 182\n",
      "union 182\n",
      "spirit 182\n",
      "complete 182\n",
      "except 181\n",
      "wrote 181\n",
      "i'll 181\n",
      "moved 181\n",
      "support 180\n",
      "return 180\n",
      "conditions 180\n",
      "recent 179\n",
      "attention 179\n",
      "late 179\n",
      "particular 179\n",
      "live 177\n",
      "hope 177\n",
      "costs 176\n",
      "else 176\n",
      "brown 176\n",
      "taking 175\n",
      "couldn't 175\n",
      "forces 175\n",
      "nations 175\n",
      "beyond 175\n",
      "stage 175\n",
      "read 174\n",
      "report 174\n",
      "coming 174\n",
      "hours 174\n",
      "person 174\n",
      "inside 174\n",
      "dead 174\n",
      "material 174\n",
      "instead 173\n",
      "lost 173\n",
      "heart 173\n",
      "looking 173\n",
      "low 173\n",
      "miles 173\n",
      "data 173\n",
      "added 172\n",
      "pay 172\n",
      "amount 172\n",
      "followed 172\n",
      "feeling 172\n",
      "1960 172\n",
      "single 172\n",
      "makes 172\n",
      "research 171\n",
      "including 171\n",
      "basic 171\n",
      "hundred 171\n",
      "move 171\n",
      "industry 171\n",
      "cold 171\n",
      "simply 171\n",
      "developed 170\n",
      "tried 170\n",
      "hold 169\n",
      "can't 169\n",
      "reached 169\n",
      "committee 168\n",
      "island 167\n",
      "defense 167\n",
      "equipment 167\n",
      "actually 166\n",
      "shown 166\n",
      "son 165\n",
      "central 165\n",
      "religious 165\n",
      "river 165\n",
      "getting 164\n",
      "st. 164\n",
      "beginning 164\n",
      "sort 164\n",
      "ten 164\n",
      "received 163\n",
      "& 163\n",
      "doing 163\n",
      "terms 163\n",
      "trying 163\n",
      "rest 163\n",
      "medical 162\n",
      "u.s. 162\n",
      "care 162\n",
      "especially 162\n",
      "friends 162\n",
      "picture 162\n",
      "indeed 162\n",
      "administration 161\n",
      "fine 161\n",
      "subject 161\n",
      "difficult 161\n",
      "building 160\n",
      "higher 160\n",
      "wall 160\n",
      "simple 160\n",
      "meeting 159\n",
      "walked 159\n",
      "floor 158\n",
      "foreign 158\n",
      "bring 158\n",
      "similar 157\n",
      "passed 157\n",
      "range 157\n",
      "paper 157\n",
      "property 156\n",
      "natural 156\n",
      "final 156\n",
      "training 156\n",
      "county 155\n",
      "police 155\n",
      "cent 155\n",
      "international 155\n",
      "growth 155\n",
      "market 155\n",
      "wasn't 154\n",
      "talk 154\n",
      "start 154\n",
      "written 154\n",
      "hear 153\n",
      "suddenly 153\n",
      "story 153\n",
      "issue 152\n",
      "congress 152\n",
      "needs 152\n",
      "10 152\n",
      "answer 152\n",
      "hall 152\n",
      "likely 151\n",
      "working 151\n",
      "countries 151\n",
      "considered 151\n",
      "you're 151\n",
      "earth 150\n",
      "sat 150\n",
      "purpose 149\n",
      "meet 149\n",
      "labor 149\n",
      "results 149\n",
      "entire 149\n",
      "happened 149\n",
      "william 148\n",
      "cases 148\n",
      "stand 148\n",
      "difference 148\n",
      "production 148\n",
      "hair 148\n",
      "involved 147\n",
      "fall 147\n",
      "stock 147\n",
      "food 147\n",
      "earlier 146\n",
      "increased 146\n",
      "whom 146\n",
      "particularly 146\n",
      "paid 145\n",
      "sent 145\n",
      "effort 145\n",
      "knowledge 145\n",
      "hour 145\n",
      "letter 145\n",
      "club 145\n",
      "using 145\n",
      "below 145\n",
      "thinking 145\n",
      "yes 144\n",
      "christian 144\n",
      "blue 143\n",
      "ready 143\n",
      "bill 143\n",
      "deal 143\n",
      "points 143\n",
      "trade 143\n",
      "certainly 143\n",
      "ideas 143\n",
      "industrial 143\n",
      "square 143\n",
      "boys 143\n",
      "methods 142\n",
      "addition 142\n",
      "method 142\n",
      "bad 142\n",
      "due 142\n",
      "5 142\n",
      "girls 142\n",
      "moral 142\n",
      "decided 141\n",
      "reading 141\n",
      "statement 141\n",
      "weeks 141\n",
      "neither 141\n",
      "nearly 141\n",
      "directly 141\n",
      "showed 141\n",
      "throughout 141\n",
      "according 140\n",
      "questions 140\n",
      "color 140\n",
      "kennedy 140\n",
      "anyone 140\n",
      "try 140\n",
      "services 139\n",
      "programs 139\n",
      "nation 139\n",
      "lay 139\n",
      "french 139\n",
      "size 138\n",
      "remember 138\n",
      "physical 138\n",
      "record 137\n",
      "member 137\n",
      "comes 137\n",
      "understand 137\n",
      "southern 137\n",
      "western 137\n",
      "strength 137\n",
      "population 136\n",
      "normal 136\n",
      "merely 135\n",
      "district 135\n",
      "volume 135\n",
      "concerned 135\n",
      "appeared 135\n",
      "temperature 135\n",
      "1961 134\n",
      "aid 134\n",
      "trouble 134\n",
      "trial 134\n",
      "summer 134\n",
      "direction 134\n",
      "ran 134\n",
      "sales 133\n",
      "list 133\n",
      "continued 133\n",
      "friend 133\n",
      "evening 133\n",
      "maybe 133\n",
      "literature 133\n",
      "generally 132\n",
      "association 132\n",
      "provided 132\n",
      "led 132\n",
      "army 132\n",
      "met 132\n",
      "influence 132\n",
      "opened 131\n",
      "former 131\n",
      "science 131\n",
      "student 131\n",
      "step 131\n",
      "changes 131\n",
      "chance 131\n",
      "husband 131\n",
      "hot 130\n",
      "series 130\n",
      "average 130\n",
      "works 130\n",
      "month 130\n",
      "cause 130\n",
      "effective 129\n",
      "george 129\n",
      "planning 129\n",
      "systems 129\n",
      "wouldn't 129\n",
      "direct 129\n",
      "soviet 129\n",
      "stopped 129\n",
      "wrong 129\n",
      "lead 129\n",
      "myself 129\n",
      "piece 129\n",
      "theory 129\n",
      "ask 128\n",
      "worked 128\n",
      "freedom 128\n",
      "organization 128\n",
      "clearly 128\n",
      "movement 128\n",
      "ways 128\n",
      "press 127\n",
      "somewhat 127\n",
      "spring 127\n",
      "efforts 127\n",
      "consider 127\n",
      "meaning 127\n",
      "bed 127\n",
      "fear 127\n",
      "lot 127\n",
      "treatment 127\n",
      "beautiful 127\n",
      "note 127\n",
      "forms 127\n",
      "placed 126\n",
      "hotel 126\n",
      "truth 126\n",
      "apparently 125\n",
      "degree 125\n",
      "groups 125\n",
      "he's 125\n",
      "plant 125\n",
      "carried 125\n",
      "wide 125\n",
      "i've 125\n",
      "respect 125\n",
      "man's 125\n",
      "herself 125\n",
      "numbers 125\n",
      "manner 124\n",
      "reaction 124\n",
      "easy 124\n",
      "farm 124\n",
      "immediately 123\n",
      "running 123\n",
      "approach 123\n",
      "game 123\n",
      "recently 123\n",
      "larger 123\n",
      "lower 123\n",
      "charge 122\n",
      "couple 122\n",
      "de 122\n",
      "daily 122\n",
      "eye 122\n",
      "performance 122\n",
      "feed 122\n",
      "oh 122\n",
      "march 121\n",
      "persons 121\n",
      "understanding 121\n",
      "arms 121\n",
      "opportunity 121\n",
      "c 121\n",
      "blood 121\n",
      "additional 120\n",
      "j. 120\n",
      "technical 120\n",
      "fiscal 120\n",
      "radio 120\n",
      "described 120\n",
      "stop 120\n",
      "progress 120\n",
      "steps 119\n",
      "test 119\n",
      "chief 119\n",
      "reported 119\n",
      "served 119\n",
      "based 119\n",
      "main 119\n",
      "determined 119\n",
      "image 119\n",
      "decision 119\n",
      "window 119\n",
      "religion 119\n",
      "aj 118\n",
      "gun 118\n",
      "responsibility 118\n",
      "middle 118\n",
      "europe 118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "british 118\n",
      "character 118\n",
      "learned 117\n",
      "horse 117\n",
      "writing 117\n",
      "appear 117\n",
      "s. 117\n",
      "account 117\n",
      "ones 116\n",
      "serious 116\n",
      "activity 116\n",
      "types 116\n",
      "green 116\n",
      "length 116\n",
      "lived 115\n",
      "audience 115\n",
      "letters 115\n",
      "returned 115\n",
      "obtained 115\n",
      "nuclear 115\n",
      "specific 115\n",
      "corner 115\n",
      "forward 115\n",
      "activities 115\n",
      "slowly 115\n",
      "doubt 114\n",
      "6 114\n",
      "justice 114\n",
      "moving 114\n",
      "latter 114\n",
      "gives 114\n",
      "straight 114\n",
      "hit 114\n",
      "plane 114\n",
      "quality 114\n",
      "design 114\n",
      "obviously 114\n",
      "operation 113\n",
      "plans 113\n",
      "shot 113\n",
      "seven 113\n",
      "a. 113\n",
      "choice 113\n",
      "poor 113\n",
      "staff 113\n",
      "function 113\n",
      "figures 113\n",
      "parts 113\n",
      "stay 113\n",
      "saying 113\n",
      "include 113\n",
      "15 113\n",
      "born 113\n",
      "pattern 113\n",
      "30 112\n",
      "cars 112\n",
      "whatever 112\n",
      "sun 112\n",
      "faith 111\n",
      "pool 111\n",
      "hospital 110\n",
      "corps 110\n",
      "wish 110\n",
      "lack 110\n",
      "completely 110\n",
      "heavy 110\n",
      "waiting 110\n",
      "speak 110\n",
      "ball 110\n",
      "standard 110\n",
      "extent 110\n",
      "visit 109\n",
      "democratic 109\n",
      "firm 109\n",
      "income 109\n",
      "ahead 109\n",
      "deep 109\n",
      "there's 109\n",
      "language 109\n",
      "principle 109\n",
      "none 108\n",
      "price 108\n",
      "designed 108\n",
      "indicated 108\n",
      "analysis 108\n",
      "distance 108\n",
      "expect 108\n",
      "established 108\n",
      "products 108\n",
      "effects 108\n",
      "growing 108\n",
      "importance 108\n",
      "continue 107\n",
      "serve 107\n",
      "determine 107\n",
      "cities 107\n",
      "elements 107\n",
      "negro 107\n",
      "leaders 107\n",
      "division 107\n",
      "pretty 107\n",
      "easily 107\n",
      "existence 107\n",
      "attitude 107\n",
      "stress 107\n",
      "8 106\n",
      "afternoon 106\n",
      "limited 106\n",
      "hardly 106\n",
      "agreement 106\n",
      "factors 106\n",
      "scene 106\n",
      "remained 106\n",
      "closed 106\n",
      "write 106\n",
      "applied 106\n",
      "health 105\n",
      "married 105\n",
      "suggested 105\n",
      "attack 105\n",
      "rhode 105\n",
      "interested 105\n",
      "station 105\n",
      "professional 105\n",
      "won't 105\n",
      "drive 105\n",
      "season 105\n",
      "reach 105\n",
      "b 105\n",
      "despite 104\n",
      "current 104\n",
      "spent 104\n",
      "eight 104\n",
      "covered 104\n",
      "role 104\n",
      "played 104\n",
      "i'd 104\n",
      "becomes 104\n",
      "date 103\n",
      "council 103\n",
      "race 103\n",
      "unit 103\n",
      "commission 103\n",
      "original 103\n",
      "mouth 103\n",
      "reasons 103\n",
      "studies 103\n",
      "exactly 103\n",
      "machine 103\n",
      "built 103\n",
      "teeth 103\n",
      "relations 102\n",
      "rise 102\n",
      "demand 102\n",
      "prepared 102\n",
      "1959 102\n",
      "related 102\n",
      "rates 102\n",
      "news 102\n",
      "supply 102\n",
      "james 101\n",
      "director 101\n",
      "sunday 101\n",
      "bit 101\n",
      "raised 101\n",
      "events 101\n",
      "unless 101\n",
      "officer 101\n",
      "dropped 101\n",
      "playing 101\n",
      "trees 101\n",
      "standing 101\n",
      "doctor 100\n",
      "places 100\n",
      "facilities 100\n",
      "walk 100\n",
      "energy 100\n",
      "thomas 100\n",
      "talking 100\n",
      "meant 100\n",
      "clay 100\n",
      "sides 100\n",
      "gas 99\n",
      "filled 99\n",
      "techniques 99\n",
      "june 99\n",
      "knows 99\n",
      "hadn't 99\n",
      "glass 99\n",
      "jazz 99\n",
      "poet 99\n",
      "actual 99\n",
      "fight 98\n",
      "concern 98\n",
      "caught 98\n",
      "share 98\n",
      "popular 98\n",
      "mass 98\n",
      "claim 98\n",
      "entered 98\n",
      "chicago 98\n",
      "happy 98\n",
      "bridge 98\n",
      "institutions 98\n",
      "style 98\n",
      "he'd 98\n",
      "follow 97\n",
      "dollars 97\n",
      "communist 97\n",
      "status 97\n",
      "included 97\n",
      "thousand 97\n",
      "christ 97\n",
      "isn't 97\n",
      "heat 97\n",
      "radiation 97\n",
      "materials 97\n",
      "cattle 97\n",
      "suppose 97\n",
      "primary 96\n",
      "accepted 96\n",
      "books 96\n",
      "charles 96\n",
      "12 96\n",
      "sitting 96\n",
      "conference 96\n",
      "opinion 96\n",
      "usual 96\n",
      "churches 96\n",
      "film 96\n",
      "giving 96\n",
      "behavior 96\n",
      "considerable 96\n",
      "funds 95\n",
      "construction 95\n",
      "attempt 95\n",
      "changed 95\n",
      "proper 95\n",
      "successful 95\n",
      "marriage 95\n",
      "sea 95\n",
      "oil 95\n",
      "sir 95\n",
      "hell 95\n",
      "wait 94\n",
      "sign 94\n",
      "worth 94\n",
      "source 94\n",
      "highly 94\n",
      "park 94\n",
      "7 94\n",
      "discussion 94\n",
      "everyone 94\n",
      "practice 94\n",
      "arm 94\n",
      "tradition 94\n",
      "shows 94\n",
      "someone 94\n",
      "authority 93\n",
      "older 93\n",
      "annual 93\n",
      "project 93\n",
      "c. 93\n",
      "americans 93\n",
      "lord 93\n",
      "success 93\n",
      "remain 93\n",
      "principal 92\n",
      "20 92\n",
      "leadership 92\n",
      "jack 92\n",
      "obvious 92\n",
      "fell 92\n",
      "thin 92\n",
      "pieces 92\n",
      "management 91\n",
      "1958 91\n",
      "measure 91\n",
      "parents 91\n",
      "security 91\n",
      "base 91\n",
      "entirely 91\n",
      "civil 91\n",
      "frequently 91\n",
      "records 91\n",
      "structure 91\n",
      "dinner 91\n",
      "weight 91\n",
      "condition 91\n",
      "mike 91\n",
      "objective 91\n",
      "complex 91\n",
      "produced 90\n",
      "noted 90\n",
      "caused 90\n",
      "equal 90\n",
      "balance 90\n",
      "you'll 90\n",
      "purposes 90\n",
      "corporation 90\n",
      "dance 90\n",
      "kitchen 90\n",
      "failure 89\n",
      "pass 89\n",
      "goes 89\n",
      "names 89\n",
      "quickly 89\n",
      "regard 89\n",
      "published 89\n",
      "famous 89\n",
      "develop 89\n",
      "clothes 89\n",
      "laws 88\n",
      "announced 88\n",
      "carry 88\n",
      "cover 88\n",
      "moreover 88\n",
      "add 88\n",
      "greatest 88\n",
      "check 88\n",
      "enemy 88\n",
      "leaving 88\n",
      "key 88\n",
      "manager 88\n",
      "doesn't 88\n",
      "active 88\n",
      "break 88\n",
      "bottom 88\n",
      "pain 88\n",
      "relationship 88\n",
      "sources 88\n",
      "poetry 88\n",
      "assistance 87\n",
      "operating 87\n",
      "battle 87\n",
      "companies 87\n",
      "fixed 87\n",
      "possibility 87\n",
      "mary 87\n",
      "product 87\n",
      "spoke 87\n",
      "units 87\n",
      "touch 87\n",
      "bright 87\n",
      "finished 87\n",
      "carefully 87\n",
      "facts 87\n",
      "previous 86\n",
      "citizens 86\n",
      "takes 86\n",
      "e. 86\n",
      "allowed 86\n",
      "require 86\n",
      "workers 86\n",
      "build 86\n",
      "patient 86\n",
      "financial 86\n",
      "philosophy 86\n",
      "loss 86\n",
      "rose 86\n",
      "died 86\n",
      "scientific 86\n",
      "otherwise 86\n",
      "inches 86\n",
      "significant 86\n",
      "seeing 86\n",
      "distribution 85\n",
      "marked 85\n",
      "post 85\n",
      "rules 85\n",
      "capital 85\n",
      "captain 85\n",
      "relatively 85\n",
      "classes 85\n",
      "variety 85\n",
      "stated 85\n",
      "shape 85\n",
      "stations 85\n",
      "german 85\n",
      "musical 85\n",
      "concept 85\n",
      "reports 84\n",
      "proposed 84\n",
      "w. 84\n",
      "begin 84\n",
      "impossible 84\n",
      "affairs 84\n",
      "named 84\n",
      "circumstances 84\n",
      "learn 84\n",
      "remains 84\n",
      "appears 84\n",
      "strange 84\n",
      "catholic 84\n",
      "operations 84\n",
      "collection 84\n",
      "aware 84\n",
      "sex 84\n",
      "broad 84\n",
      "henry 83\n",
      "robert 83\n",
      "governor 83\n",
      "offered 83\n",
      "bank 83\n",
      "team 83\n",
      "yesterday 83\n",
      "requirements 83\n",
      "capacity 83\n",
      "speed 83\n",
      "prevent 83\n",
      "regular 83\n",
      "officers 83\n",
      "houses 83\n",
      "mark 83\n",
      "opening 83\n",
      "spread 83\n",
      "winter 83\n",
      "ship 83\n",
      "slightly 83\n",
      "remembered 83\n",
      "interests 83\n",
      "sight 83\n",
      "bar 82\n",
      "produce 82\n",
      "crisis 82\n",
      "25 82\n",
      "youth 82\n",
      "presented 82\n",
      "interesting 82\n",
      "fresh 82\n",
      "train 82\n",
      "instance 82\n",
      "drink 82\n",
      "poems 82\n",
      "agreed 81\n",
      "campaign 81\n",
      "event 81\n",
      "subjects 81\n",
      "forced 81\n",
      "nine 81\n",
      "essential 81\n",
      "immediate 81\n",
      "lives 81\n",
      "file 81\n",
      "provides 81\n",
      "watch 81\n",
      "opposite 81\n",
      "apartment 81\n",
      "created 81\n",
      "germany 81\n",
      "trip 81\n",
      "neck 81\n",
      "watched 81\n",
      "index 81\n",
      "cells 81\n",
      "session 80\n",
      "offer 80\n",
      "fully 80\n",
      "teacher 80\n",
      "recognized 80\n",
      "providence 80\n",
      "explained 80\n",
      "indicate 80\n",
      "twenty 80\n",
      "lady 80\n",
      "russian 80\n",
      "features 80\n",
      "gray 80\n",
      "term 79\n",
      "studied 79\n",
      "sam 79\n",
      "economy 79\n",
      "reduced 79\n",
      "maximum 79\n",
      "separate 79\n",
      "procedure 79\n",
      "atmosphere 79\n",
      "desire 79\n",
      "mentioned 79\n",
      "reality 79\n",
      "expression 79\n",
      "differences 79\n",
      "fair 78\n",
      "enter 78\n",
      "traditional 78\n",
      "mission 78\n",
      "favor 78\n",
      "looks 78\n",
      "secret 78\n",
      "fast 78\n",
      "picked 78\n",
      "coffee 78\n",
      "smaller 78\n",
      "edge 78\n",
      "tone 78\n",
      "beside 78\n",
      "literary 78\n",
      "- 78\n",
      "election 77\n",
      "judge 77\n",
      "title 77\n",
      "permit 77\n",
      "address 77\n",
      "rights 77\n",
      "vocational 77\n",
      "laid 77\n",
      "response 77\n",
      "believed 77\n",
      "model 77\n",
      "100 77\n",
      "solid 77\n",
      "follows 77\n",
      "editor 77\n",
      "t 77\n",
      "anode 77\n",
      "receive 76\n",
      "b. 76\n",
      "quiet 76\n",
      "telephone 76\n",
      "hearing 76\n",
      "buildings 76\n",
      "formed 76\n",
      "nice 76\n",
      "watching 76\n",
      "memory 76\n",
      "presence 76\n",
      "difficulty 76\n",
      "region 76\n",
      "knife 76\n",
      "p 76\n",
      "bottle 76\n",
      "jr. 75\n",
      "personnel 75\n",
      "fit 75\n",
      "official 75\n",
      "vote 75\n",
      "junior 75\n",
      "treated 75\n",
      "expressed 75\n",
      "planned 75\n",
      "round 75\n",
      "dog 75\n",
      "virginia 75\n",
      "killed 75\n",
      "camp 75\n",
      "stayed 75\n",
      "murder 75\n",
      "removed 75\n",
      "rock 75\n",
      "turning 75\n",
      "pointed 74\n",
      "november 74\n",
      "selected 74\n",
      "berlin 74\n",
      "claims 74\n",
      "increasing 74\n",
      "leader 74\n",
      "positive 74\n",
      "frame 74\n",
      "gain 74\n",
      "twice 74\n",
      "failed 74\n",
      "nobody 74\n",
      "send 74\n",
      "ability 74\n",
      "fourth 74\n",
      "interior 74\n",
      "jewish 74\n",
      "store 74\n",
      "faculty 74\n",
      "standards 74\n",
      "rich 74\n",
      "contrast 74\n",
      "observed 74\n",
      "nevertheless 73\n",
      "brief 73\n",
      "louis 73\n",
      "individuals 73\n",
      "rule 73\n",
      "powers 73\n",
      "advantage 73\n",
      "discovered 73\n",
      "pulled 73\n",
      "writer 73\n",
      "chapter 73\n",
      "writers 73\n",
      "brother 73\n",
      "valley 73\n",
      "membership 73\n",
      "die 73\n",
      "items 72\n",
      "daughter 72\n",
      "platform 72\n",
      "allow 72\n",
      "ordinary 72\n",
      "jones 72\n",
      "faces 72\n",
      "accept 72\n",
      "plus 72\n",
      "master 72\n",
      "legal 72\n",
      "hill 72\n",
      "fighting 72\n",
      "resources 72\n",
      "increases 72\n",
      "assumed 72\n",
      "sharp 72\n",
      "everybody 72\n",
      "broke 72\n",
      "command 72\n",
      "evil 72\n",
      "wants 72\n",
      "village 72\n",
      "phase 72\n",
      "russia 72\n",
      "detail 72\n",
      "morgan 72\n",
      "somehow 72\n",
      "fields 72\n",
      "familiar 72\n",
      "upper 72\n",
      "wine 72\n",
      "boat 72\n",
      "april 71\n",
      "unity 71\n",
      "richard 71\n",
      "responsible 71\n",
      "factor 71\n",
      "h. 71\n",
      "chosen 71\n",
      "principles 71\n",
      "constant 71\n",
      "proved 71\n",
      "carrying 71\n",
      "mercer 71\n",
      "column 71\n",
      "forth 71\n",
      "beauty 71\n",
      "compared 71\n",
      "approximately 71\n",
      "du 71\n",
      "historical 71\n",
      "smiled 71\n",
      "universe 71\n",
      "fig. 71\n",
      "calls 70\n",
      "san 70\n",
      "educational 70\n",
      "independent 70\n",
      "danger 70\n",
      "dogs 70\n",
      "waited 70\n",
      "rain 70\n",
      "song 70\n",
      "naturally 70\n",
      "box 70\n",
      "buy 70\n",
      "shelter 70\n",
      "drawn 70\n",
      "dust 70\n",
      "communism 70\n",
      "exchange 70\n",
      "sections 70\n",
      "walls 70\n",
      "foot 70\n",
      "aircraft 70\n",
      "independence 70\n",
      "revolution 70\n",
      "realize 69\n",
      "texas 69\n",
      "seek 69\n",
      "willing 69\n",
      "league 69\n",
      "teachers 69\n",
      "connection 69\n",
      "politics 69\n",
      "liberal 69\n",
      "clean 69\n",
      "completed 69\n",
      "weather 69\n",
      "fashion 69\n",
      "ordered 69\n",
      "levels 69\n",
      "sweet 69\n",
      "settled 69\n",
      "realized 69\n",
      "let's 69\n",
      "ancient 69\n",
      "china 69\n",
      "lips 69\n",
      "won 68\n",
      "policies 68\n",
      "actions 68\n",
      "monday 68\n",
      "directed 68\n",
      "leading 68\n",
      "frank 68\n",
      "statements 68\n",
      "projects 68\n",
      "starting 68\n",
      "initial 68\n",
      "application 68\n",
      "traffic 68\n",
      "stands 68\n",
      "signs 68\n",
      "families 68\n",
      "quick 68\n",
      "khrushchev 68\n",
      "largely 68\n",
      "flow 68\n",
      "drew 68\n",
      "animal 68\n",
      "beat 68\n",
      "horses 68\n",
      "characteristic 68\n",
      "excellent 68\n",
      "practical 68\n",
      "electric 68\n",
      "electronic 68\n",
      "pictures 68\n",
      "ought 68\n",
      "protection 68\n",
      "article 68\n",
      "appropriate 68\n",
      "fifty 68\n",
      "minimum 68\n",
      "dry 68\n",
      "emotional 68\n",
      "she'd 68\n",
      "jury 67\n",
      "career 67\n",
      "chairman 67\n",
      "aside 67\n",
      "asking 67\n",
      "estimated 67\n",
      "teaching 67\n",
      "reference 67\n",
      "saturday 67\n",
      "flat 67\n",
      "ends 67\n",
      "background 67\n",
      "sit 67\n",
      "dress 67\n",
      "occurred 67\n",
      "warm 67\n",
      "potential 67\n",
      "impact 67\n",
      "yourself 67\n",
      "legs 67\n",
      "you've 67\n",
      "wonder 67\n",
      "communication 67\n",
      "answered 67\n",
      "thick 67\n",
      "birth 66\n",
      "declared 66\n",
      "honor 66\n",
      "july 66\n",
      "significance 66\n",
      "score 66\n",
      "helped 66\n",
      "gross 66\n",
      "issues 66\n",
      "forest 66\n",
      "search 66\n",
      "block 66\n",
      "cutting 66\n",
      "substantial 66\n",
      "gets 66\n",
      "relief 66\n",
      "plays 66\n",
      "arts 66\n",
      "besides 66\n",
      "employees 66\n",
      "page 66\n",
      "intellectual 66\n",
      "properties 66\n",
      "experiments 66\n",
      "closely 66\n",
      "chair 66\n",
      "capable 66\n",
      "adequate 66\n",
      "measured 66\n",
      "ourselves 66\n",
      "fingers 66\n",
      "hanover 66\n",
      "attorney 65\n",
      "d. 65\n",
      "passing 65\n",
      "discussed 65\n",
      "achievement 65\n",
      "headquarters 65\n",
      "rapidly 65\n",
      "object 65\n",
      "escape 65\n",
      "jobs 65\n",
      "join 65\n",
      "phil 65\n",
      "california 65\n",
      "supposed 65\n",
      "they're 65\n",
      "typical 65\n",
      "wore 65\n",
      "cell 65\n",
      "newspaper 65\n",
      "desk 65\n",
      "one's 65\n",
      "imagination 65\n",
      "hung 65\n",
      "holding 65\n",
      "objects 65\n",
      "sleep 65\n",
      "dominant 65\n",
      "reasonable 64\n",
      "matters 64\n",
      "resolution 64\n",
      "site 64\n",
      "credit 64\n",
      "aspects 64\n",
      "message 64\n",
      "maintenance 64\n",
      "laos 64\n",
      "explain 64\n",
      "we'll 64\n",
      "located 64\n",
      "towards 64\n",
      "belief 64\n",
      "yards 64\n",
      "bodies 64\n",
      "contemporary 64\n",
      "primarily 64\n",
      "grew 64\n",
      "spiritual 64\n",
      "dream 64\n",
      "empty 64\n",
      "wind 63\n",
      "tom 63\n",
      "kill 63\n",
      "benefit 63\n",
      "signal 63\n",
      "tomorrow 63\n",
      "sufficient 63\n",
      "dramatic 63\n",
      "fellow 63\n",
      "happen 63\n",
      "no. 63\n",
      "p.m. 63\n",
      "jesus 63\n",
      "contact 63\n",
      "unusual 63\n",
      "argument 63\n",
      "powerful 63\n",
      "narrow 63\n",
      "parker 63\n",
      "shop 63\n",
      "rifle 63\n",
      "highest 63\n",
      "broken 63\n",
      "appeal 63\n",
      "competition 63\n",
      "domestic 63\n",
      "grow 63\n",
      "experiment 63\n",
      "assume 63\n",
      "relation 63\n",
      "location 63\n",
      "reduce 62\n",
      "homes 62\n",
      "portion 62\n",
      "officials 62\n",
      "m. 62\n",
      "senate 62\n",
      "fund 62\n",
      "9 62\n",
      "billion 62\n",
      "rising 62\n",
      "11 62\n",
      "speaking 62\n",
      "internal 62\n",
      "struggle 62\n",
      "agencies 62\n",
      "u. 62\n",
      "december 62\n",
      "equally 62\n",
      "sets 62\n",
      "please 62\n",
      "drove 62\n",
      "arrived 62\n",
      "save 62\n",
      "achieved 62\n",
      "assignment 62\n",
      "baby 62\n",
      "guests 62\n",
      "greatly 62\n",
      "recognize 62\n",
      "wilson 62\n",
      "library 62\n",
      "careful 62\n",
      "pleasure 62\n",
      "cool 62\n",
      "extreme 62\n",
      "concerning 62\n",
      "governments 61\n",
      "procedures 61\n",
      "prices 61\n",
      "duty 61\n",
      "courses 61\n",
      "friendly 61\n",
      "we're 61\n",
      "r. 61\n",
      "coast 61\n",
      "acting 61\n",
      "50 61\n",
      "closer 61\n",
      "speech 61\n",
      "european 61\n",
      "showing 61\n",
      "boston 61\n",
      "victory 61\n",
      "beach 61\n",
      "minister 61\n",
      "commercial 61\n",
      "metal 61\n",
      "possibly 61\n",
      "tests 61\n",
      "soft 61\n",
      "kid 61\n",
      "vast 61\n",
      "continuing 61\n",
      "associated 61\n",
      "shoulder 61\n",
      "weapons 61\n",
      "shore 61\n",
      "greek 61\n",
      "travel 61\n",
      "imagine 61\n",
      "feelings 61\n",
      "organizations 61\n",
      "ideal 61\n",
      "eat 61\n",
      "friday 60\n",
      "keeping 60\n",
      "heavily 60\n",
      "armed 60\n",
      "ended 60\n",
      "learning 60\n",
      "text 60\n",
      "existing 60\n",
      "scale 60\n",
      "setting 60\n",
      "goal 60\n",
      "task 60\n",
      "contract 60\n",
      "garden 60\n",
      "nose 60\n",
      "refused 60\n",
      "streets 60\n",
      "orchestra 60\n",
      "contained 60\n",
      "machinery 60\n",
      "chemical 60\n",
      "onto 60\n",
      "circle 60\n",
      "slow 60\n",
      "maintain 60\n",
      "fat 60\n",
      "somewhere 60\n",
      "technique 60\n",
      "stared 60\n",
      "moon 60\n",
      "notice 59\n",
      "drop 59\n",
      "budget 59\n",
      "providing 59\n",
      "f. 59\n",
      "formula 59\n",
      "housing 59\n",
      "tension 59\n",
      "advance 59\n",
      "repeated 59\n",
      "parties 59\n",
      "uses 59\n",
      "judgment 59\n",
      "taste 59\n",
      "novel 59\n",
      "headed 59\n",
      "sensitive 59\n",
      "conclusion 59\n",
      "roof 59\n",
      "solution 59\n",
      "bible 59\n",
      "lie 59\n",
      "ultimate 59\n",
      "songs 59\n",
      "struck 59\n",
      "negroes 59\n",
      "snow 59\n",
      "tree 59\n",
      "plants 59\n",
      "finds 59\n",
      "stories 59\n",
      "mine 59\n",
      "painting 59\n",
      "exist 59\n",
      "thirty 59\n",
      "sexual 59\n",
      "tuesday 58\n",
      "roads 58\n",
      "commerce 58\n",
      "p. 58\n",
      "dallas 58\n",
      "establish 58\n",
      "previously 58\n",
      "causes 58\n",
      "talked 58\n",
      "railroad 58\n",
      "critical 58\n",
      "remove 58\n",
      "emphasis 58\n",
      "grounds 58\n",
      "neighborhood 58\n",
      "surprised 58\n",
      "minor 58\n",
      "india 58\n",
      "understood 58\n",
      "perfect 58\n",
      "avoid 58\n",
      "somebody 58\n",
      "hole 58\n",
      "hence 58\n",
      "leg 58\n",
      "busy 58\n",
      "occasion 58\n",
      "smile 58\n",
      "stone 58\n",
      "roman 58\n",
      "unique 58\n",
      "animals 58\n",
      "sky 58\n",
      "safe 58\n",
      "etc. 58\n",
      "orders 58\n",
      "fairly 58\n",
      "liked 58\n",
      "useful 58\n",
      "exercise 58\n",
      "lose 58\n",
      "culture 58\n",
      "pale 58\n",
      "wondered 58\n",
      "charged 57\n",
      "details 57\n",
      "informed 57\n",
      "permitted 57\n",
      "professor 57\n",
      "replied 57\n",
      "completion 57\n",
      "processes 57\n",
      "apart 57\n",
      "apparent 57\n",
      "bay 57\n",
      "truck 57\n",
      "majority 57\n",
      "afraid 57\n",
      "artist 57\n",
      "goods 57\n",
      "birds 57\n",
      "appearance 57\n",
      "baseball 57\n",
      "spot 57\n",
      "flowers 57\n",
      "lewis 57\n",
      "notes 57\n",
      "enjoyed 57\n",
      "entrance 57\n",
      "uncle 57\n",
      "alive 57\n",
      "beneath 57\n",
      "combination 57\n",
      "truly 57\n",
      "congo 57\n",
      "becoming 57\n",
      "requires 57\n",
      "sample 57\n",
      "bear 57\n",
      "dictionary 57\n",
      "shook 57\n",
      "granted 56\n",
      "l. 56\n",
      "confidence 56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2001\n",
      "epoch: 0 sentence: 0/57013 loss: 7.593820249538516\n",
      "epoch: 0 sentence: 10/57013 loss: 7.531913467711308\n",
      "epoch: 0 sentence: 20/57013 loss: 7.072205513241184\n",
      "epoch: 0 sentence: 30/57013 loss: 6.835117171459249\n",
      "epoch: 0 sentence: 40/57013 loss: 6.6918522637075\n",
      "epoch: 0 sentence: 50/57013 loss: 7.421653838781926\n",
      "epoch: 0 sentence: 60/57013 loss: 6.76632509733388\n",
      "epoch: 0 sentence: 70/57013 loss: 6.823194040965012\n",
      "epoch: 0 sentence: 80/57013 loss: 6.67201265376792\n",
      "epoch: 0 sentence: 90/57013 loss: 6.432276041844373\n",
      "epoch: 0 sentence: 100/57013 loss: 6.940502753472279\n",
      "epoch: 0 sentence: 110/57013 loss: 6.478915658693771\n",
      "epoch: 0 sentence: 120/57013 loss: 5.93697363829109\n",
      "epoch: 0 sentence: 130/57013 loss: 6.758266608373121\n",
      "epoch: 0 sentence: 140/57013 loss: 6.798812375950884\n",
      "epoch: 0 sentence: 150/57013 loss: 7.585112097254626\n",
      "epoch: 0 sentence: 160/57013 loss: 6.512803793550572\n",
      "epoch: 0 sentence: 170/57013 loss: 6.63578076271213\n",
      "epoch: 0 sentence: 180/57013 loss: 6.348235178708559\n",
      "epoch: 0 sentence: 190/57013 loss: 6.494120286325257\n",
      "epoch: 0 sentence: 200/57013 loss: 5.938725195823815\n",
      "epoch: 0 sentence: 210/57013 loss: 6.471812421535424\n",
      "epoch: 0 sentence: 220/57013 loss: 6.6110193402275685\n",
      "epoch: 0 sentence: 230/57013 loss: 6.547975097891362\n",
      "epoch: 0 sentence: 240/57013 loss: 6.041254416680702\n",
      "epoch: 0 sentence: 250/57013 loss: 5.915394669080988\n",
      "epoch: 0 sentence: 260/57013 loss: 6.469598328310438\n",
      "epoch: 0 sentence: 270/57013 loss: 5.955972614808057\n",
      "epoch: 0 sentence: 280/57013 loss: 5.438485728873467\n",
      "epoch: 0 sentence: 290/57013 loss: 5.975877330969461\n",
      "epoch: 0 sentence: 300/57013 loss: 6.112312548844986\n",
      "epoch: 0 sentence: 310/57013 loss: 6.159278790856704\n",
      "epoch: 0 sentence: 320/57013 loss: 6.397052575447298\n",
      "epoch: 0 sentence: 330/57013 loss: 7.256350571187358\n",
      "epoch: 0 sentence: 340/57013 loss: 6.19955291553139\n",
      "epoch: 0 sentence: 350/57013 loss: 6.456385658982528\n",
      "epoch: 0 sentence: 360/57013 loss: 6.397564358836564\n",
      "epoch: 0 sentence: 370/57013 loss: 5.4112716117308945\n",
      "epoch: 0 sentence: 380/57013 loss: 6.731961199323817\n",
      "epoch: 0 sentence: 390/57013 loss: 6.831009422513001\n",
      "epoch: 0 sentence: 400/57013 loss: 6.387320826812646\n",
      "epoch: 0 sentence: 410/57013 loss: 6.060617385395852\n",
      "epoch: 0 sentence: 420/57013 loss: 5.569986869824945\n",
      "epoch: 0 sentence: 430/57013 loss: 6.315923665799763\n",
      "epoch: 0 sentence: 440/57013 loss: 6.842976000260215\n",
      "epoch: 0 sentence: 450/57013 loss: 6.318520995127084\n",
      "epoch: 0 sentence: 460/57013 loss: 4.776875191869056\n",
      "epoch: 0 sentence: 470/57013 loss: 7.061018512541198\n",
      "epoch: 0 sentence: 480/57013 loss: 5.499460379045397\n",
      "epoch: 0 sentence: 490/57013 loss: 3.8847984184722897\n",
      "epoch: 0 sentence: 500/57013 loss: 5.294868671812761\n",
      "epoch: 0 sentence: 510/57013 loss: 5.963061645437498\n",
      "epoch: 0 sentence: 520/57013 loss: 6.705041338363325\n",
      "epoch: 0 sentence: 530/57013 loss: 5.96745717224547\n",
      "epoch: 0 sentence: 540/57013 loss: 5.803719950661757\n",
      "epoch: 0 sentence: 550/57013 loss: 6.904321719263885\n",
      "epoch: 0 sentence: 560/57013 loss: 1.6355011795302725\n",
      "epoch: 0 sentence: 570/57013 loss: 4.434824667417042\n",
      "epoch: 0 sentence: 580/57013 loss: 6.2839576521896525\n",
      "epoch: 0 sentence: 590/57013 loss: 6.384817151013218\n",
      "epoch: 0 sentence: 600/57013 loss: 6.370981770820721\n",
      "epoch: 0 sentence: 610/57013 loss: 5.365618677000567\n",
      "epoch: 0 sentence: 620/57013 loss: 6.1448514493401865\n",
      "epoch: 0 sentence: 630/57013 loss: 6.591789960187373\n",
      "epoch: 0 sentence: 640/57013 loss: 5.787962381337171\n",
      "epoch: 0 sentence: 650/57013 loss: 4.952835920142984\n",
      "epoch: 0 sentence: 660/57013 loss: 6.4956554978370304\n",
      "epoch: 0 sentence: 670/57013 loss: 6.0514638114785875\n",
      "epoch: 0 sentence: 680/57013 loss: 5.66359106147113\n",
      "epoch: 0 sentence: 690/57013 loss: 5.422879231029451\n",
      "epoch: 0 sentence: 700/57013 loss: 5.923061066442756\n",
      "epoch: 0 sentence: 710/57013 loss: 6.154032484053697\n",
      "epoch: 0 sentence: 720/57013 loss: 5.74191406698552\n",
      "epoch: 0 sentence: 730/57013 loss: 5.9273399321870786\n",
      "epoch: 0 sentence: 740/57013 loss: 6.213153780794835\n",
      "epoch: 0 sentence: 750/57013 loss: 3.9323729310403657\n",
      "epoch: 0 sentence: 760/57013 loss: 5.03238354051608\n",
      "epoch: 0 sentence: 770/57013 loss: 5.9401035096409425\n",
      "epoch: 0 sentence: 780/57013 loss: 5.910556743853461\n",
      "epoch: 0 sentence: 790/57013 loss: 4.315574041846443\n",
      "epoch: 0 sentence: 800/57013 loss: 4.435617846958015\n",
      "epoch: 0 sentence: 810/57013 loss: 6.1723080375030355\n",
      "epoch: 0 sentence: 820/57013 loss: 6.4891186324481\n",
      "epoch: 0 sentence: 830/57013 loss: 5.48062456164564\n",
      "epoch: 0 sentence: 840/57013 loss: 7.015961268927175\n",
      "epoch: 0 sentence: 850/57013 loss: 6.413820009639993\n",
      "epoch: 0 sentence: 860/57013 loss: 5.818629749490466\n",
      "epoch: 0 sentence: 870/57013 loss: 5.3371698464003225\n",
      "epoch: 0 sentence: 880/57013 loss: 4.680938060442723\n",
      "epoch: 0 sentence: 890/57013 loss: 6.946512454623122\n",
      "epoch: 0 sentence: 900/57013 loss: 6.964254263227811\n",
      "epoch: 0 sentence: 910/57013 loss: 6.370735814108767\n",
      "epoch: 0 sentence: 920/57013 loss: 4.9698077891804076\n",
      "epoch: 0 sentence: 930/57013 loss: 6.423829734060539\n",
      "epoch: 0 sentence: 940/57013 loss: 5.745212003647404\n",
      "epoch: 0 sentence: 950/57013 loss: 6.286097301823588\n",
      "epoch: 0 sentence: 960/57013 loss: 6.038164183900296\n",
      "epoch: 0 sentence: 970/57013 loss: 5.283899875243093\n",
      "epoch: 0 sentence: 980/57013 loss: 3.9573155299030542\n",
      "epoch: 0 sentence: 990/57013 loss: 5.372547847869626\n",
      "epoch: 0 sentence: 1000/57013 loss: 7.718680286108171\n",
      "epoch: 0 sentence: 1010/57013 loss: 5.49289933927017\n",
      "epoch: 0 sentence: 1020/57013 loss: 6.75509403907598\n",
      "epoch: 0 sentence: 1030/57013 loss: 4.993332336865574\n",
      "epoch: 0 sentence: 1040/57013 loss: 5.7438825074594195\n",
      "epoch: 0 sentence: 1050/57013 loss: 5.478253166737144\n",
      "epoch: 0 sentence: 1060/57013 loss: 5.790500946008721\n",
      "epoch: 0 sentence: 1070/57013 loss: 5.699602777750584\n",
      "epoch: 0 sentence: 1080/57013 loss: 6.23523568393265\n",
      "epoch: 0 sentence: 1090/57013 loss: 6.023212750827088\n",
      "epoch: 0 sentence: 1100/57013 loss: 6.057226583989602\n",
      "epoch: 0 sentence: 1110/57013 loss: 4.592597403640479\n",
      "epoch: 0 sentence: 1120/57013 loss: 5.843507125978335\n",
      "epoch: 0 sentence: 1130/57013 loss: 6.335310823678541\n",
      "epoch: 0 sentence: 1140/57013 loss: 6.018527226134938\n",
      "epoch: 0 sentence: 1150/57013 loss: 6.723462263122913\n",
      "epoch: 0 sentence: 1160/57013 loss: 6.950229499354853\n",
      "epoch: 0 sentence: 1170/57013 loss: 6.432950163622973\n",
      "epoch: 0 sentence: 1180/57013 loss: 6.816988762304813\n",
      "epoch: 0 sentence: 1190/57013 loss: 5.311989260499453\n",
      "epoch: 0 sentence: 1200/57013 loss: 4.128344309307377\n",
      "epoch: 0 sentence: 1210/57013 loss: 6.939759646688411\n",
      "epoch: 0 sentence: 1220/57013 loss: 6.252282196059474\n",
      "epoch: 0 sentence: 1230/57013 loss: 3.853135842790203\n",
      "epoch: 0 sentence: 1240/57013 loss: 5.864220085833398\n",
      "epoch: 0 sentence: 1250/57013 loss: 6.4489076135974\n",
      "epoch: 0 sentence: 1260/57013 loss: 5.594016438215445\n",
      "epoch: 0 sentence: 1270/57013 loss: 4.968301529750683\n",
      "epoch: 0 sentence: 1280/57013 loss: 5.118206322221137\n",
      "epoch: 0 sentence: 1290/57013 loss: 5.153781729975705\n",
      "epoch: 0 sentence: 1300/57013 loss: 4.2379692813561425\n",
      "epoch: 0 sentence: 1310/57013 loss: 1.686309516702993\n",
      "epoch: 0 sentence: 1320/57013 loss: 6.56714571493701\n",
      "epoch: 0 sentence: 1330/57013 loss: 5.5737569610009405\n",
      "epoch: 0 sentence: 1340/57013 loss: 6.151362897270742\n",
      "epoch: 0 sentence: 1350/57013 loss: 6.72241141248254\n",
      "epoch: 0 sentence: 1360/57013 loss: 5.390216231695576\n",
      "epoch: 0 sentence: 1370/57013 loss: 5.761262791223861\n",
      "epoch: 0 sentence: 1380/57013 loss: 5.817335270453639\n",
      "epoch: 0 sentence: 1390/57013 loss: 5.293481157341042\n",
      "epoch: 0 sentence: 1400/57013 loss: 5.803845366948563\n",
      "epoch: 0 sentence: 1410/57013 loss: 6.539434177554295\n",
      "epoch: 0 sentence: 1420/57013 loss: 6.964684613815385\n",
      "epoch: 0 sentence: 1430/57013 loss: 4.286737239367519\n",
      "epoch: 0 sentence: 1440/57013 loss: 5.628874324536832\n",
      "epoch: 0 sentence: 1450/57013 loss: 6.307955244885344\n",
      "epoch: 0 sentence: 1460/57013 loss: 5.141119647972818\n",
      "epoch: 0 sentence: 1470/57013 loss: 6.738268093967258\n",
      "epoch: 0 sentence: 1480/57013 loss: 3.326051706418402\n",
      "epoch: 0 sentence: 1490/57013 loss: 5.533164398225228\n",
      "epoch: 0 sentence: 1500/57013 loss: 5.928959476347147\n",
      "epoch: 0 sentence: 1510/57013 loss: 6.3011972695049385\n",
      "epoch: 0 sentence: 1520/57013 loss: 4.446387948125659\n",
      "epoch: 0 sentence: 1530/57013 loss: 6.383447489590459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 1540/57013 loss: 6.656123545861389\n",
      "epoch: 0 sentence: 1550/57013 loss: 5.342389133625163\n",
      "epoch: 0 sentence: 1560/57013 loss: 6.872887063028901\n",
      "epoch: 0 sentence: 1570/57013 loss: 5.303443484865176\n",
      "epoch: 0 sentence: 1580/57013 loss: 3.732765164509344\n",
      "epoch: 0 sentence: 1590/57013 loss: 6.278826589363579\n",
      "epoch: 0 sentence: 1600/57013 loss: 6.3256717368773465\n",
      "epoch: 0 sentence: 1610/57013 loss: 5.62799638355587\n",
      "epoch: 0 sentence: 1620/57013 loss: 6.769003665981598\n",
      "epoch: 0 sentence: 1630/57013 loss: 5.744026578019667\n",
      "epoch: 0 sentence: 1640/57013 loss: 4.2119182265143404\n",
      "epoch: 0 sentence: 1650/57013 loss: 5.029108895722792\n",
      "epoch: 0 sentence: 1660/57013 loss: 6.93610548105657\n",
      "epoch: 0 sentence: 1670/57013 loss: 5.247563029530555\n",
      "epoch: 0 sentence: 1680/57013 loss: 5.962735280486289\n",
      "epoch: 0 sentence: 1690/57013 loss: 4.657367839744051\n",
      "epoch: 0 sentence: 1700/57013 loss: 6.247854332079492\n",
      "epoch: 0 sentence: 1710/57013 loss: 6.312692840361287\n",
      "epoch: 0 sentence: 1720/57013 loss: 4.1095962384514655\n",
      "epoch: 0 sentence: 1730/57013 loss: 5.164643789104266\n",
      "epoch: 0 sentence: 1740/57013 loss: 6.018635278548177\n",
      "epoch: 0 sentence: 1750/57013 loss: 5.594586396936205\n",
      "epoch: 0 sentence: 1760/57013 loss: 4.928810883397855\n",
      "epoch: 0 sentence: 1770/57013 loss: 6.4387935792896815\n",
      "epoch: 0 sentence: 1780/57013 loss: 4.185073060590675\n",
      "epoch: 0 sentence: 1790/57013 loss: 5.877943845759105\n",
      "epoch: 0 sentence: 1800/57013 loss: 4.935682798319611\n",
      "epoch: 0 sentence: 1810/57013 loss: 6.859551574608129\n",
      "epoch: 0 sentence: 1820/57013 loss: 5.333173985873073\n",
      "epoch: 0 sentence: 1830/57013 loss: 4.945083916753932\n",
      "epoch: 0 sentence: 1840/57013 loss: 5.277039039641385\n",
      "epoch: 0 sentence: 1850/57013 loss: 6.081157448250177\n",
      "epoch: 0 sentence: 1860/57013 loss: 6.410350173450625\n",
      "epoch: 0 sentence: 1870/57013 loss: 6.987160361929372\n",
      "epoch: 0 sentence: 1880/57013 loss: 7.053493515562388\n",
      "epoch: 0 sentence: 1890/57013 loss: 4.1597165680150034\n",
      "epoch: 0 sentence: 1900/57013 loss: 5.807459167105569\n",
      "epoch: 0 sentence: 1910/57013 loss: 6.882632862823093\n",
      "epoch: 0 sentence: 1920/57013 loss: 5.4102254792834135\n",
      "epoch: 0 sentence: 1930/57013 loss: 7.864358286996251\n",
      "epoch: 0 sentence: 1940/57013 loss: 4.966761228016411\n",
      "epoch: 0 sentence: 1950/57013 loss: 6.610946327563459\n",
      "epoch: 0 sentence: 1960/57013 loss: 5.069641946100113\n",
      "epoch: 0 sentence: 1970/57013 loss: 5.507284651381285\n",
      "epoch: 0 sentence: 1980/57013 loss: 6.105223155414624\n",
      "epoch: 0 sentence: 1990/57013 loss: 6.168954721572274\n",
      "epoch: 0 sentence: 2000/57013 loss: 4.488459921948177\n",
      "epoch: 0 sentence: 2010/57013 loss: 5.628144649654181\n",
      "epoch: 0 sentence: 2020/57013 loss: 5.80961343637465\n",
      "epoch: 0 sentence: 2030/57013 loss: 5.14647356101686\n",
      "epoch: 0 sentence: 2040/57013 loss: 5.085216954779453\n",
      "epoch: 0 sentence: 2050/57013 loss: 4.18080709634601\n",
      "epoch: 0 sentence: 2060/57013 loss: 5.960882064400687\n",
      "epoch: 0 sentence: 2070/57013 loss: 3.479902230919982\n",
      "epoch: 0 sentence: 2080/57013 loss: 6.231041943143644\n",
      "epoch: 0 sentence: 2090/57013 loss: 5.495966680061751\n",
      "epoch: 0 sentence: 2100/57013 loss: 2.9733081147021942\n",
      "epoch: 0 sentence: 2110/57013 loss: 4.989024258057124\n",
      "epoch: 0 sentence: 2120/57013 loss: 6.133999846301448\n",
      "epoch: 0 sentence: 2130/57013 loss: 3.9430981057976577\n",
      "epoch: 0 sentence: 2140/57013 loss: 3.7097532281146446\n",
      "epoch: 0 sentence: 2150/57013 loss: 4.732215616716942\n",
      "epoch: 0 sentence: 2160/57013 loss: 4.701209307483089\n",
      "epoch: 0 sentence: 2170/57013 loss: 4.550231570768497\n",
      "epoch: 0 sentence: 2180/57013 loss: 5.777785568527872\n",
      "epoch: 0 sentence: 2190/57013 loss: 4.661429804426416\n",
      "epoch: 0 sentence: 2200/57013 loss: 3.991535364700161\n",
      "epoch: 0 sentence: 2210/57013 loss: 5.492665389549681\n",
      "epoch: 0 sentence: 2220/57013 loss: 4.972864060554743\n",
      "epoch: 0 sentence: 2230/57013 loss: 5.124372775424183\n",
      "epoch: 0 sentence: 2240/57013 loss: 5.606425504260378\n",
      "epoch: 0 sentence: 2250/57013 loss: 6.186524843837237\n",
      "epoch: 0 sentence: 2260/57013 loss: 6.49344794852986\n",
      "epoch: 0 sentence: 2270/57013 loss: 6.585269978034411\n",
      "epoch: 0 sentence: 2280/57013 loss: 5.701180701852879\n",
      "epoch: 0 sentence: 2290/57013 loss: 4.796091633970543\n",
      "epoch: 0 sentence: 2300/57013 loss: 5.0056851062966015\n",
      "epoch: 0 sentence: 2310/57013 loss: 5.259402222266507\n",
      "epoch: 0 sentence: 2320/57013 loss: 5.111341098048275\n",
      "epoch: 0 sentence: 2330/57013 loss: 6.384060473675955\n",
      "epoch: 0 sentence: 2340/57013 loss: 4.354513322752134\n",
      "epoch: 0 sentence: 2350/57013 loss: 5.297799502090946\n",
      "epoch: 0 sentence: 2360/57013 loss: 4.247802392814594\n",
      "epoch: 0 sentence: 2370/57013 loss: 5.664999130057348\n",
      "epoch: 0 sentence: 2380/57013 loss: 6.045256507737252\n",
      "epoch: 0 sentence: 2390/57013 loss: 4.730560624393865\n",
      "epoch: 0 sentence: 2400/57013 loss: 5.180851889007952\n",
      "epoch: 0 sentence: 2410/57013 loss: 6.418497320949179\n",
      "epoch: 0 sentence: 2420/57013 loss: 5.774340707623194\n",
      "epoch: 0 sentence: 2430/57013 loss: 5.751587825749578\n",
      "epoch: 0 sentence: 2440/57013 loss: 6.11805133214914\n",
      "epoch: 0 sentence: 2450/57013 loss: 5.412780403337689\n",
      "epoch: 0 sentence: 2460/57013 loss: 4.063669186546948\n",
      "epoch: 0 sentence: 2470/57013 loss: 5.669559378861301\n",
      "epoch: 0 sentence: 2480/57013 loss: 5.469446447233277\n",
      "epoch: 0 sentence: 2490/57013 loss: 4.233689182668201\n",
      "epoch: 0 sentence: 2500/57013 loss: 4.963133657833285\n",
      "epoch: 0 sentence: 2510/57013 loss: 5.586274264865835\n",
      "epoch: 0 sentence: 2520/57013 loss: 3.7740073081235574\n",
      "epoch: 0 sentence: 2530/57013 loss: 5.6003076185421605\n",
      "epoch: 0 sentence: 2540/57013 loss: 5.2667432328168635\n",
      "epoch: 0 sentence: 2550/57013 loss: 5.201506676959598\n",
      "epoch: 0 sentence: 2560/57013 loss: 6.8110885551011755\n",
      "epoch: 0 sentence: 2570/57013 loss: 5.675312191538547\n",
      "epoch: 0 sentence: 2580/57013 loss: 5.870480059186968\n",
      "epoch: 0 sentence: 2590/57013 loss: 5.888848889754688\n",
      "epoch: 0 sentence: 2600/57013 loss: 5.111557446953666\n",
      "epoch: 0 sentence: 2610/57013 loss: 5.772215536813677\n",
      "epoch: 0 sentence: 2620/57013 loss: 5.93662622949879\n",
      "epoch: 0 sentence: 2630/57013 loss: 5.420157458156249\n",
      "epoch: 0 sentence: 2640/57013 loss: 4.867379196404016\n",
      "epoch: 0 sentence: 2650/57013 loss: 6.801533976842574\n",
      "epoch: 0 sentence: 2660/57013 loss: 4.057219434642146\n",
      "epoch: 0 sentence: 2670/57013 loss: 4.45646093284525\n",
      "epoch: 0 sentence: 2680/57013 loss: 6.176902582335554\n",
      "epoch: 0 sentence: 2690/57013 loss: 4.357257484128233\n",
      "epoch: 0 sentence: 2700/57013 loss: 4.910182550647429\n",
      "epoch: 0 sentence: 2710/57013 loss: 6.286962645243922\n",
      "epoch: 0 sentence: 2720/57013 loss: 5.2711415935654005\n",
      "epoch: 0 sentence: 2730/57013 loss: 5.324719833355451\n",
      "epoch: 0 sentence: 2740/57013 loss: 5.171251809416396\n",
      "epoch: 0 sentence: 2750/57013 loss: 5.035519024289549\n",
      "epoch: 0 sentence: 2760/57013 loss: 5.8825211697204285\n",
      "epoch: 0 sentence: 2770/57013 loss: 3.726998076608864\n",
      "epoch: 0 sentence: 2780/57013 loss: 5.550384242749885\n",
      "epoch: 0 sentence: 2790/57013 loss: 6.4961865664442335\n",
      "epoch: 0 sentence: 2800/57013 loss: 5.45564468858205\n",
      "epoch: 0 sentence: 2810/57013 loss: 4.015451416566258\n",
      "epoch: 0 sentence: 2820/57013 loss: 6.653745199847371\n",
      "epoch: 0 sentence: 2830/57013 loss: 4.566281949192626\n",
      "epoch: 0 sentence: 2840/57013 loss: 4.802630772008569\n",
      "epoch: 0 sentence: 2850/57013 loss: 6.806908517816177\n",
      "epoch: 0 sentence: 2860/57013 loss: 5.97834948893504\n",
      "epoch: 0 sentence: 2870/57013 loss: 5.417756271610611\n",
      "epoch: 0 sentence: 2880/57013 loss: 4.825328815782194\n",
      "epoch: 0 sentence: 2890/57013 loss: 5.533845248013178\n",
      "epoch: 0 sentence: 2900/57013 loss: 4.965598344283224\n",
      "epoch: 0 sentence: 2910/57013 loss: 6.049323400726002\n",
      "epoch: 0 sentence: 2920/57013 loss: 4.7285739113198\n",
      "epoch: 0 sentence: 2930/57013 loss: 5.66537039104004\n",
      "epoch: 0 sentence: 2940/57013 loss: 5.122014985743586\n",
      "epoch: 0 sentence: 2950/57013 loss: 5.86231024783842\n",
      "epoch: 0 sentence: 2960/57013 loss: 4.971931243628579\n",
      "epoch: 0 sentence: 2970/57013 loss: 5.017337395042889\n",
      "epoch: 0 sentence: 2980/57013 loss: 4.292030744628592\n",
      "epoch: 0 sentence: 2990/57013 loss: 5.316152780310989\n",
      "epoch: 0 sentence: 3000/57013 loss: 5.199631404011723\n",
      "epoch: 0 sentence: 3010/57013 loss: 5.021015959194195\n",
      "epoch: 0 sentence: 3020/57013 loss: 5.979678978306538\n",
      "epoch: 0 sentence: 3030/57013 loss: 4.375275068924813\n",
      "epoch: 0 sentence: 3040/57013 loss: 5.760185270359123\n",
      "epoch: 0 sentence: 3050/57013 loss: 4.719383757472159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 3060/57013 loss: 4.661379211588379\n",
      "epoch: 0 sentence: 3070/57013 loss: 6.3217098390922315\n",
      "epoch: 0 sentence: 3080/57013 loss: 3.928294592111104\n",
      "epoch: 0 sentence: 3090/57013 loss: 5.809389110734038\n",
      "epoch: 0 sentence: 3100/57013 loss: 4.834347427354105\n",
      "epoch: 0 sentence: 3110/57013 loss: 5.795412029938113\n",
      "epoch: 0 sentence: 3120/57013 loss: 1.4812870721789375\n",
      "epoch: 0 sentence: 3130/57013 loss: 5.594728991911543\n",
      "epoch: 0 sentence: 3140/57013 loss: 5.50648470334044\n",
      "epoch: 0 sentence: 3150/57013 loss: 5.823493410734148\n",
      "epoch: 0 sentence: 3160/57013 loss: 5.990326015078617\n",
      "epoch: 0 sentence: 3170/57013 loss: 5.733049525074628\n",
      "epoch: 0 sentence: 3180/57013 loss: 5.504076558747172\n",
      "epoch: 0 sentence: 3190/57013 loss: 6.089933593275007\n",
      "epoch: 0 sentence: 3200/57013 loss: 4.8692151390521365\n",
      "epoch: 0 sentence: 3210/57013 loss: 5.695747569482937\n",
      "epoch: 0 sentence: 3220/57013 loss: 3.764996374042273\n",
      "epoch: 0 sentence: 3230/57013 loss: 6.188855792250565\n",
      "epoch: 0 sentence: 3240/57013 loss: 4.426608551668613\n",
      "epoch: 0 sentence: 3250/57013 loss: 5.447745752124885\n",
      "epoch: 0 sentence: 3260/57013 loss: 3.803518403217201\n",
      "epoch: 0 sentence: 3270/57013 loss: 5.087261308871496\n",
      "epoch: 0 sentence: 3280/57013 loss: 4.474898459056225\n",
      "epoch: 0 sentence: 3290/57013 loss: 4.682621309127729\n",
      "epoch: 0 sentence: 3300/57013 loss: 4.917599752652762\n",
      "epoch: 0 sentence: 3310/57013 loss: 5.881529101669899\n",
      "epoch: 0 sentence: 3320/57013 loss: 6.1764950558349225\n",
      "epoch: 0 sentence: 3330/57013 loss: 4.756066189064872\n",
      "epoch: 0 sentence: 3340/57013 loss: 3.7082147638688476\n",
      "epoch: 0 sentence: 3350/57013 loss: 2.6997831415815576\n",
      "epoch: 0 sentence: 3360/57013 loss: 6.087101827691804\n",
      "epoch: 0 sentence: 3370/57013 loss: 4.442290052129707\n",
      "epoch: 0 sentence: 3380/57013 loss: 5.33457685439559\n",
      "epoch: 0 sentence: 3390/57013 loss: 6.983378870757219\n",
      "epoch: 0 sentence: 3400/57013 loss: 5.1169564862512855\n",
      "epoch: 0 sentence: 3410/57013 loss: 5.548526281567113\n",
      "epoch: 0 sentence: 3420/57013 loss: 6.185179804972597\n",
      "epoch: 0 sentence: 3430/57013 loss: 6.183686482129014\n",
      "epoch: 0 sentence: 3440/57013 loss: 6.032089207313612\n",
      "epoch: 0 sentence: 3450/57013 loss: 5.936608608147656\n",
      "epoch: 0 sentence: 3460/57013 loss: 5.644565147684556\n",
      "epoch: 0 sentence: 3470/57013 loss: 5.395692010111374\n",
      "epoch: 0 sentence: 3480/57013 loss: 6.078813992114153\n",
      "epoch: 0 sentence: 3490/57013 loss: 5.712483027411644\n",
      "epoch: 0 sentence: 3500/57013 loss: 5.934447894987089\n",
      "epoch: 0 sentence: 3510/57013 loss: 5.135085001053389\n",
      "epoch: 0 sentence: 3520/57013 loss: 5.668270421606181\n",
      "epoch: 0 sentence: 3530/57013 loss: 5.5529473058759065\n",
      "epoch: 0 sentence: 3540/57013 loss: 5.333925200111492\n",
      "epoch: 0 sentence: 3550/57013 loss: 5.654623945193298\n",
      "epoch: 0 sentence: 3560/57013 loss: 4.834358104627783\n",
      "epoch: 0 sentence: 3570/57013 loss: 6.357348738425053\n",
      "epoch: 0 sentence: 3580/57013 loss: 5.880795619619036\n",
      "epoch: 0 sentence: 3590/57013 loss: 5.775549220955646\n",
      "epoch: 0 sentence: 3600/57013 loss: 5.129174096549637\n",
      "epoch: 0 sentence: 3610/57013 loss: 4.717645616411741\n",
      "epoch: 0 sentence: 3620/57013 loss: 4.6787015084352195\n",
      "epoch: 0 sentence: 3630/57013 loss: 5.366944681153054\n",
      "epoch: 0 sentence: 3640/57013 loss: 5.306768907658942\n",
      "epoch: 0 sentence: 3650/57013 loss: 5.985053615310855\n",
      "epoch: 0 sentence: 3660/57013 loss: 3.7977464458586123\n",
      "epoch: 0 sentence: 3670/57013 loss: 4.51808149781097\n",
      "epoch: 0 sentence: 3680/57013 loss: 5.48407070505096\n",
      "epoch: 0 sentence: 3690/57013 loss: 5.354233526066032\n",
      "epoch: 0 sentence: 3700/57013 loss: 5.636937191445785\n",
      "epoch: 0 sentence: 3710/57013 loss: 4.578467098946314\n",
      "epoch: 0 sentence: 3720/57013 loss: 6.198558046608631\n",
      "epoch: 0 sentence: 3730/57013 loss: 6.465507950696794\n",
      "epoch: 0 sentence: 3740/57013 loss: 3.9274160671142653\n",
      "epoch: 0 sentence: 3750/57013 loss: 5.561609886487358\n",
      "epoch: 0 sentence: 3760/57013 loss: 5.660031394982383\n",
      "epoch: 0 sentence: 3770/57013 loss: 4.0744103051889535\n",
      "epoch: 0 sentence: 3780/57013 loss: 6.349625280945547\n",
      "epoch: 0 sentence: 3790/57013 loss: 5.6437764493789295\n",
      "epoch: 0 sentence: 3800/57013 loss: 6.035023710643\n",
      "epoch: 0 sentence: 3810/57013 loss: 3.379641883503851\n",
      "epoch: 0 sentence: 3820/57013 loss: 6.81209502634499\n",
      "epoch: 0 sentence: 3830/57013 loss: 6.424462652231812\n",
      "epoch: 0 sentence: 3840/57013 loss: 5.911434943128065\n",
      "epoch: 0 sentence: 3850/57013 loss: 5.257738084344607\n",
      "epoch: 0 sentence: 3860/57013 loss: 3.0429363276709616\n",
      "epoch: 0 sentence: 3870/57013 loss: 5.830023882549241\n",
      "epoch: 0 sentence: 3880/57013 loss: 3.8837030543348727\n",
      "epoch: 0 sentence: 3890/57013 loss: 5.574672220322374\n",
      "epoch: 0 sentence: 3900/57013 loss: 3.864198244880625\n",
      "epoch: 0 sentence: 3910/57013 loss: 5.583121359285024\n",
      "epoch: 0 sentence: 3920/57013 loss: 4.591632650057449\n",
      "epoch: 0 sentence: 3930/57013 loss: 5.60528842099875\n",
      "epoch: 0 sentence: 3940/57013 loss: 3.880908377934529\n",
      "epoch: 0 sentence: 3950/57013 loss: 6.426983417294672\n",
      "epoch: 0 sentence: 3960/57013 loss: 4.509853040447983\n",
      "epoch: 0 sentence: 3970/57013 loss: 5.194247296410452\n",
      "epoch: 0 sentence: 3980/57013 loss: 3.9528402767568194\n",
      "epoch: 0 sentence: 3990/57013 loss: 4.241230530310405\n",
      "epoch: 0 sentence: 4000/57013 loss: 5.306048078170496\n",
      "epoch: 0 sentence: 4010/57013 loss: 5.509494669832866\n",
      "epoch: 0 sentence: 4020/57013 loss: 5.11310173169962\n",
      "epoch: 0 sentence: 4030/57013 loss: 4.084142403003109\n",
      "epoch: 0 sentence: 4040/57013 loss: 5.902868310017379\n",
      "epoch: 0 sentence: 4050/57013 loss: 4.984812842536525\n",
      "epoch: 0 sentence: 4060/57013 loss: 6.339728403883365\n",
      "epoch: 0 sentence: 4070/57013 loss: 5.484315724345141\n",
      "epoch: 0 sentence: 4080/57013 loss: 5.409580554040694\n",
      "epoch: 0 sentence: 4090/57013 loss: 5.22766324193883\n",
      "epoch: 0 sentence: 4100/57013 loss: 6.636452787632972\n",
      "epoch: 0 sentence: 4110/57013 loss: 6.457108154450027\n",
      "epoch: 0 sentence: 4120/57013 loss: 6.131731050442191\n",
      "epoch: 0 sentence: 4130/57013 loss: 5.538173603869088\n",
      "epoch: 0 sentence: 4140/57013 loss: 4.95125436513575\n",
      "epoch: 0 sentence: 4150/57013 loss: 6.927490310167035\n",
      "epoch: 0 sentence: 4160/57013 loss: 2.514923912141032\n",
      "epoch: 0 sentence: 4170/57013 loss: 5.104683585324108\n",
      "epoch: 0 sentence: 4180/57013 loss: 5.856261411098946\n",
      "epoch: 0 sentence: 4190/57013 loss: 4.8414987729476815\n",
      "epoch: 0 sentence: 4200/57013 loss: 5.297942368505171\n",
      "epoch: 0 sentence: 4210/57013 loss: 4.693611755140757\n",
      "epoch: 0 sentence: 4220/57013 loss: 4.228070987463943\n",
      "epoch: 0 sentence: 4230/57013 loss: 4.534834153169788\n",
      "epoch: 0 sentence: 4240/57013 loss: 5.224085916948318\n",
      "epoch: 0 sentence: 4250/57013 loss: 6.099574409715639\n",
      "epoch: 0 sentence: 4260/57013 loss: 5.702749443385144\n",
      "epoch: 0 sentence: 4270/57013 loss: 5.114810138527856\n",
      "epoch: 0 sentence: 4280/57013 loss: 4.925280267594689\n",
      "epoch: 0 sentence: 4290/57013 loss: 3.843097616930106\n",
      "epoch: 0 sentence: 4300/57013 loss: 5.313248079062462\n",
      "epoch: 0 sentence: 4310/57013 loss: 5.934801600889715\n",
      "epoch: 0 sentence: 4320/57013 loss: 6.049645263266612\n",
      "epoch: 0 sentence: 4330/57013 loss: 3.0351848696982016\n",
      "epoch: 0 sentence: 4340/57013 loss: 4.039080480216648\n",
      "epoch: 0 sentence: 4350/57013 loss: 7.868833378454777\n",
      "epoch: 0 sentence: 4360/57013 loss: 5.792399138520059\n",
      "epoch: 0 sentence: 4370/57013 loss: 4.755378937393334\n",
      "epoch: 0 sentence: 4380/57013 loss: 5.432502756634762\n",
      "epoch: 0 sentence: 4390/57013 loss: 5.808990671493119\n",
      "epoch: 0 sentence: 4400/57013 loss: 3.7171419282815368\n",
      "epoch: 0 sentence: 4410/57013 loss: 5.872525574852406\n",
      "epoch: 0 sentence: 4420/57013 loss: 3.6490745253457715\n",
      "epoch: 0 sentence: 4430/57013 loss: 4.178496825033921\n",
      "epoch: 0 sentence: 4440/57013 loss: 4.943845319891795\n",
      "epoch: 0 sentence: 4450/57013 loss: 5.907694113315535\n",
      "epoch: 0 sentence: 4460/57013 loss: 4.004175502746737\n",
      "epoch: 0 sentence: 4470/57013 loss: 5.625839663109496\n",
      "epoch: 0 sentence: 4480/57013 loss: 5.892587619852854\n",
      "epoch: 0 sentence: 4490/57013 loss: 5.0038861852364755\n",
      "epoch: 0 sentence: 4500/57013 loss: 2.048213040128774\n",
      "epoch: 0 sentence: 4510/57013 loss: 6.331274621843142\n",
      "epoch: 0 sentence: 4520/57013 loss: 5.325954906571612\n",
      "epoch: 0 sentence: 4530/57013 loss: 5.3172453939495705\n",
      "epoch: 0 sentence: 4540/57013 loss: 5.019970695025921\n",
      "epoch: 0 sentence: 4550/57013 loss: 5.851499191880377\n",
      "epoch: 0 sentence: 4560/57013 loss: 5.299726951552931\n",
      "epoch: 0 sentence: 4570/57013 loss: 4.7121699601672065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 4580/57013 loss: 6.273608837740478\n",
      "epoch: 0 sentence: 4590/57013 loss: 5.503510836189491\n",
      "epoch: 0 sentence: 4600/57013 loss: 3.9512224747007187\n",
      "epoch: 0 sentence: 4610/57013 loss: 4.581331826351061\n",
      "epoch: 0 sentence: 4620/57013 loss: 5.106674917992787\n",
      "epoch: 0 sentence: 4630/57013 loss: 5.449333531485289\n",
      "epoch: 0 sentence: 4640/57013 loss: 5.242643339417535\n",
      "epoch: 0 sentence: 4650/57013 loss: 4.748875095504932\n",
      "epoch: 0 sentence: 4660/57013 loss: 6.219789883009098\n",
      "epoch: 0 sentence: 4670/57013 loss: 4.621856749234476\n",
      "epoch: 0 sentence: 4680/57013 loss: 6.751154781599387\n",
      "epoch: 0 sentence: 4690/57013 loss: 5.124312043693103\n",
      "epoch: 0 sentence: 4700/57013 loss: 5.400350492699829\n",
      "epoch: 0 sentence: 4710/57013 loss: 5.489110123709306\n",
      "epoch: 0 sentence: 4720/57013 loss: 6.700798876133483\n",
      "epoch: 0 sentence: 4730/57013 loss: 5.2704261758313\n",
      "epoch: 0 sentence: 4740/57013 loss: 5.727403237097908\n",
      "epoch: 0 sentence: 4750/57013 loss: 3.678941560170131\n",
      "epoch: 0 sentence: 4760/57013 loss: 4.507416796022586\n",
      "epoch: 0 sentence: 4770/57013 loss: 5.320601094538631\n",
      "epoch: 0 sentence: 4780/57013 loss: 6.9011491729723815\n",
      "epoch: 0 sentence: 4790/57013 loss: 6.043732255187268\n",
      "epoch: 0 sentence: 4800/57013 loss: 5.426670228554563\n",
      "epoch: 0 sentence: 4810/57013 loss: 6.254635006840921\n",
      "epoch: 0 sentence: 4820/57013 loss: 5.4288342687913165\n",
      "epoch: 0 sentence: 4830/57013 loss: 6.512988748001963\n",
      "epoch: 0 sentence: 4840/57013 loss: 6.413728222174679\n",
      "epoch: 0 sentence: 4850/57013 loss: 6.26578840338465\n",
      "epoch: 0 sentence: 4860/57013 loss: 5.691195062790001\n",
      "epoch: 0 sentence: 4870/57013 loss: 4.609299796778414\n",
      "epoch: 0 sentence: 4880/57013 loss: 1.6131146312416353\n",
      "epoch: 0 sentence: 4890/57013 loss: 5.871435180136661\n",
      "epoch: 0 sentence: 4900/57013 loss: 4.880525253875005\n",
      "epoch: 0 sentence: 4910/57013 loss: 5.3698130812683145\n",
      "epoch: 0 sentence: 4920/57013 loss: 5.299561642241761\n",
      "epoch: 0 sentence: 4930/57013 loss: 5.947341577872483\n",
      "epoch: 0 sentence: 4940/57013 loss: 3.9402794501057206\n",
      "epoch: 0 sentence: 4950/57013 loss: 4.836342006600106\n",
      "epoch: 0 sentence: 4960/57013 loss: 5.099980806870403\n",
      "epoch: 0 sentence: 4970/57013 loss: 5.169672282233149\n",
      "epoch: 0 sentence: 4980/57013 loss: 4.692205887271398\n",
      "epoch: 0 sentence: 4990/57013 loss: 4.375432096419528\n",
      "epoch: 0 sentence: 5000/57013 loss: 3.6274653811737334\n",
      "epoch: 0 sentence: 5010/57013 loss: 5.256440545712857\n",
      "epoch: 0 sentence: 5020/57013 loss: 5.120259399710254\n",
      "epoch: 0 sentence: 5030/57013 loss: 4.678882777414284\n",
      "epoch: 0 sentence: 5040/57013 loss: 5.163444541456088\n",
      "epoch: 0 sentence: 5050/57013 loss: 5.004568056162986\n",
      "epoch: 0 sentence: 5060/57013 loss: 4.6651092843683735\n",
      "epoch: 0 sentence: 5070/57013 loss: 6.328432932741268\n",
      "epoch: 0 sentence: 5080/57013 loss: 5.822214942564926\n",
      "epoch: 0 sentence: 5090/57013 loss: 5.574368639808616\n",
      "epoch: 0 sentence: 5100/57013 loss: 2.722677003306359\n",
      "epoch: 0 sentence: 5110/57013 loss: 4.912959460852015\n",
      "epoch: 0 sentence: 5120/57013 loss: 5.985521615160543\n",
      "epoch: 0 sentence: 5130/57013 loss: 4.714260731471135\n",
      "epoch: 0 sentence: 5140/57013 loss: 5.402292520696124\n",
      "epoch: 0 sentence: 5150/57013 loss: 5.824362453039832\n",
      "epoch: 0 sentence: 5160/57013 loss: 4.544806877434185\n",
      "epoch: 0 sentence: 5170/57013 loss: 4.800381299462212\n",
      "epoch: 0 sentence: 5180/57013 loss: 4.437132656940386\n",
      "epoch: 0 sentence: 5190/57013 loss: 5.821357042915834\n",
      "epoch: 0 sentence: 5200/57013 loss: 5.372270641299725\n",
      "epoch: 0 sentence: 5210/57013 loss: 5.178560729583999\n",
      "epoch: 0 sentence: 5220/57013 loss: 4.911250269565126\n",
      "epoch: 0 sentence: 5230/57013 loss: 6.370202491987889\n",
      "epoch: 0 sentence: 5240/57013 loss: 5.853665980368828\n",
      "epoch: 0 sentence: 5250/57013 loss: 5.671425324588074\n",
      "epoch: 0 sentence: 5260/57013 loss: 5.705483623003598\n",
      "epoch: 0 sentence: 5270/57013 loss: 5.065032376640474\n",
      "epoch: 0 sentence: 5280/57013 loss: 4.695487014196873\n",
      "epoch: 0 sentence: 5290/57013 loss: 5.910924701750602\n",
      "epoch: 0 sentence: 5300/57013 loss: 5.192176149333124\n",
      "epoch: 0 sentence: 5310/57013 loss: 5.0997939701833035\n",
      "epoch: 0 sentence: 5320/57013 loss: 5.410293446414985\n",
      "epoch: 0 sentence: 5330/57013 loss: 6.399837267567091\n",
      "epoch: 0 sentence: 5340/57013 loss: 5.894090529627451\n",
      "epoch: 0 sentence: 5350/57013 loss: 6.423005106696199\n",
      "epoch: 0 sentence: 5360/57013 loss: 1.6333813850070316\n",
      "epoch: 0 sentence: 5370/57013 loss: 3.510531961200423\n",
      "epoch: 0 sentence: 5380/57013 loss: 6.23933277056818\n",
      "epoch: 0 sentence: 5390/57013 loss: 4.257210161699401\n",
      "epoch: 0 sentence: 5400/57013 loss: 6.464899358885738\n",
      "epoch: 0 sentence: 5410/57013 loss: 6.448527097010176\n",
      "epoch: 0 sentence: 5420/57013 loss: 4.428515337131747\n",
      "epoch: 0 sentence: 5430/57013 loss: 6.08415723814479\n",
      "epoch: 0 sentence: 5440/57013 loss: 4.0480328818360585\n",
      "epoch: 0 sentence: 5450/57013 loss: 3.693884113831692\n",
      "epoch: 0 sentence: 5460/57013 loss: 4.559570037727117\n",
      "epoch: 0 sentence: 5470/57013 loss: 5.423320960261057\n",
      "epoch: 0 sentence: 5480/57013 loss: 3.621980277474435\n",
      "epoch: 0 sentence: 5490/57013 loss: 4.474949804039986\n",
      "epoch: 0 sentence: 5500/57013 loss: 5.292256401463514\n",
      "epoch: 0 sentence: 5510/57013 loss: 4.891376724542377\n",
      "epoch: 0 sentence: 5520/57013 loss: 5.056413075367648\n",
      "epoch: 0 sentence: 5530/57013 loss: 5.13177296231705\n",
      "epoch: 0 sentence: 5540/57013 loss: 3.3861391788760193\n",
      "epoch: 0 sentence: 5550/57013 loss: 4.910277610366342\n",
      "epoch: 0 sentence: 5560/57013 loss: 4.898652472813287\n",
      "epoch: 0 sentence: 5570/57013 loss: 3.847364530005992\n",
      "epoch: 0 sentence: 5580/57013 loss: 6.621284507471517\n",
      "epoch: 0 sentence: 5590/57013 loss: 5.771538839698858\n",
      "epoch: 0 sentence: 5600/57013 loss: 5.5291512934999005\n",
      "epoch: 0 sentence: 5610/57013 loss: 4.754214069024067\n",
      "epoch: 0 sentence: 5620/57013 loss: 4.5283429089302425\n",
      "epoch: 0 sentence: 5630/57013 loss: 5.274846691552321\n",
      "epoch: 0 sentence: 5640/57013 loss: 5.890032386433159\n",
      "epoch: 0 sentence: 5650/57013 loss: 2.461260420242991\n",
      "epoch: 0 sentence: 5660/57013 loss: 4.858775849136212\n",
      "epoch: 0 sentence: 5670/57013 loss: 5.051688378083057\n",
      "epoch: 0 sentence: 5680/57013 loss: 3.004510098794562\n",
      "epoch: 0 sentence: 5690/57013 loss: 5.2869125405869015\n",
      "epoch: 0 sentence: 5700/57013 loss: 3.960587492845174\n",
      "epoch: 0 sentence: 5710/57013 loss: 4.441099383364679\n",
      "epoch: 0 sentence: 5720/57013 loss: 2.70459997902858\n",
      "epoch: 0 sentence: 5730/57013 loss: 5.573259161014586\n",
      "epoch: 0 sentence: 5740/57013 loss: 4.318835065457885\n",
      "epoch: 0 sentence: 5750/57013 loss: 5.922776065988014\n",
      "epoch: 0 sentence: 5760/57013 loss: 6.337325228242031\n",
      "epoch: 0 sentence: 5770/57013 loss: 6.201494674752681\n",
      "epoch: 0 sentence: 5780/57013 loss: 5.061078116778556\n",
      "epoch: 0 sentence: 5790/57013 loss: 5.328220666592787\n",
      "epoch: 0 sentence: 5800/57013 loss: 5.786789680511703\n",
      "epoch: 0 sentence: 5810/57013 loss: 6.6431251985065485\n",
      "epoch: 0 sentence: 5820/57013 loss: 3.1131671107442456\n",
      "epoch: 0 sentence: 5830/57013 loss: 3.1316758339661854\n",
      "epoch: 0 sentence: 5840/57013 loss: 5.143479775711472\n",
      "epoch: 0 sentence: 5850/57013 loss: 4.567584133435114\n",
      "epoch: 0 sentence: 5860/57013 loss: 5.046663924644343\n",
      "epoch: 0 sentence: 5870/57013 loss: 5.931387128389947\n",
      "epoch: 0 sentence: 5880/57013 loss: 4.384154838464972\n",
      "epoch: 0 sentence: 5890/57013 loss: 6.359134073105541\n",
      "epoch: 0 sentence: 5900/57013 loss: 5.642288429315393\n",
      "epoch: 0 sentence: 5910/57013 loss: 6.109612889194948\n",
      "epoch: 0 sentence: 5920/57013 loss: 5.404854031799282\n",
      "epoch: 0 sentence: 5930/57013 loss: 5.385990193253334\n",
      "epoch: 0 sentence: 5940/57013 loss: 5.652794259610313\n",
      "epoch: 0 sentence: 5950/57013 loss: 5.641868696915033\n",
      "epoch: 0 sentence: 5960/57013 loss: 5.763345545090662\n",
      "epoch: 0 sentence: 5970/57013 loss: 4.660666169771028\n",
      "epoch: 0 sentence: 5980/57013 loss: 5.657742435945145\n",
      "epoch: 0 sentence: 5990/57013 loss: 4.849811982996516\n",
      "epoch: 0 sentence: 6000/57013 loss: 5.441087039385226\n",
      "epoch: 0 sentence: 6010/57013 loss: 5.324914080023705\n",
      "epoch: 0 sentence: 6020/57013 loss: 3.7345882342591032\n",
      "epoch: 0 sentence: 6030/57013 loss: 3.9027568244487574\n",
      "epoch: 0 sentence: 6040/57013 loss: 5.951214051715345\n",
      "epoch: 0 sentence: 6050/57013 loss: 4.2704777366007844\n",
      "epoch: 0 sentence: 6060/57013 loss: 5.187216428887299\n",
      "epoch: 0 sentence: 6070/57013 loss: 5.429754785586924\n",
      "epoch: 0 sentence: 6080/57013 loss: 5.375802323648638\n",
      "epoch: 0 sentence: 6090/57013 loss: 4.979083034060243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 6100/57013 loss: 5.282906381129936\n",
      "epoch: 0 sentence: 6110/57013 loss: 4.635811718612026\n",
      "epoch: 0 sentence: 6120/57013 loss: 5.609831058640465\n",
      "epoch: 0 sentence: 6130/57013 loss: 3.75424435867126\n",
      "epoch: 0 sentence: 6140/57013 loss: 6.480903483043997\n",
      "epoch: 0 sentence: 6150/57013 loss: 5.632942558583939\n",
      "epoch: 0 sentence: 6160/57013 loss: 7.44001359192823\n",
      "epoch: 0 sentence: 6170/57013 loss: 4.334678117245927\n",
      "epoch: 0 sentence: 6180/57013 loss: 5.7233373751759995\n",
      "epoch: 0 sentence: 6190/57013 loss: 4.932580974525039\n",
      "epoch: 0 sentence: 6200/57013 loss: 4.627693284088711\n",
      "epoch: 0 sentence: 6210/57013 loss: 5.542072631496647\n",
      "epoch: 0 sentence: 6220/57013 loss: 6.656849687944029\n",
      "epoch: 0 sentence: 6230/57013 loss: 3.315900853528309\n",
      "epoch: 0 sentence: 6240/57013 loss: 6.228155817452967\n",
      "epoch: 0 sentence: 6250/57013 loss: 6.23127521744038\n",
      "epoch: 0 sentence: 6260/57013 loss: 4.177791259723224\n",
      "epoch: 0 sentence: 6270/57013 loss: 5.095316629911011\n",
      "epoch: 0 sentence: 6280/57013 loss: 5.015790852365875\n",
      "epoch: 0 sentence: 6290/57013 loss: 4.774419202794002\n",
      "epoch: 0 sentence: 6300/57013 loss: 5.6002383818391\n",
      "epoch: 0 sentence: 6310/57013 loss: 4.809190396819342\n",
      "epoch: 0 sentence: 6320/57013 loss: 5.906177937566146\n",
      "epoch: 0 sentence: 6330/57013 loss: 5.4300495584746775\n",
      "epoch: 0 sentence: 6340/57013 loss: 5.701546078228965\n",
      "epoch: 0 sentence: 6350/57013 loss: 3.1861156586942796\n",
      "epoch: 0 sentence: 6360/57013 loss: 6.389099030261054\n",
      "epoch: 0 sentence: 6370/57013 loss: 5.84738056377226\n",
      "epoch: 0 sentence: 6380/57013 loss: 4.432190491033714\n",
      "epoch: 0 sentence: 6390/57013 loss: 4.334119079719739\n",
      "epoch: 0 sentence: 6400/57013 loss: 5.362424385567794\n",
      "epoch: 0 sentence: 6410/57013 loss: 4.48066265997354\n",
      "epoch: 0 sentence: 6420/57013 loss: 3.619188128841836\n",
      "epoch: 0 sentence: 6430/57013 loss: 3.7733212116959094\n",
      "epoch: 0 sentence: 6440/57013 loss: 1.713543999682135\n",
      "epoch: 0 sentence: 6450/57013 loss: 4.535916232905957\n",
      "epoch: 0 sentence: 6460/57013 loss: 4.519761966127378\n",
      "epoch: 0 sentence: 6470/57013 loss: 3.7840425921558745\n",
      "epoch: 0 sentence: 6480/57013 loss: 6.1315416120482915\n",
      "epoch: 0 sentence: 6490/57013 loss: 5.518418300495808\n",
      "epoch: 0 sentence: 6500/57013 loss: 4.5411366503479895\n",
      "epoch: 0 sentence: 6510/57013 loss: 3.995520268475964\n",
      "epoch: 0 sentence: 6520/57013 loss: 5.795603886294301\n",
      "epoch: 0 sentence: 6530/57013 loss: 3.39575736248683\n",
      "epoch: 0 sentence: 6540/57013 loss: 5.136761218485969\n",
      "epoch: 0 sentence: 6550/57013 loss: 4.8336290520695435\n",
      "epoch: 0 sentence: 6560/57013 loss: 5.484432375449348\n",
      "epoch: 0 sentence: 6570/57013 loss: 5.827371267273136\n",
      "epoch: 0 sentence: 6580/57013 loss: 4.670476780731561\n",
      "epoch: 0 sentence: 6590/57013 loss: 4.919745069635105\n",
      "epoch: 0 sentence: 6600/57013 loss: 5.835038699193847\n",
      "epoch: 0 sentence: 6610/57013 loss: 3.774997702579436\n",
      "epoch: 0 sentence: 6620/57013 loss: 4.019823244485982\n",
      "epoch: 0 sentence: 6630/57013 loss: 4.999829510737702\n",
      "epoch: 0 sentence: 6640/57013 loss: 4.97241041312361\n",
      "epoch: 0 sentence: 6650/57013 loss: 5.1367480079823356\n",
      "epoch: 0 sentence: 6660/57013 loss: 4.613806646878159\n",
      "epoch: 0 sentence: 6670/57013 loss: 4.405740549658962\n",
      "epoch: 0 sentence: 6680/57013 loss: 4.475655182771843\n",
      "epoch: 0 sentence: 6690/57013 loss: 3.7214151207959136\n",
      "epoch: 0 sentence: 6700/57013 loss: 5.028900330295146\n",
      "epoch: 0 sentence: 6710/57013 loss: 5.483811703353098\n",
      "epoch: 0 sentence: 6720/57013 loss: 4.351648933161098\n",
      "epoch: 0 sentence: 6730/57013 loss: 4.867036738582256\n",
      "epoch: 0 sentence: 6740/57013 loss: 1.7069175374671868\n",
      "epoch: 0 sentence: 6750/57013 loss: 3.2963434222509047\n",
      "epoch: 0 sentence: 6760/57013 loss: 5.127261066589469\n",
      "epoch: 0 sentence: 6770/57013 loss: 3.584337898950496\n",
      "epoch: 0 sentence: 6780/57013 loss: 5.4278901835165625\n",
      "epoch: 0 sentence: 6790/57013 loss: 4.462838754903397\n",
      "epoch: 0 sentence: 6800/57013 loss: 6.17614842251473\n",
      "epoch: 0 sentence: 6810/57013 loss: 4.962792196474857\n",
      "epoch: 0 sentence: 6820/57013 loss: 5.347079231961089\n",
      "epoch: 0 sentence: 6830/57013 loss: 4.174643077156183\n",
      "epoch: 0 sentence: 6840/57013 loss: 5.014671285424152\n",
      "epoch: 0 sentence: 6850/57013 loss: 5.236268096380461\n",
      "epoch: 0 sentence: 6860/57013 loss: 5.086400907569625\n",
      "epoch: 0 sentence: 6870/57013 loss: 4.327084559563967\n",
      "epoch: 0 sentence: 6880/57013 loss: 5.764778346783233\n",
      "epoch: 0 sentence: 6890/57013 loss: 6.527121487911177\n",
      "epoch: 0 sentence: 6900/57013 loss: 4.24686050595869\n",
      "epoch: 0 sentence: 6910/57013 loss: 4.916015527474484\n",
      "epoch: 0 sentence: 6920/57013 loss: 5.01506056587501\n",
      "epoch: 0 sentence: 6930/57013 loss: 4.778652268029216\n",
      "epoch: 0 sentence: 6940/57013 loss: 5.17922742227613\n",
      "epoch: 0 sentence: 6950/57013 loss: 6.268835531153395\n",
      "epoch: 0 sentence: 6960/57013 loss: 5.187358215087067\n",
      "epoch: 0 sentence: 6970/57013 loss: 5.805572991420296\n",
      "epoch: 0 sentence: 6980/57013 loss: 6.560386884188352\n",
      "epoch: 0 sentence: 6990/57013 loss: 5.07664055674608\n",
      "epoch: 0 sentence: 7000/57013 loss: 4.4981646110381215\n",
      "epoch: 0 sentence: 7010/57013 loss: 5.329630477878784\n",
      "epoch: 0 sentence: 7020/57013 loss: 5.8252860492227825\n",
      "epoch: 0 sentence: 7030/57013 loss: 5.903086581048431\n",
      "epoch: 0 sentence: 7040/57013 loss: 4.866131932875491\n",
      "epoch: 0 sentence: 7050/57013 loss: 4.313252554282516\n",
      "epoch: 0 sentence: 7060/57013 loss: 5.223046474045896\n",
      "epoch: 0 sentence: 7070/57013 loss: 6.11849498171751\n",
      "epoch: 0 sentence: 7080/57013 loss: 6.112311384036989\n",
      "epoch: 0 sentence: 7090/57013 loss: 5.088502200230548\n",
      "epoch: 0 sentence: 7100/57013 loss: 6.302166375693011\n",
      "epoch: 0 sentence: 7110/57013 loss: 5.752213473142905\n",
      "epoch: 0 sentence: 7120/57013 loss: 5.474834493486157\n",
      "epoch: 0 sentence: 7130/57013 loss: 5.222592257703372\n",
      "epoch: 0 sentence: 7140/57013 loss: 3.8617036640589513\n",
      "epoch: 0 sentence: 7150/57013 loss: 5.384208757737214\n",
      "epoch: 0 sentence: 7160/57013 loss: 4.860348422441409\n",
      "epoch: 0 sentence: 7170/57013 loss: 4.006448503540048\n",
      "epoch: 0 sentence: 7180/57013 loss: 5.061218943374473\n",
      "epoch: 0 sentence: 7190/57013 loss: 4.9715011833650244\n",
      "epoch: 0 sentence: 7200/57013 loss: 4.8724137403307735\n",
      "epoch: 0 sentence: 7210/57013 loss: 6.08187157375893\n",
      "epoch: 0 sentence: 7220/57013 loss: 4.643070564094107\n",
      "epoch: 0 sentence: 7230/57013 loss: 4.120139775571297\n",
      "epoch: 0 sentence: 7240/57013 loss: 5.335537868080039\n",
      "epoch: 0 sentence: 7250/57013 loss: 4.774749912034647\n",
      "epoch: 0 sentence: 7260/57013 loss: 4.801837465927327\n",
      "epoch: 0 sentence: 7270/57013 loss: 5.568472124460626\n",
      "epoch: 0 sentence: 7280/57013 loss: 4.967057565336036\n",
      "epoch: 0 sentence: 7290/57013 loss: 5.40641078868512\n",
      "epoch: 0 sentence: 7300/57013 loss: 5.055873765523035\n",
      "epoch: 0 sentence: 7310/57013 loss: 6.019072796221848\n",
      "epoch: 0 sentence: 7320/57013 loss: 5.229862681099321\n",
      "epoch: 0 sentence: 7330/57013 loss: 4.481052221425997\n",
      "epoch: 0 sentence: 7340/57013 loss: 4.714097107470278\n",
      "epoch: 0 sentence: 7350/57013 loss: 6.077876741137951\n",
      "epoch: 0 sentence: 7360/57013 loss: 5.268079920340223\n",
      "epoch: 0 sentence: 7370/57013 loss: 6.348846300610867\n",
      "epoch: 0 sentence: 7380/57013 loss: 5.789129357884569\n",
      "epoch: 0 sentence: 7390/57013 loss: 4.787268424001639\n",
      "epoch: 0 sentence: 7400/57013 loss: 4.513848878148588\n",
      "epoch: 0 sentence: 7410/57013 loss: 5.442129573642374\n",
      "epoch: 0 sentence: 7420/57013 loss: 6.590611551796612\n",
      "epoch: 0 sentence: 7430/57013 loss: 5.180287756924916\n",
      "epoch: 0 sentence: 7440/57013 loss: 3.6408084814450916\n",
      "epoch: 0 sentence: 7450/57013 loss: 3.0129447808716794\n",
      "epoch: 0 sentence: 7460/57013 loss: 4.435112475440271\n",
      "epoch: 0 sentence: 7470/57013 loss: 4.705382505818461\n",
      "epoch: 0 sentence: 7480/57013 loss: 4.282111004376626\n",
      "epoch: 0 sentence: 7490/57013 loss: 3.2482072658870202\n",
      "epoch: 0 sentence: 7500/57013 loss: 4.504718483720866\n",
      "epoch: 0 sentence: 7510/57013 loss: 4.938518608518642\n",
      "epoch: 0 sentence: 7520/57013 loss: 5.813654696334656\n",
      "epoch: 0 sentence: 7530/57013 loss: 4.33415421276728\n",
      "epoch: 0 sentence: 7540/57013 loss: 4.782577046364635\n",
      "epoch: 0 sentence: 7550/57013 loss: 6.5138333923614535\n",
      "epoch: 0 sentence: 7560/57013 loss: 3.9424723900653493\n",
      "epoch: 0 sentence: 7570/57013 loss: 5.203502536575249\n",
      "epoch: 0 sentence: 7580/57013 loss: 2.649162200617036\n",
      "epoch: 0 sentence: 7590/57013 loss: 5.35214965320763\n",
      "epoch: 0 sentence: 7600/57013 loss: 4.890340031307216\n",
      "epoch: 0 sentence: 7610/57013 loss: 5.662996676784971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 7620/57013 loss: 3.6792677064044446\n",
      "epoch: 0 sentence: 7630/57013 loss: 5.660211341093782\n",
      "epoch: 0 sentence: 7640/57013 loss: 4.257904349516416\n",
      "epoch: 0 sentence: 7650/57013 loss: 3.2437188767489054\n",
      "epoch: 0 sentence: 7660/57013 loss: 4.510321265338928\n",
      "epoch: 0 sentence: 7670/57013 loss: 4.370729208626298\n",
      "epoch: 0 sentence: 7680/57013 loss: 5.353896973978813\n",
      "epoch: 0 sentence: 7690/57013 loss: 3.9440203299617633\n",
      "epoch: 0 sentence: 7700/57013 loss: 6.2726283096040385\n",
      "epoch: 0 sentence: 7710/57013 loss: 3.874478675922609\n",
      "epoch: 0 sentence: 7720/57013 loss: 4.036938642365738\n",
      "epoch: 0 sentence: 7730/57013 loss: 5.139021687163095\n",
      "epoch: 0 sentence: 7740/57013 loss: 4.62410462893891\n",
      "epoch: 0 sentence: 7750/57013 loss: 4.129545264211474\n",
      "epoch: 0 sentence: 7760/57013 loss: 6.742717940269524\n",
      "epoch: 0 sentence: 7770/57013 loss: 5.068445940246848\n",
      "epoch: 0 sentence: 7780/57013 loss: 3.3505484344662615\n",
      "epoch: 0 sentence: 7790/57013 loss: 5.545790395252846\n",
      "epoch: 0 sentence: 7800/57013 loss: 3.2699682588393415\n",
      "epoch: 0 sentence: 7810/57013 loss: 5.682904111755514\n",
      "epoch: 0 sentence: 7820/57013 loss: 4.438148928270111\n",
      "epoch: 0 sentence: 7830/57013 loss: 4.30332045594089\n",
      "epoch: 0 sentence: 7840/57013 loss: 6.276428474015324\n",
      "epoch: 0 sentence: 7850/57013 loss: 3.9593139586676807\n",
      "epoch: 0 sentence: 7860/57013 loss: 4.91248637586225\n",
      "epoch: 0 sentence: 7870/57013 loss: 4.735881734619738\n",
      "epoch: 0 sentence: 7880/57013 loss: 4.820181774669179\n",
      "epoch: 0 sentence: 7890/57013 loss: 4.35607906292346\n",
      "epoch: 0 sentence: 7900/57013 loss: 4.131946547542876\n",
      "epoch: 0 sentence: 7910/57013 loss: 4.568814967789884\n",
      "epoch: 0 sentence: 7920/57013 loss: 6.217910843057276\n",
      "epoch: 0 sentence: 7930/57013 loss: 5.035683168030018\n",
      "epoch: 0 sentence: 7940/57013 loss: 5.429036811364299\n",
      "epoch: 0 sentence: 7950/57013 loss: 3.32654097953416\n",
      "epoch: 0 sentence: 7960/57013 loss: 5.114940057335464\n",
      "epoch: 0 sentence: 7970/57013 loss: 4.26007680178733\n",
      "epoch: 0 sentence: 7980/57013 loss: 5.422725449789563\n",
      "epoch: 0 sentence: 7990/57013 loss: 6.982642329691724\n",
      "epoch: 0 sentence: 8000/57013 loss: 3.7021176403020664\n",
      "epoch: 0 sentence: 8010/57013 loss: 5.694624862398656\n",
      "epoch: 0 sentence: 8020/57013 loss: 4.979415368468688\n",
      "epoch: 0 sentence: 8030/57013 loss: 5.05202614603333\n",
      "epoch: 0 sentence: 8040/57013 loss: 5.845268842686102\n",
      "epoch: 0 sentence: 8050/57013 loss: 4.8729570348312015\n",
      "epoch: 0 sentence: 8060/57013 loss: 4.558783584926342\n",
      "epoch: 0 sentence: 8070/57013 loss: 5.100618825751713\n",
      "epoch: 0 sentence: 8080/57013 loss: 5.894734862215736\n",
      "epoch: 0 sentence: 8090/57013 loss: 4.92027918997877\n",
      "epoch: 0 sentence: 8100/57013 loss: 4.595192127627657\n",
      "epoch: 0 sentence: 8110/57013 loss: 3.332039739683848\n",
      "epoch: 0 sentence: 8120/57013 loss: 5.7368860221635325\n",
      "epoch: 0 sentence: 8130/57013 loss: 4.8631795341083945\n",
      "epoch: 0 sentence: 8140/57013 loss: 6.241945158945837\n",
      "epoch: 0 sentence: 8150/57013 loss: 3.7261628947128567\n",
      "epoch: 0 sentence: 8160/57013 loss: 6.309933899434751\n",
      "epoch: 0 sentence: 8170/57013 loss: 5.295440795096459\n",
      "epoch: 0 sentence: 8180/57013 loss: 5.262349689462312\n",
      "epoch: 0 sentence: 8190/57013 loss: 5.440157003308657\n",
      "epoch: 0 sentence: 8200/57013 loss: 5.588035572227941\n",
      "epoch: 0 sentence: 8210/57013 loss: 5.251738463015133\n",
      "epoch: 0 sentence: 8220/57013 loss: 6.474294426663609\n",
      "epoch: 0 sentence: 8230/57013 loss: 5.0931270987581065\n",
      "epoch: 0 sentence: 8240/57013 loss: 3.6698360489447626\n",
      "epoch: 0 sentence: 8250/57013 loss: 5.144589859915731\n",
      "epoch: 0 sentence: 8260/57013 loss: 5.6658363991131715\n",
      "epoch: 0 sentence: 8270/57013 loss: 5.103382442885523\n",
      "epoch: 0 sentence: 8280/57013 loss: 4.435196797944252\n",
      "epoch: 0 sentence: 8290/57013 loss: 6.67957721087944\n",
      "epoch: 0 sentence: 8300/57013 loss: 4.528576676841001\n",
      "epoch: 0 sentence: 8310/57013 loss: 5.774291737132025\n",
      "epoch: 0 sentence: 8320/57013 loss: 4.881952808281642\n",
      "epoch: 0 sentence: 8330/57013 loss: 5.109114377355998\n",
      "epoch: 0 sentence: 8340/57013 loss: 4.362304840939303\n",
      "epoch: 0 sentence: 8350/57013 loss: 4.682996244150954\n",
      "epoch: 0 sentence: 8360/57013 loss: 4.123570316003226\n",
      "epoch: 0 sentence: 8370/57013 loss: 4.978592541929972\n",
      "epoch: 0 sentence: 8380/57013 loss: 4.711149386315209\n",
      "epoch: 0 sentence: 8390/57013 loss: 4.439927041114194\n",
      "epoch: 0 sentence: 8400/57013 loss: 3.700313725307366\n",
      "epoch: 0 sentence: 8410/57013 loss: 6.025705497794297\n",
      "epoch: 0 sentence: 8420/57013 loss: 5.009490047183542\n",
      "epoch: 0 sentence: 8430/57013 loss: 4.41587036662809\n",
      "epoch: 0 sentence: 8440/57013 loss: 4.718152821079635\n",
      "epoch: 0 sentence: 8450/57013 loss: 3.1956242146051377\n",
      "epoch: 0 sentence: 8460/57013 loss: 4.179128238254475\n",
      "epoch: 0 sentence: 8470/57013 loss: 5.3924452444881785\n",
      "epoch: 0 sentence: 8480/57013 loss: 3.9053458500330853\n",
      "epoch: 0 sentence: 8490/57013 loss: 5.220370507822798\n",
      "epoch: 0 sentence: 8500/57013 loss: 4.844684829660398\n",
      "epoch: 0 sentence: 8510/57013 loss: 6.108222825053609\n",
      "epoch: 0 sentence: 8520/57013 loss: 4.202889749358155\n",
      "epoch: 0 sentence: 8530/57013 loss: 5.251669513566291\n",
      "epoch: 0 sentence: 8540/57013 loss: 5.723985865967432\n",
      "epoch: 0 sentence: 8550/57013 loss: 4.85418791852532\n",
      "epoch: 0 sentence: 8560/57013 loss: 4.314525830715369\n",
      "epoch: 0 sentence: 8570/57013 loss: 5.168439886517902\n",
      "epoch: 0 sentence: 8580/57013 loss: 4.795552719666579\n",
      "epoch: 0 sentence: 8590/57013 loss: 5.687698412572006\n",
      "epoch: 0 sentence: 8600/57013 loss: 5.773083586316277\n",
      "epoch: 0 sentence: 8610/57013 loss: 5.472836190640557\n",
      "epoch: 0 sentence: 8620/57013 loss: 5.837223082352647\n",
      "epoch: 0 sentence: 8630/57013 loss: 5.102681512015059\n",
      "epoch: 0 sentence: 8640/57013 loss: 4.199843179525581\n",
      "epoch: 0 sentence: 8650/57013 loss: 5.060686917254724\n",
      "epoch: 0 sentence: 8660/57013 loss: 5.527228348578942\n",
      "epoch: 0 sentence: 8670/57013 loss: 6.154113798552875\n",
      "epoch: 0 sentence: 8680/57013 loss: 4.773047447280297\n",
      "epoch: 0 sentence: 8690/57013 loss: 5.332049898298003\n",
      "epoch: 0 sentence: 8700/57013 loss: 6.531016206730193\n",
      "epoch: 0 sentence: 8710/57013 loss: 4.75685527761628\n",
      "epoch: 0 sentence: 8720/57013 loss: 6.221708115979478\n",
      "epoch: 0 sentence: 8730/57013 loss: 4.407361152743404\n",
      "epoch: 0 sentence: 8740/57013 loss: 4.417065800832313\n",
      "epoch: 0 sentence: 8750/57013 loss: 4.486715784424995\n",
      "epoch: 0 sentence: 8760/57013 loss: 5.923050585037196\n",
      "epoch: 0 sentence: 8770/57013 loss: 7.99621519298185\n",
      "epoch: 0 sentence: 8780/57013 loss: 6.147663019754088\n",
      "epoch: 0 sentence: 8790/57013 loss: 5.435569129267676\n",
      "epoch: 0 sentence: 8800/57013 loss: 4.9452677594803\n",
      "epoch: 0 sentence: 8810/57013 loss: 5.201058942274567\n",
      "epoch: 0 sentence: 8820/57013 loss: 5.165038559150422\n",
      "epoch: 0 sentence: 8830/57013 loss: 5.415318028428066\n",
      "epoch: 0 sentence: 8840/57013 loss: 3.517076683457096\n",
      "epoch: 0 sentence: 8850/57013 loss: 4.321466963216904\n",
      "epoch: 0 sentence: 8860/57013 loss: 4.924255895263183\n",
      "epoch: 0 sentence: 8870/57013 loss: 5.895240844853191\n",
      "epoch: 0 sentence: 8880/57013 loss: 5.030598748930293\n",
      "epoch: 0 sentence: 8890/57013 loss: 5.667287737169891\n",
      "epoch: 0 sentence: 8900/57013 loss: 5.3703846184391\n",
      "epoch: 0 sentence: 8910/57013 loss: 4.104391561038826\n",
      "epoch: 0 sentence: 8920/57013 loss: 5.108594733568103\n",
      "epoch: 0 sentence: 8930/57013 loss: 5.596904505072322\n",
      "epoch: 0 sentence: 8940/57013 loss: 5.394721066002794\n",
      "epoch: 0 sentence: 8950/57013 loss: 5.119362887940562\n",
      "epoch: 0 sentence: 8960/57013 loss: 3.975191711515247\n",
      "epoch: 0 sentence: 8970/57013 loss: 4.627202829404776\n",
      "epoch: 0 sentence: 8980/57013 loss: 4.3463552989598355\n",
      "epoch: 0 sentence: 8990/57013 loss: 3.970116181611003\n",
      "epoch: 0 sentence: 9000/57013 loss: 5.700213483824044\n",
      "epoch: 0 sentence: 9010/57013 loss: 4.959232797514158\n",
      "epoch: 0 sentence: 9020/57013 loss: 5.489675847529422\n",
      "epoch: 0 sentence: 9030/57013 loss: 4.6278459591727925\n",
      "epoch: 0 sentence: 9040/57013 loss: 4.664746747598874\n",
      "epoch: 0 sentence: 9050/57013 loss: 5.368822074115477\n",
      "epoch: 0 sentence: 9060/57013 loss: 4.996287087357688\n",
      "epoch: 0 sentence: 9070/57013 loss: 5.033272862399263\n",
      "epoch: 0 sentence: 9080/57013 loss: 5.875124117655123\n",
      "epoch: 0 sentence: 9090/57013 loss: 4.635477883793791\n",
      "epoch: 0 sentence: 9100/57013 loss: 6.341401503503219\n",
      "epoch: 0 sentence: 9110/57013 loss: 4.983909056853667\n",
      "epoch: 0 sentence: 9120/57013 loss: 6.333468376379608\n",
      "epoch: 0 sentence: 9130/57013 loss: 4.090210980715903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 9140/57013 loss: 5.186356241608262\n",
      "epoch: 0 sentence: 9150/57013 loss: 4.347436431999611\n",
      "epoch: 0 sentence: 9160/57013 loss: 6.672061668880573\n",
      "epoch: 0 sentence: 9170/57013 loss: 4.9999326133045\n",
      "epoch: 0 sentence: 9180/57013 loss: 5.447275766169163\n",
      "epoch: 0 sentence: 9190/57013 loss: 2.1265825646581105\n",
      "epoch: 0 sentence: 9200/57013 loss: 5.092409929335524\n",
      "epoch: 0 sentence: 9210/57013 loss: 4.771048832031451\n",
      "epoch: 0 sentence: 9220/57013 loss: 4.9036912549400435\n",
      "epoch: 0 sentence: 9230/57013 loss: 6.050807382946274\n",
      "epoch: 0 sentence: 9240/57013 loss: 3.704271643880314\n",
      "epoch: 0 sentence: 9250/57013 loss: 3.605414866775879\n",
      "epoch: 0 sentence: 9260/57013 loss: 6.102518386516422\n",
      "epoch: 0 sentence: 9270/57013 loss: 5.683141225926784\n",
      "epoch: 0 sentence: 9280/57013 loss: 5.447085452777804\n",
      "epoch: 0 sentence: 9290/57013 loss: 3.7363093212396974\n",
      "epoch: 0 sentence: 9300/57013 loss: 4.048577822584503\n",
      "epoch: 0 sentence: 9310/57013 loss: 5.119676746609577\n",
      "epoch: 0 sentence: 9320/57013 loss: 4.883838265017816\n",
      "epoch: 0 sentence: 9330/57013 loss: 3.9379946593189117\n",
      "epoch: 0 sentence: 9340/57013 loss: 5.714192146236074\n",
      "epoch: 0 sentence: 9350/57013 loss: 2.5028373623128806\n",
      "epoch: 0 sentence: 9360/57013 loss: 5.532534469355549\n",
      "epoch: 0 sentence: 9370/57013 loss: 3.8475679617496388\n",
      "epoch: 0 sentence: 9380/57013 loss: 4.835297480676936\n",
      "epoch: 0 sentence: 9390/57013 loss: 5.146873924443007\n",
      "epoch: 0 sentence: 9400/57013 loss: 4.877946573521866\n",
      "epoch: 0 sentence: 9410/57013 loss: 4.937628427526362\n",
      "epoch: 0 sentence: 9420/57013 loss: 5.1501168299895195\n",
      "epoch: 0 sentence: 9430/57013 loss: 4.685055910218127\n",
      "epoch: 0 sentence: 9440/57013 loss: 5.8668703149620365\n",
      "epoch: 0 sentence: 9450/57013 loss: 4.633304440174648\n",
      "epoch: 0 sentence: 9460/57013 loss: 4.577544136800429\n",
      "epoch: 0 sentence: 9470/57013 loss: 3.9949595441142827\n",
      "epoch: 0 sentence: 9480/57013 loss: 2.1255498443867604\n",
      "epoch: 0 sentence: 9490/57013 loss: 6.128619411159428\n",
      "epoch: 0 sentence: 9500/57013 loss: 5.731539263875223\n",
      "epoch: 0 sentence: 9510/57013 loss: 6.058144520845273\n",
      "epoch: 0 sentence: 9520/57013 loss: 4.183121384284693\n",
      "epoch: 0 sentence: 9530/57013 loss: 4.512581328398759\n",
      "epoch: 0 sentence: 9540/57013 loss: 5.277054115834889\n",
      "epoch: 0 sentence: 9550/57013 loss: 4.070638838928693\n",
      "epoch: 0 sentence: 9560/57013 loss: 5.1716046947908865\n",
      "epoch: 0 sentence: 9570/57013 loss: 5.620058551088619\n",
      "epoch: 0 sentence: 9580/57013 loss: 6.994349030851139\n",
      "epoch: 0 sentence: 9590/57013 loss: 6.394094208368548\n",
      "epoch: 0 sentence: 9600/57013 loss: 5.516599404950508\n",
      "epoch: 0 sentence: 9610/57013 loss: 4.995696350715044\n",
      "epoch: 0 sentence: 9620/57013 loss: 6.228708004490259\n",
      "epoch: 0 sentence: 9630/57013 loss: 4.286492156411625\n",
      "epoch: 0 sentence: 9640/57013 loss: 5.518105281078199\n",
      "epoch: 0 sentence: 9650/57013 loss: 5.393183419484989\n",
      "epoch: 0 sentence: 9660/57013 loss: 5.698629902748314\n",
      "epoch: 0 sentence: 9670/57013 loss: 5.0353744084517675\n",
      "epoch: 0 sentence: 9680/57013 loss: 5.498290872681512\n",
      "epoch: 0 sentence: 9690/57013 loss: 4.902074750660227\n",
      "epoch: 0 sentence: 9700/57013 loss: 4.681716782680728\n",
      "epoch: 0 sentence: 9710/57013 loss: 5.352997207490686\n",
      "epoch: 0 sentence: 9720/57013 loss: 5.499074157545762\n",
      "epoch: 0 sentence: 9730/57013 loss: 4.729592520032476\n",
      "epoch: 0 sentence: 9740/57013 loss: 6.519339646692212\n",
      "epoch: 0 sentence: 9750/57013 loss: 4.414772341661452\n",
      "epoch: 0 sentence: 9760/57013 loss: 5.33996130861511\n",
      "epoch: 0 sentence: 9770/57013 loss: 6.174918346111545\n",
      "epoch: 0 sentence: 9780/57013 loss: 4.846339923063585\n",
      "epoch: 0 sentence: 9790/57013 loss: 5.964982286959382\n",
      "epoch: 0 sentence: 9800/57013 loss: 3.5838602063700504\n",
      "epoch: 0 sentence: 9810/57013 loss: 5.845185516234402\n",
      "epoch: 0 sentence: 9820/57013 loss: 5.897536524225107\n",
      "epoch: 0 sentence: 9830/57013 loss: 6.4171590139683135\n",
      "epoch: 0 sentence: 9840/57013 loss: 3.0402200527468346\n",
      "epoch: 0 sentence: 9850/57013 loss: 4.495772923945238\n",
      "epoch: 0 sentence: 9860/57013 loss: 5.588683333079884\n",
      "epoch: 0 sentence: 9870/57013 loss: 6.7173386890621725\n",
      "epoch: 0 sentence: 9880/57013 loss: 6.374021809849257\n",
      "epoch: 0 sentence: 9890/57013 loss: 5.311023164536384\n",
      "epoch: 0 sentence: 9900/57013 loss: 4.773683676808042\n",
      "epoch: 0 sentence: 9910/57013 loss: 3.529038507725373\n",
      "epoch: 0 sentence: 9920/57013 loss: 3.23253488696476\n",
      "epoch: 0 sentence: 9930/57013 loss: 6.724872260088033\n",
      "epoch: 0 sentence: 9940/57013 loss: 5.2450308209832395\n",
      "epoch: 0 sentence: 9950/57013 loss: 3.954837827753847\n",
      "epoch: 0 sentence: 9960/57013 loss: 4.1613493103004116\n",
      "epoch: 0 sentence: 9970/57013 loss: 5.162517462461872\n",
      "epoch: 0 sentence: 9980/57013 loss: 5.794951597381877\n",
      "epoch: 0 sentence: 9990/57013 loss: 4.795274086640999\n",
      "epoch: 0 sentence: 10000/57013 loss: 6.233286659713388\n",
      "epoch: 0 sentence: 10010/57013 loss: 6.51764855896826\n",
      "epoch: 0 sentence: 10020/57013 loss: 4.642846662011816\n",
      "epoch: 0 sentence: 10030/57013 loss: 3.970691604624934\n",
      "epoch: 0 sentence: 10040/57013 loss: 5.98491630825297\n",
      "epoch: 0 sentence: 10050/57013 loss: 4.357792999227124\n",
      "epoch: 0 sentence: 10060/57013 loss: 5.6647959954002784\n",
      "epoch: 0 sentence: 10070/57013 loss: 3.59561167848449\n",
      "epoch: 0 sentence: 10080/57013 loss: 4.833049901489955\n",
      "epoch: 0 sentence: 10090/57013 loss: 5.962248148473145\n",
      "epoch: 0 sentence: 10100/57013 loss: 5.955374506160641\n",
      "epoch: 0 sentence: 10110/57013 loss: 5.8222870926763175\n",
      "epoch: 0 sentence: 10120/57013 loss: 5.113030574331186\n",
      "epoch: 0 sentence: 10130/57013 loss: 4.115876802207304\n",
      "epoch: 0 sentence: 10140/57013 loss: 5.53261598556239\n",
      "epoch: 0 sentence: 10150/57013 loss: 3.26645836049732\n",
      "epoch: 0 sentence: 10160/57013 loss: 5.548643172516393\n",
      "epoch: 0 sentence: 10170/57013 loss: 3.1375484018413826\n",
      "epoch: 0 sentence: 10180/57013 loss: 4.321273376457962\n",
      "epoch: 0 sentence: 10190/57013 loss: 4.842612968046217\n",
      "epoch: 0 sentence: 10200/57013 loss: 4.021530940402555\n",
      "epoch: 0 sentence: 10210/57013 loss: 5.083102274749799\n",
      "epoch: 0 sentence: 10220/57013 loss: 4.733913640288636\n",
      "epoch: 0 sentence: 10230/57013 loss: 5.357856147327089\n",
      "epoch: 0 sentence: 10240/57013 loss: 5.6913295078825845\n",
      "epoch: 0 sentence: 10250/57013 loss: 4.783260420393213\n",
      "epoch: 0 sentence: 10260/57013 loss: 1.9368725255348902\n",
      "epoch: 0 sentence: 10270/57013 loss: 3.8386073383453345\n",
      "epoch: 0 sentence: 10280/57013 loss: 5.193492611117711\n",
      "epoch: 0 sentence: 10290/57013 loss: 5.853948228197516\n",
      "epoch: 0 sentence: 10300/57013 loss: 3.252622979568784\n",
      "epoch: 0 sentence: 10310/57013 loss: 4.461101598080608\n",
      "epoch: 0 sentence: 10320/57013 loss: 6.540592996145743\n",
      "epoch: 0 sentence: 10330/57013 loss: 5.599764004476083\n",
      "epoch: 0 sentence: 10340/57013 loss: 5.361297245526215\n",
      "epoch: 0 sentence: 10350/57013 loss: 4.5133894158532\n",
      "epoch: 0 sentence: 10360/57013 loss: 5.375330130049044\n",
      "epoch: 0 sentence: 10370/57013 loss: 5.06030081028132\n",
      "epoch: 0 sentence: 10380/57013 loss: 4.824554156723009\n",
      "epoch: 0 sentence: 10390/57013 loss: 5.37451351209249\n",
      "epoch: 0 sentence: 10400/57013 loss: 4.194830131564025\n",
      "epoch: 0 sentence: 10410/57013 loss: 5.706456303070588\n",
      "epoch: 0 sentence: 10420/57013 loss: 3.6449027157133225\n",
      "epoch: 0 sentence: 10430/57013 loss: 4.290537512840113\n",
      "epoch: 0 sentence: 10440/57013 loss: 1.773103224402965\n",
      "epoch: 0 sentence: 10450/57013 loss: 5.387214963196168\n",
      "epoch: 0 sentence: 10460/57013 loss: 4.648469027316578\n",
      "epoch: 0 sentence: 10470/57013 loss: 5.719006714084501\n",
      "epoch: 0 sentence: 10480/57013 loss: 5.031249966124827\n",
      "epoch: 0 sentence: 10490/57013 loss: 3.946176562991709\n",
      "epoch: 0 sentence: 10500/57013 loss: 6.080493826850131\n",
      "epoch: 0 sentence: 10510/57013 loss: 6.326516911182273\n",
      "epoch: 0 sentence: 10520/57013 loss: 6.0697657041297015\n",
      "epoch: 0 sentence: 10530/57013 loss: 6.389476261547984\n",
      "epoch: 0 sentence: 10540/57013 loss: 5.483015869317296\n",
      "epoch: 0 sentence: 10550/57013 loss: 6.924038410064208\n",
      "epoch: 0 sentence: 10560/57013 loss: 3.9787255218146385\n",
      "epoch: 0 sentence: 10570/57013 loss: 4.538959969921944\n",
      "epoch: 0 sentence: 10580/57013 loss: 6.007828889422968\n",
      "epoch: 0 sentence: 10590/57013 loss: 4.49424957894254\n",
      "epoch: 0 sentence: 10600/57013 loss: 4.977398224741658\n",
      "epoch: 0 sentence: 10610/57013 loss: 5.21148049956244\n",
      "epoch: 0 sentence: 10620/57013 loss: 5.047246921159229\n",
      "epoch: 0 sentence: 10630/57013 loss: 3.7270446896706746\n",
      "epoch: 0 sentence: 10640/57013 loss: 6.835828060083623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 10650/57013 loss: 4.30534652110017\n",
      "epoch: 0 sentence: 10660/57013 loss: 5.563348392032291\n",
      "epoch: 0 sentence: 10670/57013 loss: 4.44374089889303\n",
      "epoch: 0 sentence: 10680/57013 loss: 6.373305896545898\n",
      "epoch: 0 sentence: 10690/57013 loss: 4.909106446575087\n",
      "epoch: 0 sentence: 10700/57013 loss: 5.804349304244278\n",
      "epoch: 0 sentence: 10710/57013 loss: 3.9199371722677623\n",
      "epoch: 0 sentence: 10720/57013 loss: 4.533398633743326\n",
      "epoch: 0 sentence: 10730/57013 loss: 5.787827423493429\n",
      "epoch: 0 sentence: 10740/57013 loss: 4.128655344521801\n",
      "epoch: 0 sentence: 10750/57013 loss: 4.973194789233464\n",
      "epoch: 0 sentence: 10760/57013 loss: 5.701185312293625\n",
      "epoch: 0 sentence: 10770/57013 loss: 5.4047668105117745\n",
      "epoch: 0 sentence: 10780/57013 loss: 4.5530544937680775\n",
      "epoch: 0 sentence: 10790/57013 loss: 1.4955485891718887\n",
      "epoch: 0 sentence: 10800/57013 loss: 5.726593117679253\n",
      "epoch: 0 sentence: 10810/57013 loss: 5.466419129076961\n",
      "epoch: 0 sentence: 10820/57013 loss: 5.3540156963408325\n",
      "epoch: 0 sentence: 10830/57013 loss: 5.4567512122450275\n",
      "epoch: 0 sentence: 10840/57013 loss: 4.5316442330847195\n",
      "epoch: 0 sentence: 10850/57013 loss: 4.82701052547735\n",
      "epoch: 0 sentence: 10860/57013 loss: 4.788817514665943\n",
      "epoch: 0 sentence: 10870/57013 loss: 5.402045335989583\n",
      "epoch: 0 sentence: 10880/57013 loss: 5.526268866476544\n",
      "epoch: 0 sentence: 10890/57013 loss: 5.883699706837725\n",
      "epoch: 0 sentence: 10900/57013 loss: 6.389676278235316\n",
      "epoch: 0 sentence: 10910/57013 loss: 5.310860830348191\n",
      "epoch: 0 sentence: 10920/57013 loss: 5.604897698335156\n",
      "epoch: 0 sentence: 10930/57013 loss: 4.965941731455599\n",
      "epoch: 0 sentence: 10940/57013 loss: 5.418903244827948\n",
      "epoch: 0 sentence: 10950/57013 loss: 3.601944299074369\n",
      "epoch: 0 sentence: 10960/57013 loss: 5.306158871318407\n",
      "epoch: 0 sentence: 10970/57013 loss: 5.581889948285873\n",
      "epoch: 0 sentence: 10980/57013 loss: 4.0035502735154544\n",
      "epoch: 0 sentence: 10990/57013 loss: 3.562470713187576\n",
      "epoch: 0 sentence: 11000/57013 loss: 3.26651235970854\n",
      "epoch: 0 sentence: 11010/57013 loss: 5.001596833170009\n",
      "epoch: 0 sentence: 11020/57013 loss: 5.551580076995559\n",
      "epoch: 0 sentence: 11030/57013 loss: 5.408122777383798\n",
      "epoch: 0 sentence: 11040/57013 loss: 3.8411712031643916\n",
      "epoch: 0 sentence: 11050/57013 loss: 5.374822658417821\n",
      "epoch: 0 sentence: 11060/57013 loss: 5.090659378441717\n",
      "epoch: 0 sentence: 11070/57013 loss: 5.841060446032025\n",
      "epoch: 0 sentence: 11080/57013 loss: 5.654516123696612\n",
      "epoch: 0 sentence: 11090/57013 loss: 4.115423208237628\n",
      "epoch: 0 sentence: 11100/57013 loss: 5.057726791457758\n",
      "epoch: 0 sentence: 11110/57013 loss: 4.077153851145975\n",
      "epoch: 0 sentence: 11120/57013 loss: 5.376787504034056\n",
      "epoch: 0 sentence: 11130/57013 loss: 6.007612681882558\n",
      "epoch: 0 sentence: 11140/57013 loss: 4.624948431336183\n",
      "epoch: 0 sentence: 11150/57013 loss: 5.49860889993312\n",
      "epoch: 0 sentence: 11160/57013 loss: 5.512579719441677\n",
      "epoch: 0 sentence: 11170/57013 loss: 3.4355955104235485\n",
      "epoch: 0 sentence: 11180/57013 loss: 5.7202138252834365\n",
      "epoch: 0 sentence: 11190/57013 loss: 5.62890853473925\n",
      "epoch: 0 sentence: 11200/57013 loss: 2.890028964359448\n",
      "epoch: 0 sentence: 11210/57013 loss: 5.057528265907136\n",
      "epoch: 0 sentence: 11220/57013 loss: 4.827109566220148\n",
      "epoch: 0 sentence: 11230/57013 loss: 4.062575766916899\n",
      "epoch: 0 sentence: 11240/57013 loss: 4.544503867643692\n",
      "epoch: 0 sentence: 11250/57013 loss: 3.78698386886985\n",
      "epoch: 0 sentence: 11260/57013 loss: 4.993295624591167\n",
      "epoch: 0 sentence: 11270/57013 loss: 5.782430382478493\n",
      "epoch: 0 sentence: 11280/57013 loss: 4.865774827514015\n",
      "epoch: 0 sentence: 11290/57013 loss: 5.96691097947939\n",
      "epoch: 0 sentence: 11300/57013 loss: 5.183604325881541\n",
      "epoch: 0 sentence: 11310/57013 loss: 4.630852209610712\n",
      "epoch: 0 sentence: 11320/57013 loss: 3.4172223285034704\n",
      "epoch: 0 sentence: 11330/57013 loss: 5.327545589668702\n",
      "epoch: 0 sentence: 11340/57013 loss: 5.155020156472433\n",
      "epoch: 0 sentence: 11350/57013 loss: 5.477768723873442\n",
      "epoch: 0 sentence: 11360/57013 loss: 6.094343443069973\n",
      "epoch: 0 sentence: 11370/57013 loss: 4.583222113584362\n",
      "epoch: 0 sentence: 11380/57013 loss: 5.023500145169999\n",
      "epoch: 0 sentence: 11390/57013 loss: 4.40511936594498\n",
      "epoch: 0 sentence: 11400/57013 loss: 6.380454937323089\n",
      "epoch: 0 sentence: 11410/57013 loss: 5.571208333723423\n",
      "epoch: 0 sentence: 11420/57013 loss: 4.589531805719495\n",
      "epoch: 0 sentence: 11430/57013 loss: 4.380005095672789\n",
      "epoch: 0 sentence: 11440/57013 loss: 5.854328884079548\n",
      "epoch: 0 sentence: 11450/57013 loss: 4.112150847412476\n",
      "epoch: 0 sentence: 11460/57013 loss: 5.75421197180817\n",
      "epoch: 0 sentence: 11470/57013 loss: 5.130076568094637\n",
      "epoch: 0 sentence: 11480/57013 loss: 5.369414033853028\n",
      "epoch: 0 sentence: 11490/57013 loss: 3.3226102713688466\n",
      "epoch: 0 sentence: 11500/57013 loss: 5.532233396439873\n",
      "epoch: 0 sentence: 11510/57013 loss: 4.742256924982766\n",
      "epoch: 0 sentence: 11520/57013 loss: 5.749091073808204\n",
      "epoch: 0 sentence: 11530/57013 loss: 5.936216673635153\n",
      "epoch: 0 sentence: 11540/57013 loss: 5.7148420373147735\n",
      "epoch: 0 sentence: 11550/57013 loss: 5.394809645045111\n",
      "epoch: 0 sentence: 11560/57013 loss: 4.849808653007249\n",
      "epoch: 0 sentence: 11570/57013 loss: 5.544944082575487\n",
      "epoch: 0 sentence: 11580/57013 loss: 4.988364025513321\n",
      "epoch: 0 sentence: 11590/57013 loss: 4.482444240433811\n",
      "epoch: 0 sentence: 11600/57013 loss: 3.9455443406208377\n",
      "epoch: 0 sentence: 11610/57013 loss: 4.576638366915646\n",
      "epoch: 0 sentence: 11620/57013 loss: 5.15137690418865\n",
      "epoch: 0 sentence: 11630/57013 loss: 4.540100744563348\n",
      "epoch: 0 sentence: 11640/57013 loss: 5.623432621518055\n",
      "epoch: 0 sentence: 11650/57013 loss: 5.454729563353378\n",
      "epoch: 0 sentence: 11660/57013 loss: 5.726358761079917\n",
      "epoch: 0 sentence: 11670/57013 loss: 5.098327212888937\n",
      "epoch: 0 sentence: 11680/57013 loss: 4.950614469386418\n",
      "epoch: 0 sentence: 11690/57013 loss: 5.58765342271629\n",
      "epoch: 0 sentence: 11700/57013 loss: 3.7889222806549636\n",
      "epoch: 0 sentence: 11710/57013 loss: 4.720177489757642\n",
      "epoch: 0 sentence: 11720/57013 loss: 5.9427042532243\n",
      "epoch: 0 sentence: 11730/57013 loss: 5.234459173497554\n",
      "epoch: 0 sentence: 11740/57013 loss: 5.042964347312503\n",
      "epoch: 0 sentence: 11750/57013 loss: 4.472016862060361\n",
      "epoch: 0 sentence: 11760/57013 loss: 4.886887863674174\n",
      "epoch: 0 sentence: 11770/57013 loss: 5.166072838625337\n",
      "epoch: 0 sentence: 11780/57013 loss: 2.8588354512310317\n",
      "epoch: 0 sentence: 11790/57013 loss: 3.0167816202924524\n",
      "epoch: 0 sentence: 11800/57013 loss: 5.589588596027126\n",
      "epoch: 0 sentence: 11810/57013 loss: 5.240895532550197\n",
      "epoch: 0 sentence: 11820/57013 loss: 5.6745661913472505\n",
      "epoch: 0 sentence: 11830/57013 loss: 3.2394153057689152\n",
      "epoch: 0 sentence: 11840/57013 loss: 4.396160517132183\n",
      "epoch: 0 sentence: 11850/57013 loss: 5.652191210635565\n",
      "epoch: 0 sentence: 11860/57013 loss: 3.6136258093522047\n",
      "epoch: 0 sentence: 11870/57013 loss: 4.359202539940553\n",
      "epoch: 0 sentence: 11880/57013 loss: 4.503966728416495\n",
      "epoch: 0 sentence: 11890/57013 loss: 5.180394986661546\n",
      "epoch: 0 sentence: 11900/57013 loss: 5.067708754995127\n",
      "epoch: 0 sentence: 11910/57013 loss: 1.7756167517853694\n",
      "epoch: 0 sentence: 11920/57013 loss: 5.230932919282644\n",
      "epoch: 0 sentence: 11930/57013 loss: 6.4814499867144235\n",
      "epoch: 0 sentence: 11940/57013 loss: 6.171288638593779\n",
      "epoch: 0 sentence: 11950/57013 loss: 5.989260669968618\n",
      "epoch: 0 sentence: 11960/57013 loss: 5.324542376337097\n",
      "epoch: 0 sentence: 11970/57013 loss: 4.479922291187777\n",
      "epoch: 0 sentence: 11980/57013 loss: 4.67869940775766\n",
      "epoch: 0 sentence: 11990/57013 loss: 3.6828393345958044\n",
      "epoch: 0 sentence: 12000/57013 loss: 5.488366714476831\n",
      "epoch: 0 sentence: 12010/57013 loss: 6.638556963984652\n",
      "epoch: 0 sentence: 12020/57013 loss: 5.647234919504012\n",
      "epoch: 0 sentence: 12030/57013 loss: 4.151114943097948\n",
      "epoch: 0 sentence: 12040/57013 loss: 3.350336665595161\n",
      "epoch: 0 sentence: 12050/57013 loss: 5.3445343465003505\n",
      "epoch: 0 sentence: 12060/57013 loss: 3.0209288416175926\n",
      "epoch: 0 sentence: 12070/57013 loss: 4.205763588001641\n",
      "epoch: 0 sentence: 12080/57013 loss: 4.788661187784634\n",
      "epoch: 0 sentence: 12090/57013 loss: 4.671678428081439\n",
      "epoch: 0 sentence: 12100/57013 loss: 6.116929075928253\n",
      "epoch: 0 sentence: 12110/57013 loss: 4.682943071911071\n",
      "epoch: 0 sentence: 12120/57013 loss: 4.551805734522961\n",
      "epoch: 0 sentence: 12130/57013 loss: 5.517667372135092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 12140/57013 loss: 5.2404822451613144\n",
      "epoch: 0 sentence: 12150/57013 loss: 4.237678470051593\n",
      "epoch: 0 sentence: 12160/57013 loss: 1.9071546376173738\n",
      "epoch: 0 sentence: 12170/57013 loss: 4.933424689961165\n",
      "epoch: 0 sentence: 12180/57013 loss: 1.5756415823793515\n",
      "epoch: 0 sentence: 12190/57013 loss: 6.4426073739359575\n",
      "epoch: 0 sentence: 12200/57013 loss: 3.4027951173543833\n",
      "epoch: 0 sentence: 12210/57013 loss: 5.3085568753155155\n",
      "epoch: 0 sentence: 12220/57013 loss: 4.61873395910317\n",
      "epoch: 0 sentence: 12230/57013 loss: 5.550969031480569\n",
      "epoch: 0 sentence: 12240/57013 loss: 4.645838297896059\n",
      "epoch: 0 sentence: 12250/57013 loss: 3.7113744350582643\n",
      "epoch: 0 sentence: 12260/57013 loss: 5.167335571751707\n",
      "epoch: 0 sentence: 12270/57013 loss: 5.712750696191784\n",
      "epoch: 0 sentence: 12280/57013 loss: 4.586592813982139\n",
      "epoch: 0 sentence: 12290/57013 loss: 5.346184040708392\n",
      "epoch: 0 sentence: 12300/57013 loss: 5.5081178146021506\n",
      "epoch: 0 sentence: 12310/57013 loss: 4.757417802529427\n",
      "epoch: 0 sentence: 12320/57013 loss: 5.08977627294846\n",
      "epoch: 0 sentence: 12330/57013 loss: 4.354889427221303\n",
      "epoch: 0 sentence: 12340/57013 loss: 3.899728692384445\n",
      "epoch: 0 sentence: 12350/57013 loss: 4.424821730064827\n",
      "epoch: 0 sentence: 12360/57013 loss: 4.354372660053463\n",
      "epoch: 0 sentence: 12370/57013 loss: 4.633351414477715\n",
      "epoch: 0 sentence: 12380/57013 loss: 4.205248537429696\n",
      "epoch: 0 sentence: 12390/57013 loss: 5.907732591164282\n",
      "epoch: 0 sentence: 12400/57013 loss: 5.5096344572801135\n",
      "epoch: 0 sentence: 12410/57013 loss: 5.601796533959176\n",
      "epoch: 0 sentence: 12420/57013 loss: 4.864684227843263\n",
      "epoch: 0 sentence: 12430/57013 loss: 4.371021642030848\n",
      "epoch: 0 sentence: 12440/57013 loss: 4.778522202247204\n",
      "epoch: 0 sentence: 12450/57013 loss: 4.944217031748069\n",
      "epoch: 0 sentence: 12460/57013 loss: 5.240906586480275\n",
      "epoch: 0 sentence: 12470/57013 loss: 5.5494465788610885\n",
      "epoch: 0 sentence: 12480/57013 loss: 5.100567289971865\n",
      "epoch: 0 sentence: 12490/57013 loss: 4.3391622502475435\n",
      "epoch: 0 sentence: 12500/57013 loss: 5.102270910542345\n",
      "epoch: 0 sentence: 12510/57013 loss: 5.417310179288791\n",
      "epoch: 0 sentence: 12520/57013 loss: 3.4002904949448243\n",
      "epoch: 0 sentence: 12530/57013 loss: 4.777229276196962\n",
      "epoch: 0 sentence: 12540/57013 loss: 4.786677551460257\n",
      "epoch: 0 sentence: 12550/57013 loss: 6.366675366573116\n",
      "epoch: 0 sentence: 12560/57013 loss: 4.3497220673973525\n",
      "epoch: 0 sentence: 12570/57013 loss: 4.358807715708366\n",
      "epoch: 0 sentence: 12580/57013 loss: 5.049660273048472\n",
      "epoch: 0 sentence: 12590/57013 loss: 5.02890538562101\n",
      "epoch: 0 sentence: 12600/57013 loss: 4.6468782372959145\n",
      "epoch: 0 sentence: 12610/57013 loss: 4.813665092586463\n",
      "epoch: 0 sentence: 12620/57013 loss: 5.445493377833799\n",
      "epoch: 0 sentence: 12630/57013 loss: 5.586389358149805\n",
      "epoch: 0 sentence: 12640/57013 loss: 3.9160323962832453\n",
      "epoch: 0 sentence: 12650/57013 loss: 4.708987953309527\n",
      "epoch: 0 sentence: 12660/57013 loss: 3.5875779596862203\n",
      "epoch: 0 sentence: 12670/57013 loss: 5.231819039885034\n",
      "epoch: 0 sentence: 12680/57013 loss: 4.768984264107681\n",
      "epoch: 0 sentence: 12690/57013 loss: 5.129847109244046\n",
      "epoch: 0 sentence: 12700/57013 loss: 2.820295547496592\n",
      "epoch: 0 sentence: 12710/57013 loss: 4.53456697037227\n",
      "epoch: 0 sentence: 12720/57013 loss: 5.144438143138634\n",
      "epoch: 0 sentence: 12730/57013 loss: 4.47566058456621\n",
      "epoch: 0 sentence: 12740/57013 loss: 5.225673670092315\n",
      "epoch: 0 sentence: 12750/57013 loss: 3.7499335490337247\n",
      "epoch: 0 sentence: 12760/57013 loss: 5.2675085647902895\n",
      "epoch: 0 sentence: 12770/57013 loss: 5.582668276097925\n",
      "epoch: 0 sentence: 12780/57013 loss: 2.968100901094764\n",
      "epoch: 0 sentence: 12790/57013 loss: 5.801063356875666\n",
      "epoch: 0 sentence: 12800/57013 loss: 5.864123333700351\n",
      "epoch: 0 sentence: 12810/57013 loss: 5.068809267363475\n",
      "epoch: 0 sentence: 12820/57013 loss: 1.5994874314724334\n",
      "epoch: 0 sentence: 12830/57013 loss: 4.652893166104846\n",
      "epoch: 0 sentence: 12840/57013 loss: 4.685016779695951\n",
      "epoch: 0 sentence: 12850/57013 loss: 4.201605581979343\n",
      "epoch: 0 sentence: 12860/57013 loss: 5.701192731429009\n",
      "epoch: 0 sentence: 12870/57013 loss: 6.603660624970646\n",
      "epoch: 0 sentence: 12880/57013 loss: 5.195481289015919\n",
      "epoch: 0 sentence: 12890/57013 loss: 5.549690441974389\n",
      "epoch: 0 sentence: 12900/57013 loss: 5.3543991260627\n",
      "epoch: 0 sentence: 12910/57013 loss: 5.360950796455339\n",
      "epoch: 0 sentence: 12920/57013 loss: 5.128025767623238\n",
      "epoch: 0 sentence: 12930/57013 loss: 5.144640773065647\n",
      "epoch: 0 sentence: 12940/57013 loss: 3.9362031767830503\n",
      "epoch: 0 sentence: 12950/57013 loss: 4.89714415258109\n",
      "epoch: 0 sentence: 12960/57013 loss: 5.994161747073024\n",
      "epoch: 0 sentence: 12970/57013 loss: 6.4865734193265965\n",
      "epoch: 0 sentence: 12980/57013 loss: 5.790342472543409\n",
      "epoch: 0 sentence: 12990/57013 loss: 5.134023818791999\n",
      "epoch: 0 sentence: 13000/57013 loss: 5.775270743103507\n",
      "epoch: 0 sentence: 13010/57013 loss: 3.3508453342764306\n",
      "epoch: 0 sentence: 13020/57013 loss: 5.706377664934415\n",
      "epoch: 0 sentence: 13030/57013 loss: 2.1079463982442923\n",
      "epoch: 0 sentence: 13040/57013 loss: 5.38687474306239\n",
      "epoch: 0 sentence: 13050/57013 loss: 6.132595082663401\n",
      "epoch: 0 sentence: 13060/57013 loss: 5.424455288659113\n",
      "epoch: 0 sentence: 13070/57013 loss: 5.325792616744482\n",
      "epoch: 0 sentence: 13080/57013 loss: 4.721617736581717\n",
      "epoch: 0 sentence: 13090/57013 loss: 4.990137922682078\n",
      "epoch: 0 sentence: 13100/57013 loss: 4.655914826091531\n",
      "epoch: 0 sentence: 13110/57013 loss: 5.783024011633543\n",
      "epoch: 0 sentence: 13120/57013 loss: 4.987444601684544\n",
      "epoch: 0 sentence: 13130/57013 loss: 5.824746891445379\n",
      "epoch: 0 sentence: 13140/57013 loss: 5.580100350356069\n",
      "epoch: 0 sentence: 13150/57013 loss: 6.104967195556957\n",
      "epoch: 0 sentence: 13160/57013 loss: 6.813685625033059\n",
      "epoch: 0 sentence: 13170/57013 loss: 3.370173180897438\n",
      "epoch: 0 sentence: 13180/57013 loss: 3.021358114562983\n",
      "epoch: 0 sentence: 13190/57013 loss: 5.425098799862818\n",
      "epoch: 0 sentence: 13200/57013 loss: 4.952359731521092\n",
      "epoch: 0 sentence: 13210/57013 loss: 2.641607143632127\n",
      "epoch: 0 sentence: 13220/57013 loss: 4.311193381988952\n",
      "epoch: 0 sentence: 13230/57013 loss: 4.309168168236009\n",
      "epoch: 0 sentence: 13240/57013 loss: 5.464400379408953\n",
      "epoch: 0 sentence: 13250/57013 loss: 4.578068219066794\n",
      "epoch: 0 sentence: 13260/57013 loss: 5.519298337224885\n",
      "epoch: 0 sentence: 13270/57013 loss: 5.989450799535453\n",
      "epoch: 0 sentence: 13280/57013 loss: 5.637944065736546\n",
      "epoch: 0 sentence: 13290/57013 loss: 1.6090109224879936\n",
      "epoch: 0 sentence: 13300/57013 loss: 5.782903005358451\n",
      "epoch: 0 sentence: 13310/57013 loss: 5.957927376372281\n",
      "epoch: 0 sentence: 13320/57013 loss: 3.8186118510165845\n",
      "epoch: 0 sentence: 13330/57013 loss: 5.494956296973209\n",
      "epoch: 0 sentence: 13340/57013 loss: 4.143229815851154\n",
      "epoch: 0 sentence: 13350/57013 loss: 4.337319794645118\n",
      "epoch: 0 sentence: 13360/57013 loss: 6.4049846225485325\n",
      "epoch: 0 sentence: 13370/57013 loss: 4.363705085727356\n",
      "epoch: 0 sentence: 13380/57013 loss: 4.207019087743076\n",
      "epoch: 0 sentence: 13390/57013 loss: 4.526383895195109\n",
      "epoch: 0 sentence: 13400/57013 loss: 5.001572302545421\n",
      "epoch: 0 sentence: 13410/57013 loss: 3.523960749437847\n",
      "epoch: 0 sentence: 13420/57013 loss: 4.646022389889216\n",
      "epoch: 0 sentence: 13430/57013 loss: 6.114346302887133\n",
      "epoch: 0 sentence: 13440/57013 loss: 6.61186100280573\n",
      "epoch: 0 sentence: 13450/57013 loss: 5.060878658493454\n",
      "epoch: 0 sentence: 13460/57013 loss: 3.737568053383839\n",
      "epoch: 0 sentence: 13470/57013 loss: 6.519395996738951\n",
      "epoch: 0 sentence: 13480/57013 loss: 4.579379104693956\n",
      "epoch: 0 sentence: 13490/57013 loss: 5.156127494256791\n",
      "epoch: 0 sentence: 13500/57013 loss: 5.782807733078679\n",
      "epoch: 0 sentence: 13510/57013 loss: 4.071374896629977\n",
      "epoch: 0 sentence: 13520/57013 loss: 4.403416217650022\n",
      "epoch: 0 sentence: 13530/57013 loss: 4.4757151920824345\n",
      "epoch: 0 sentence: 13540/57013 loss: 3.9662733231547453\n",
      "epoch: 0 sentence: 13550/57013 loss: 6.341377328401742\n",
      "epoch: 0 sentence: 13560/57013 loss: 4.764791276188263\n",
      "epoch: 0 sentence: 13570/57013 loss: 5.490604158426179\n",
      "epoch: 0 sentence: 13580/57013 loss: 4.4007098295257885\n",
      "epoch: 0 sentence: 13590/57013 loss: 4.828416745911946\n",
      "epoch: 0 sentence: 13600/57013 loss: 2.109409299846751\n",
      "epoch: 0 sentence: 13610/57013 loss: 3.7716818850456124\n",
      "epoch: 0 sentence: 13620/57013 loss: 6.179615132546065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 13630/57013 loss: 5.167852910242463\n",
      "epoch: 0 sentence: 13640/57013 loss: 4.888165931457821\n",
      "epoch: 0 sentence: 13650/57013 loss: 4.395810434637934\n",
      "epoch: 0 sentence: 13660/57013 loss: 4.369322736588442\n",
      "epoch: 0 sentence: 13670/57013 loss: 5.004380234720471\n",
      "epoch: 0 sentence: 13680/57013 loss: 5.651964691785228\n",
      "epoch: 0 sentence: 13690/57013 loss: 5.230915057435187\n",
      "epoch: 0 sentence: 13700/57013 loss: 4.691685400834347\n",
      "epoch: 0 sentence: 13710/57013 loss: 6.408041413929304\n",
      "epoch: 0 sentence: 13720/57013 loss: 4.454736172761313\n",
      "epoch: 0 sentence: 13730/57013 loss: 5.259012439803237\n",
      "epoch: 0 sentence: 13740/57013 loss: 4.830800735971683\n",
      "epoch: 0 sentence: 13750/57013 loss: 5.672520476728555\n",
      "epoch: 0 sentence: 13760/57013 loss: 5.192773295493296\n",
      "epoch: 0 sentence: 13770/57013 loss: 4.781876375827114\n",
      "epoch: 0 sentence: 13780/57013 loss: 5.739578220911053\n",
      "epoch: 0 sentence: 13790/57013 loss: 6.727145910707973\n",
      "epoch: 0 sentence: 13800/57013 loss: 5.617791949401234\n",
      "epoch: 0 sentence: 13810/57013 loss: 3.2736790329476855\n",
      "epoch: 0 sentence: 13820/57013 loss: 5.6122560770457275\n",
      "epoch: 0 sentence: 13830/57013 loss: 5.109415632788414\n",
      "epoch: 0 sentence: 13840/57013 loss: 5.166280470568597\n",
      "epoch: 0 sentence: 13850/57013 loss: 4.575160725085917\n",
      "epoch: 0 sentence: 13860/57013 loss: 4.963769920204875\n",
      "epoch: 0 sentence: 13870/57013 loss: 2.733939622953855\n",
      "epoch: 0 sentence: 13880/57013 loss: 3.3976671272896146\n",
      "epoch: 0 sentence: 13890/57013 loss: 3.4663690652595593\n",
      "epoch: 0 sentence: 13900/57013 loss: 4.636464984690954\n",
      "epoch: 0 sentence: 13910/57013 loss: 4.245443502905299\n",
      "epoch: 0 sentence: 13920/57013 loss: 5.437575634391803\n",
      "epoch: 0 sentence: 13930/57013 loss: 4.344917935152743\n",
      "epoch: 0 sentence: 13940/57013 loss: 5.813186236107729\n",
      "epoch: 0 sentence: 13950/57013 loss: 4.372499812620036\n",
      "epoch: 0 sentence: 13960/57013 loss: 4.270279734517252\n",
      "epoch: 0 sentence: 13970/57013 loss: 5.338763984094147\n",
      "epoch: 0 sentence: 13980/57013 loss: 4.320752469988497\n",
      "epoch: 0 sentence: 13990/57013 loss: 5.180408659535917\n",
      "epoch: 0 sentence: 14000/57013 loss: 5.921275346934641\n",
      "epoch: 0 sentence: 14010/57013 loss: 4.9823551864823\n",
      "epoch: 0 sentence: 14020/57013 loss: 3.9009809212791597\n",
      "epoch: 0 sentence: 14030/57013 loss: 4.042647024541632\n",
      "epoch: 0 sentence: 14040/57013 loss: 5.0222718464219716\n",
      "epoch: 0 sentence: 14050/57013 loss: 5.155285129512645\n",
      "epoch: 0 sentence: 14060/57013 loss: 5.361131990771731\n",
      "epoch: 0 sentence: 14070/57013 loss: 5.568352904090215\n",
      "epoch: 0 sentence: 14080/57013 loss: 5.129115283364423\n",
      "epoch: 0 sentence: 14090/57013 loss: 5.475506675492881\n",
      "epoch: 0 sentence: 14100/57013 loss: 4.417084341230398\n",
      "epoch: 0 sentence: 14110/57013 loss: 4.76736239628606\n",
      "epoch: 0 sentence: 14120/57013 loss: 6.240856731061617\n",
      "epoch: 0 sentence: 14130/57013 loss: 4.827762605086416\n",
      "epoch: 0 sentence: 14140/57013 loss: 3.96006043980776\n",
      "epoch: 0 sentence: 14150/57013 loss: 2.8958265477823715\n",
      "epoch: 0 sentence: 14160/57013 loss: 4.514343276918094\n",
      "epoch: 0 sentence: 14170/57013 loss: 4.717861713092824\n",
      "epoch: 0 sentence: 14180/57013 loss: 4.919795327412559\n",
      "epoch: 0 sentence: 14190/57013 loss: 4.7701138325650065\n",
      "epoch: 0 sentence: 14200/57013 loss: 3.3807568780512254\n",
      "epoch: 0 sentence: 14210/57013 loss: 5.046746578694974\n",
      "epoch: 0 sentence: 14220/57013 loss: 6.774255407152773\n",
      "epoch: 0 sentence: 14230/57013 loss: 4.052632982276381\n",
      "epoch: 0 sentence: 14240/57013 loss: 4.3910169676670545\n",
      "epoch: 0 sentence: 14250/57013 loss: 5.266707485064977\n",
      "epoch: 0 sentence: 14260/57013 loss: 5.104449098955962\n",
      "epoch: 0 sentence: 14270/57013 loss: 5.659227795700638\n",
      "epoch: 0 sentence: 14280/57013 loss: 1.5414918433987512\n",
      "epoch: 0 sentence: 14290/57013 loss: 3.8447506811759204\n",
      "epoch: 0 sentence: 14300/57013 loss: 5.045062115687373\n",
      "epoch: 0 sentence: 14310/57013 loss: 5.794706474875513\n",
      "epoch: 0 sentence: 14320/57013 loss: 5.674795738987376\n",
      "epoch: 0 sentence: 14330/57013 loss: 4.681958036046489\n",
      "epoch: 0 sentence: 14340/57013 loss: 3.59366894522963\n",
      "epoch: 0 sentence: 14350/57013 loss: 4.885863275176905\n",
      "epoch: 0 sentence: 14360/57013 loss: 4.185329082240596\n",
      "epoch: 0 sentence: 14370/57013 loss: 4.306070457867859\n",
      "epoch: 0 sentence: 14380/57013 loss: 4.608965427929335\n",
      "epoch: 0 sentence: 14390/57013 loss: 6.4458006372332575\n",
      "epoch: 0 sentence: 14400/57013 loss: 6.324333832951008\n",
      "epoch: 0 sentence: 14410/57013 loss: 5.410513138386512\n",
      "epoch: 0 sentence: 14420/57013 loss: 4.298515110731094\n",
      "epoch: 0 sentence: 14430/57013 loss: 3.6204463252634658\n",
      "epoch: 0 sentence: 14440/57013 loss: 4.039015069563614\n",
      "epoch: 0 sentence: 14450/57013 loss: 5.610540080807041\n",
      "epoch: 0 sentence: 14460/57013 loss: 3.1383311738879907\n",
      "epoch: 0 sentence: 14470/57013 loss: 5.652774898938338\n",
      "epoch: 0 sentence: 14480/57013 loss: 5.157774233366084\n",
      "epoch: 0 sentence: 14490/57013 loss: 5.815742594968314\n",
      "epoch: 0 sentence: 14500/57013 loss: 5.457545189672673\n",
      "epoch: 0 sentence: 14510/57013 loss: 5.689132890990773\n",
      "epoch: 0 sentence: 14520/57013 loss: 3.8309073454125993\n",
      "epoch: 0 sentence: 14530/57013 loss: 3.2568384438745595\n",
      "epoch: 0 sentence: 14540/57013 loss: 5.447212054854584\n",
      "epoch: 0 sentence: 14550/57013 loss: 5.861802411605421\n",
      "epoch: 0 sentence: 14560/57013 loss: 4.996080563982618\n",
      "epoch: 0 sentence: 14570/57013 loss: 4.123417888406507\n",
      "epoch: 0 sentence: 14580/57013 loss: 3.118157261796829\n",
      "epoch: 0 sentence: 14590/57013 loss: 4.591052280098005\n",
      "epoch: 0 sentence: 14600/57013 loss: 4.730316148396141\n",
      "epoch: 0 sentence: 14610/57013 loss: 4.419483268236322\n",
      "epoch: 0 sentence: 14620/57013 loss: 2.0846879231518693\n",
      "epoch: 0 sentence: 14630/57013 loss: 4.271251663267807\n",
      "epoch: 0 sentence: 14640/57013 loss: 3.7695233814563154\n",
      "epoch: 0 sentence: 14650/57013 loss: 5.4215079748404635\n",
      "epoch: 0 sentence: 14660/57013 loss: 4.570594252310257\n",
      "epoch: 0 sentence: 14670/57013 loss: 4.7176024063152155\n",
      "epoch: 0 sentence: 14680/57013 loss: 5.368285133208518\n",
      "epoch: 0 sentence: 14690/57013 loss: 4.8509112925955105\n",
      "epoch: 0 sentence: 14700/57013 loss: 4.274193090911523\n",
      "epoch: 0 sentence: 14710/57013 loss: 4.355253020430323\n",
      "epoch: 0 sentence: 14720/57013 loss: 2.1873367555109295\n",
      "epoch: 0 sentence: 14730/57013 loss: 5.17463541848814\n",
      "epoch: 0 sentence: 14740/57013 loss: 5.705686512905975\n",
      "epoch: 0 sentence: 14750/57013 loss: 4.571926246191351\n",
      "epoch: 0 sentence: 14760/57013 loss: 4.406617973004513\n",
      "epoch: 0 sentence: 14770/57013 loss: 5.107172799067951\n",
      "epoch: 0 sentence: 14780/57013 loss: 3.844548497872837\n",
      "epoch: 0 sentence: 14790/57013 loss: 3.9412003428028037\n",
      "epoch: 0 sentence: 14800/57013 loss: 5.435904957118767\n",
      "epoch: 0 sentence: 14810/57013 loss: 4.933923989005352\n",
      "epoch: 0 sentence: 14820/57013 loss: 5.77572743961966\n",
      "epoch: 0 sentence: 14830/57013 loss: 5.435693929877652\n",
      "epoch: 0 sentence: 14840/57013 loss: 5.712824466301134\n",
      "epoch: 0 sentence: 14850/57013 loss: 5.033161296613516\n",
      "epoch: 0 sentence: 14860/57013 loss: 5.144061414338862\n",
      "epoch: 0 sentence: 14870/57013 loss: 4.01620603527215\n",
      "epoch: 0 sentence: 14880/57013 loss: 4.43607513580746\n",
      "epoch: 0 sentence: 14890/57013 loss: 4.561084651205314\n",
      "epoch: 0 sentence: 14900/57013 loss: 5.863761459370163\n",
      "epoch: 0 sentence: 14910/57013 loss: 2.41013412978938\n",
      "epoch: 0 sentence: 14920/57013 loss: 4.657699194598365\n",
      "epoch: 0 sentence: 14930/57013 loss: 3.633619407101603\n",
      "epoch: 0 sentence: 14940/57013 loss: 6.448492767050068\n",
      "epoch: 0 sentence: 14950/57013 loss: 5.369606371718119\n",
      "epoch: 0 sentence: 14960/57013 loss: 5.323323643397771\n",
      "epoch: 0 sentence: 14970/57013 loss: 4.782947919882757\n",
      "epoch: 0 sentence: 14980/57013 loss: 5.85820094433725\n",
      "epoch: 0 sentence: 14990/57013 loss: 5.760447048102787\n",
      "epoch: 0 sentence: 15000/57013 loss: 4.899534144482474\n",
      "epoch: 0 sentence: 15010/57013 loss: 4.459245554505022\n",
      "epoch: 0 sentence: 15020/57013 loss: 3.7905556546463535\n",
      "epoch: 0 sentence: 15030/57013 loss: 3.1510377822254676\n",
      "epoch: 0 sentence: 15040/57013 loss: 6.519833479214182\n",
      "epoch: 0 sentence: 15050/57013 loss: 5.082114741545301\n",
      "epoch: 0 sentence: 15060/57013 loss: 6.185537005369719\n",
      "epoch: 0 sentence: 15070/57013 loss: 5.825841653712465\n",
      "epoch: 0 sentence: 15080/57013 loss: 4.361328003982491\n",
      "epoch: 0 sentence: 15090/57013 loss: 6.620799967317187\n",
      "epoch: 0 sentence: 15100/57013 loss: 4.1437959588657325\n",
      "epoch: 0 sentence: 15110/57013 loss: 3.623884796205117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 15120/57013 loss: 5.113870629933678\n",
      "epoch: 0 sentence: 15130/57013 loss: 5.704785406695308\n",
      "epoch: 0 sentence: 15140/57013 loss: 4.978632857033526\n",
      "epoch: 0 sentence: 15150/57013 loss: 4.6776077940584155\n",
      "epoch: 0 sentence: 15160/57013 loss: 4.423686576625283\n",
      "epoch: 0 sentence: 15170/57013 loss: 5.5424150368505645\n",
      "epoch: 0 sentence: 15180/57013 loss: 3.413296419624736\n",
      "epoch: 0 sentence: 15190/57013 loss: 4.875062631770496\n",
      "epoch: 0 sentence: 15200/57013 loss: 3.8137641183995683\n",
      "epoch: 0 sentence: 15210/57013 loss: 4.315591548911904\n",
      "epoch: 0 sentence: 15220/57013 loss: 4.213932699454727\n",
      "epoch: 0 sentence: 15230/57013 loss: 3.788161851705894\n",
      "epoch: 0 sentence: 15240/57013 loss: 4.298883712972691\n",
      "epoch: 0 sentence: 15250/57013 loss: 2.2077020799970803\n",
      "epoch: 0 sentence: 15260/57013 loss: 4.986764484024638\n",
      "epoch: 0 sentence: 15270/57013 loss: 4.198560271700102\n",
      "epoch: 0 sentence: 15280/57013 loss: 5.401709724557573\n",
      "epoch: 0 sentence: 15290/57013 loss: 6.134236213631\n",
      "epoch: 0 sentence: 15300/57013 loss: 4.498015609126714\n",
      "epoch: 0 sentence: 15310/57013 loss: 5.800198574619182\n",
      "epoch: 0 sentence: 15320/57013 loss: 4.649442864202084\n",
      "epoch: 0 sentence: 15330/57013 loss: 5.284518475499677\n",
      "epoch: 0 sentence: 15340/57013 loss: 4.950919243997417\n",
      "epoch: 0 sentence: 15350/57013 loss: 2.193279559308402\n",
      "epoch: 0 sentence: 15360/57013 loss: 4.851197449311546\n",
      "epoch: 0 sentence: 15370/57013 loss: 4.610494924227268\n",
      "epoch: 0 sentence: 15380/57013 loss: 4.058397573521664\n",
      "epoch: 0 sentence: 15390/57013 loss: 5.028911511737694\n",
      "epoch: 0 sentence: 15400/57013 loss: 4.161547003445934\n",
      "epoch: 0 sentence: 15410/57013 loss: 4.872861558323835\n",
      "epoch: 0 sentence: 15420/57013 loss: 4.858768157182935\n",
      "epoch: 0 sentence: 15430/57013 loss: 4.52085793748169\n",
      "epoch: 0 sentence: 15440/57013 loss: 3.765830638637503\n",
      "epoch: 0 sentence: 15450/57013 loss: 5.0795878962482375\n",
      "epoch: 0 sentence: 15460/57013 loss: 4.608296430524662\n",
      "epoch: 0 sentence: 15470/57013 loss: 5.275810757954766\n",
      "epoch: 0 sentence: 15480/57013 loss: 2.2644779828719312\n",
      "epoch: 0 sentence: 15490/57013 loss: 4.937278454836661\n",
      "epoch: 0 sentence: 15500/57013 loss: 3.7042997563194904\n",
      "epoch: 0 sentence: 15510/57013 loss: 4.471101594824014\n",
      "epoch: 0 sentence: 15520/57013 loss: 4.874214331948075\n",
      "epoch: 0 sentence: 15530/57013 loss: 4.616410193755531\n",
      "epoch: 0 sentence: 15540/57013 loss: 4.331303260767194\n",
      "epoch: 0 sentence: 15550/57013 loss: 3.9691554828661477\n",
      "epoch: 0 sentence: 15560/57013 loss: 4.452228960354927\n",
      "epoch: 0 sentence: 15570/57013 loss: 5.585945356749814\n",
      "epoch: 0 sentence: 15580/57013 loss: 6.640083649903615\n",
      "epoch: 0 sentence: 15590/57013 loss: 3.3486110179870603\n",
      "epoch: 0 sentence: 15600/57013 loss: 5.763811295823619\n",
      "epoch: 0 sentence: 15610/57013 loss: 4.922590130464233\n",
      "epoch: 0 sentence: 15620/57013 loss: 4.925336248517466\n",
      "epoch: 0 sentence: 15630/57013 loss: 4.333439349504113\n",
      "epoch: 0 sentence: 15640/57013 loss: 5.534400105016655\n",
      "epoch: 0 sentence: 15650/57013 loss: 5.8023696540551155\n",
      "epoch: 0 sentence: 15660/57013 loss: 5.729298507255764\n",
      "epoch: 0 sentence: 15670/57013 loss: 5.67329384992322\n",
      "epoch: 0 sentence: 15680/57013 loss: 4.746126174675617\n",
      "epoch: 0 sentence: 15690/57013 loss: 5.44819022824786\n",
      "epoch: 0 sentence: 15700/57013 loss: 4.207940400798882\n",
      "epoch: 0 sentence: 15710/57013 loss: 4.55510395243024\n",
      "epoch: 0 sentence: 15720/57013 loss: 5.8487794756501135\n",
      "epoch: 0 sentence: 15730/57013 loss: 2.5124904527034784\n",
      "epoch: 0 sentence: 15740/57013 loss: 4.745543720605552\n",
      "epoch: 0 sentence: 15750/57013 loss: 4.770937815191192\n",
      "epoch: 0 sentence: 15760/57013 loss: 5.797440860606699\n",
      "epoch: 0 sentence: 15770/57013 loss: 2.223418426715094\n",
      "epoch: 0 sentence: 15780/57013 loss: 3.431727625222041\n",
      "epoch: 0 sentence: 15790/57013 loss: 3.5797428271041887\n",
      "epoch: 0 sentence: 15800/57013 loss: 4.351992618787668\n",
      "epoch: 0 sentence: 15810/57013 loss: 5.628415051422054\n",
      "epoch: 0 sentence: 15820/57013 loss: 4.760109860959208\n",
      "epoch: 0 sentence: 15830/57013 loss: 5.429830941629749\n",
      "epoch: 0 sentence: 15840/57013 loss: 5.695775776366049\n",
      "epoch: 0 sentence: 15850/57013 loss: 3.729331252502909\n",
      "epoch: 0 sentence: 15860/57013 loss: 3.4316501252047065\n",
      "epoch: 0 sentence: 15870/57013 loss: 5.34769931730566\n",
      "epoch: 0 sentence: 15880/57013 loss: 5.352682298020913\n",
      "epoch: 0 sentence: 15890/57013 loss: 4.143079471395272\n",
      "epoch: 0 sentence: 15900/57013 loss: 2.923407749233479\n",
      "epoch: 0 sentence: 15910/57013 loss: 4.058511954983504\n",
      "epoch: 0 sentence: 15920/57013 loss: 3.7051690625003073\n",
      "epoch: 0 sentence: 15930/57013 loss: 5.385212203449988\n",
      "epoch: 0 sentence: 15940/57013 loss: 5.74566810894227\n",
      "epoch: 0 sentence: 15950/57013 loss: 5.038349067377878\n",
      "epoch: 0 sentence: 15960/57013 loss: 5.725065718556719\n",
      "epoch: 0 sentence: 15970/57013 loss: 4.54204700027779\n",
      "epoch: 0 sentence: 15980/57013 loss: 4.542649247848843\n",
      "epoch: 0 sentence: 15990/57013 loss: 4.847945569214569\n",
      "epoch: 0 sentence: 16000/57013 loss: 5.315751504386923\n",
      "epoch: 0 sentence: 16010/57013 loss: 4.5969310595048585\n",
      "epoch: 0 sentence: 16020/57013 loss: 3.994735100140797\n",
      "epoch: 0 sentence: 16030/57013 loss: 5.0044669698734525\n",
      "epoch: 0 sentence: 16040/57013 loss: 5.5634773621882605\n",
      "epoch: 0 sentence: 16050/57013 loss: 3.812484568310087\n",
      "epoch: 0 sentence: 16060/57013 loss: 4.726498507792986\n",
      "epoch: 0 sentence: 16070/57013 loss: 3.676571799178296\n",
      "epoch: 0 sentence: 16080/57013 loss: 5.664098018365987\n",
      "epoch: 0 sentence: 16090/57013 loss: 5.374969736819881\n",
      "epoch: 0 sentence: 16100/57013 loss: 4.803842067045184\n",
      "epoch: 0 sentence: 16110/57013 loss: 5.519386380123835\n",
      "epoch: 0 sentence: 16120/57013 loss: 5.637717630543022\n",
      "epoch: 0 sentence: 16130/57013 loss: 6.504980030897161\n",
      "epoch: 0 sentence: 16140/57013 loss: 6.11577710765029\n",
      "epoch: 0 sentence: 16150/57013 loss: 4.44491474720685\n",
      "epoch: 0 sentence: 16160/57013 loss: 4.757808153551638\n",
      "epoch: 0 sentence: 16170/57013 loss: 5.186948331471697\n",
      "epoch: 0 sentence: 16180/57013 loss: 4.790135823178659\n",
      "epoch: 0 sentence: 16190/57013 loss: 3.478540928939543\n",
      "epoch: 0 sentence: 16200/57013 loss: 5.189601348063317\n",
      "epoch: 0 sentence: 16210/57013 loss: 6.152299065275941\n",
      "epoch: 0 sentence: 16220/57013 loss: 5.5495335535356265\n",
      "epoch: 0 sentence: 16230/57013 loss: 5.843307265178627\n",
      "epoch: 0 sentence: 16240/57013 loss: 4.6618703868010245\n",
      "epoch: 0 sentence: 16250/57013 loss: 6.016631430625718\n",
      "epoch: 0 sentence: 16260/57013 loss: 2.583439618672092\n",
      "epoch: 0 sentence: 16270/57013 loss: 3.8827172942696446\n",
      "epoch: 0 sentence: 16280/57013 loss: 5.132361477593551\n",
      "epoch: 0 sentence: 16290/57013 loss: 4.400419621714202\n",
      "epoch: 0 sentence: 16300/57013 loss: 5.128377704028916\n",
      "epoch: 0 sentence: 16310/57013 loss: 4.861389133507567\n",
      "epoch: 0 sentence: 16320/57013 loss: 4.365772534937618\n",
      "epoch: 0 sentence: 16330/57013 loss: 4.755536055462952\n",
      "epoch: 0 sentence: 16340/57013 loss: 4.054345166991\n",
      "epoch: 0 sentence: 16350/57013 loss: 6.006607124325984\n",
      "epoch: 0 sentence: 16360/57013 loss: 5.190822061436285\n",
      "epoch: 0 sentence: 16370/57013 loss: 3.7418413012157834\n",
      "epoch: 0 sentence: 16380/57013 loss: 4.0059656771978345\n",
      "epoch: 0 sentence: 16390/57013 loss: 5.638486298884472\n",
      "epoch: 0 sentence: 16400/57013 loss: 4.877228664203405\n",
      "epoch: 0 sentence: 16410/57013 loss: 4.59713857890551\n",
      "epoch: 0 sentence: 16420/57013 loss: 4.13683870981825\n",
      "epoch: 0 sentence: 16430/57013 loss: 4.630332004784114\n",
      "epoch: 0 sentence: 16440/57013 loss: 4.481255750972723\n",
      "epoch: 0 sentence: 16450/57013 loss: 5.519522126557909\n",
      "epoch: 0 sentence: 16460/57013 loss: 4.805596092552915\n",
      "epoch: 0 sentence: 16470/57013 loss: 4.684448885344948\n",
      "epoch: 0 sentence: 16480/57013 loss: 5.761266548125373\n",
      "epoch: 0 sentence: 16490/57013 loss: 5.610608152918784\n",
      "epoch: 0 sentence: 16500/57013 loss: 4.972361025634095\n",
      "epoch: 0 sentence: 16510/57013 loss: 5.310980888170454\n",
      "epoch: 0 sentence: 16520/57013 loss: 3.9428559501058973\n",
      "epoch: 0 sentence: 16530/57013 loss: 3.163598937679968\n",
      "epoch: 0 sentence: 16540/57013 loss: 6.587467021366891\n",
      "epoch: 0 sentence: 16550/57013 loss: 4.329017219348533\n",
      "epoch: 0 sentence: 16560/57013 loss: 4.236964855960347\n",
      "epoch: 0 sentence: 16570/57013 loss: 4.817692026321826\n",
      "epoch: 0 sentence: 16580/57013 loss: 4.809633841676808\n",
      "epoch: 0 sentence: 16590/57013 loss: 4.64744427330489\n",
      "epoch: 0 sentence: 16600/57013 loss: 5.810484300419225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 16610/57013 loss: 4.874701189417329\n",
      "epoch: 0 sentence: 16620/57013 loss: 5.206786107383088\n",
      "epoch: 0 sentence: 16630/57013 loss: 5.754297362732233\n",
      "epoch: 0 sentence: 16640/57013 loss: 3.927469571689838\n",
      "epoch: 0 sentence: 16650/57013 loss: 4.171821600568081\n",
      "epoch: 0 sentence: 16660/57013 loss: 5.2950966510355775\n",
      "epoch: 0 sentence: 16670/57013 loss: 5.772780713801406\n",
      "epoch: 0 sentence: 16680/57013 loss: 5.002117026148169\n",
      "epoch: 0 sentence: 16690/57013 loss: 5.381504954335776\n",
      "epoch: 0 sentence: 16700/57013 loss: 4.192628611216881\n",
      "epoch: 0 sentence: 16710/57013 loss: 5.463522817813321\n",
      "epoch: 0 sentence: 16720/57013 loss: 6.435493740466045\n",
      "epoch: 0 sentence: 16730/57013 loss: 4.6497167672759625\n",
      "epoch: 0 sentence: 16740/57013 loss: 5.691896680865474\n",
      "epoch: 0 sentence: 16750/57013 loss: 5.131740723530955\n",
      "epoch: 0 sentence: 16760/57013 loss: 4.97267816532791\n",
      "epoch: 0 sentence: 16770/57013 loss: 4.36558217465575\n",
      "epoch: 0 sentence: 16780/57013 loss: 4.263363576469511\n",
      "epoch: 0 sentence: 16790/57013 loss: 5.017731987471255\n",
      "epoch: 0 sentence: 16800/57013 loss: 5.7134807272817625\n",
      "epoch: 0 sentence: 16810/57013 loss: 5.185895744212485\n",
      "epoch: 0 sentence: 16820/57013 loss: 3.78344856567637\n",
      "epoch: 0 sentence: 16830/57013 loss: 3.424476897476403\n",
      "epoch: 0 sentence: 16840/57013 loss: 6.016985826942179\n",
      "epoch: 0 sentence: 16850/57013 loss: 3.413659965643765\n",
      "epoch: 0 sentence: 16860/57013 loss: 4.53815492391904\n",
      "epoch: 0 sentence: 16870/57013 loss: 5.059140089281652\n",
      "epoch: 0 sentence: 16880/57013 loss: 5.307110903451622\n",
      "epoch: 0 sentence: 16890/57013 loss: 4.2981107560059035\n",
      "epoch: 0 sentence: 16900/57013 loss: 5.545901981323102\n",
      "epoch: 0 sentence: 16910/57013 loss: 4.6948820219152285\n",
      "epoch: 0 sentence: 16920/57013 loss: 5.340947365940017\n",
      "epoch: 0 sentence: 16930/57013 loss: 4.025175594060778\n",
      "epoch: 0 sentence: 16940/57013 loss: 4.565932007555064\n",
      "epoch: 0 sentence: 16950/57013 loss: 4.341283955012475\n",
      "epoch: 0 sentence: 16960/57013 loss: 3.3921575841046896\n",
      "epoch: 0 sentence: 16970/57013 loss: 4.8741530293987525\n",
      "epoch: 0 sentence: 16980/57013 loss: 4.9958633332019105\n",
      "epoch: 0 sentence: 16990/57013 loss: 5.335544314159947\n",
      "epoch: 0 sentence: 17000/57013 loss: 6.488702154406449\n",
      "epoch: 0 sentence: 17010/57013 loss: 5.0690189330126305\n",
      "epoch: 0 sentence: 17020/57013 loss: 5.515842390946641\n",
      "epoch: 0 sentence: 17030/57013 loss: 5.514276676140957\n",
      "epoch: 0 sentence: 17040/57013 loss: 5.2622869405721655\n",
      "epoch: 0 sentence: 17050/57013 loss: 5.570235652501474\n",
      "epoch: 0 sentence: 17060/57013 loss: 5.346001404384376\n",
      "epoch: 0 sentence: 17070/57013 loss: 2.333232960580793\n",
      "epoch: 0 sentence: 17080/57013 loss: 4.329131899889301\n",
      "epoch: 0 sentence: 17090/57013 loss: 5.298802556252781\n",
      "epoch: 0 sentence: 17100/57013 loss: 4.887184099044765\n",
      "epoch: 0 sentence: 17110/57013 loss: 4.748350659771149\n",
      "epoch: 0 sentence: 17120/57013 loss: 5.881510417402395\n",
      "epoch: 0 sentence: 17130/57013 loss: 5.950376393731792\n",
      "epoch: 0 sentence: 17140/57013 loss: 3.1371693737740713\n",
      "epoch: 0 sentence: 17150/57013 loss: 5.040179611394239\n",
      "epoch: 0 sentence: 17160/57013 loss: 5.091457111658271\n",
      "epoch: 0 sentence: 17170/57013 loss: 4.257027777274522\n",
      "epoch: 0 sentence: 17180/57013 loss: 5.430515098522214\n",
      "epoch: 0 sentence: 17190/57013 loss: 5.210931000965442\n",
      "epoch: 0 sentence: 17200/57013 loss: 4.920036874582289\n",
      "epoch: 0 sentence: 17210/57013 loss: 5.781344914687278\n",
      "epoch: 0 sentence: 17220/57013 loss: 4.852210447022501\n",
      "epoch: 0 sentence: 17230/57013 loss: 6.2054608678692205\n",
      "epoch: 0 sentence: 17240/57013 loss: 4.943872539123703\n",
      "epoch: 0 sentence: 17250/57013 loss: 4.052168139325122\n",
      "epoch: 0 sentence: 17260/57013 loss: 5.147350595778877\n",
      "epoch: 0 sentence: 17270/57013 loss: 4.7384939656179395\n",
      "epoch: 0 sentence: 17280/57013 loss: 4.471590019643995\n",
      "epoch: 0 sentence: 17290/57013 loss: 5.958731502255297\n",
      "epoch: 0 sentence: 17300/57013 loss: 5.570535096198547\n",
      "epoch: 0 sentence: 17310/57013 loss: 4.086177267784584\n",
      "epoch: 0 sentence: 17320/57013 loss: 5.82800902621349\n",
      "epoch: 0 sentence: 17330/57013 loss: 5.1603532112227155\n",
      "epoch: 0 sentence: 17340/57013 loss: 5.066099944467588\n",
      "epoch: 0 sentence: 17350/57013 loss: 4.286641249115314\n",
      "epoch: 0 sentence: 17360/57013 loss: 4.436870491584302\n",
      "epoch: 0 sentence: 17370/57013 loss: 5.514934571289225\n",
      "epoch: 0 sentence: 17380/57013 loss: 5.867422624526006\n",
      "epoch: 0 sentence: 17390/57013 loss: 4.391054749534773\n",
      "epoch: 0 sentence: 17400/57013 loss: 3.798614202596504\n",
      "epoch: 0 sentence: 17410/57013 loss: 4.273385265237467\n",
      "epoch: 0 sentence: 17420/57013 loss: 3.48154600995629\n",
      "epoch: 0 sentence: 17430/57013 loss: 4.464188580302443\n",
      "epoch: 0 sentence: 17440/57013 loss: 4.923593304037185\n",
      "epoch: 0 sentence: 17450/57013 loss: 3.8814952248662453\n",
      "epoch: 0 sentence: 17460/57013 loss: 5.80024621333523\n",
      "epoch: 0 sentence: 17470/57013 loss: 4.6868596362238275\n",
      "epoch: 0 sentence: 17480/57013 loss: 5.12306158895653\n",
      "epoch: 0 sentence: 17490/57013 loss: 5.618323627549632\n",
      "epoch: 0 sentence: 17500/57013 loss: 5.406279267794076\n",
      "epoch: 0 sentence: 17510/57013 loss: 5.289268919660886\n",
      "epoch: 0 sentence: 17520/57013 loss: 3.612042013000144\n",
      "epoch: 0 sentence: 17530/57013 loss: 5.360861944536881\n",
      "epoch: 0 sentence: 17540/57013 loss: 4.10936816593431\n",
      "epoch: 0 sentence: 17550/57013 loss: 5.095296232618119\n",
      "epoch: 0 sentence: 17560/57013 loss: 5.370213290538402\n",
      "epoch: 0 sentence: 17570/57013 loss: 3.9898422338393535\n",
      "epoch: 0 sentence: 17580/57013 loss: 4.030488010373933\n",
      "epoch: 0 sentence: 17590/57013 loss: 3.3438063681488543\n",
      "epoch: 0 sentence: 17600/57013 loss: 5.2159603429704795\n",
      "epoch: 0 sentence: 17610/57013 loss: 4.999873369748124\n",
      "epoch: 0 sentence: 17620/57013 loss: 4.75807266649756\n",
      "epoch: 0 sentence: 17630/57013 loss: 3.0880056198329013\n",
      "epoch: 0 sentence: 17640/57013 loss: 4.432974426050915\n",
      "epoch: 0 sentence: 17650/57013 loss: 4.6382789152879385\n",
      "epoch: 0 sentence: 17660/57013 loss: 4.191078566098431\n",
      "epoch: 0 sentence: 17670/57013 loss: 4.453392126555834\n",
      "epoch: 0 sentence: 17680/57013 loss: 4.37855819160061\n",
      "epoch: 0 sentence: 17690/57013 loss: 4.867274604307046\n",
      "epoch: 0 sentence: 17700/57013 loss: 5.182387449824362\n",
      "epoch: 0 sentence: 17710/57013 loss: 5.485004363344335\n",
      "epoch: 0 sentence: 17720/57013 loss: 4.35262715783042\n",
      "epoch: 0 sentence: 17730/57013 loss: 4.845626680001063\n",
      "epoch: 0 sentence: 17740/57013 loss: 1.7211573422782802\n",
      "epoch: 0 sentence: 17750/57013 loss: 5.694621659361287\n",
      "epoch: 0 sentence: 17760/57013 loss: 4.886270250063598\n",
      "epoch: 0 sentence: 17770/57013 loss: 4.460252173302522\n",
      "epoch: 0 sentence: 17780/57013 loss: 4.810370622703365\n",
      "epoch: 0 sentence: 17790/57013 loss: 4.149739691171759\n",
      "epoch: 0 sentence: 17800/57013 loss: 3.0648931387711853\n",
      "epoch: 0 sentence: 17810/57013 loss: 5.235806886869412\n",
      "epoch: 0 sentence: 17820/57013 loss: 5.086628319719721\n",
      "epoch: 0 sentence: 17830/57013 loss: 4.06545298965749\n",
      "epoch: 0 sentence: 17840/57013 loss: 5.03061554703918\n",
      "epoch: 0 sentence: 17850/57013 loss: 4.414973644608375\n",
      "epoch: 0 sentence: 17860/57013 loss: 3.4288808030538838\n",
      "epoch: 0 sentence: 17870/57013 loss: 3.901684719007635\n",
      "epoch: 0 sentence: 17880/57013 loss: 5.3265546410725175\n",
      "epoch: 0 sentence: 17890/57013 loss: 4.208405572249682\n",
      "epoch: 0 sentence: 17900/57013 loss: 5.344250557104417\n",
      "epoch: 0 sentence: 17910/57013 loss: 4.891589753634886\n",
      "epoch: 0 sentence: 17920/57013 loss: 5.176176501331812\n",
      "epoch: 0 sentence: 17930/57013 loss: 4.697031765807793\n",
      "epoch: 0 sentence: 17940/57013 loss: 4.740377392054186\n",
      "epoch: 0 sentence: 17950/57013 loss: 3.387123373887461\n",
      "epoch: 0 sentence: 17960/57013 loss: 5.807605363525553\n",
      "epoch: 0 sentence: 17970/57013 loss: 5.6798846693147915\n",
      "epoch: 0 sentence: 17980/57013 loss: 4.227176552053243\n",
      "epoch: 0 sentence: 17990/57013 loss: 6.273573757345495\n",
      "epoch: 0 sentence: 18000/57013 loss: 5.2025250547284525\n",
      "epoch: 0 sentence: 18010/57013 loss: 3.990924460492644\n",
      "epoch: 0 sentence: 18020/57013 loss: 5.6836643789106365\n",
      "epoch: 0 sentence: 18030/57013 loss: 3.68626231439724\n",
      "epoch: 0 sentence: 18040/57013 loss: 5.928347721996758\n",
      "epoch: 0 sentence: 18050/57013 loss: 3.6264010527690482\n",
      "epoch: 0 sentence: 18060/57013 loss: 6.224062890325449\n",
      "epoch: 0 sentence: 18070/57013 loss: 5.173854009043055\n",
      "epoch: 0 sentence: 18080/57013 loss: 4.9996863426758855\n",
      "epoch: 0 sentence: 18090/57013 loss: 4.669705551149296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 18100/57013 loss: 5.3883570841034\n",
      "epoch: 0 sentence: 18110/57013 loss: 4.884713976128933\n",
      "epoch: 0 sentence: 18120/57013 loss: 6.254205792340961\n",
      "epoch: 0 sentence: 18130/57013 loss: 4.0430446117968355\n",
      "epoch: 0 sentence: 18140/57013 loss: 4.579117845224651\n",
      "epoch: 0 sentence: 18150/57013 loss: 5.123758935696793\n",
      "epoch: 0 sentence: 18160/57013 loss: 4.823610882592927\n",
      "epoch: 0 sentence: 18170/57013 loss: 4.626418083648088\n",
      "epoch: 0 sentence: 18180/57013 loss: 4.304339128324974\n",
      "epoch: 0 sentence: 18190/57013 loss: 5.537507764562786\n",
      "epoch: 0 sentence: 18200/57013 loss: 4.3215864908345845\n",
      "epoch: 0 sentence: 18210/57013 loss: 4.438278534071041\n",
      "epoch: 0 sentence: 18220/57013 loss: 5.3426089904234715\n",
      "epoch: 0 sentence: 18230/57013 loss: 4.9290892506327975\n",
      "epoch: 0 sentence: 18240/57013 loss: 5.187715499829946\n",
      "epoch: 0 sentence: 18250/57013 loss: 3.9157003265986017\n",
      "epoch: 0 sentence: 18260/57013 loss: 3.8920836882706715\n",
      "epoch: 0 sentence: 18270/57013 loss: 5.821445552714037\n",
      "epoch: 0 sentence: 18280/57013 loss: 4.361751425747849\n",
      "epoch: 0 sentence: 18290/57013 loss: 5.217078419212477\n",
      "epoch: 0 sentence: 18300/57013 loss: 4.032955481435601\n",
      "epoch: 0 sentence: 18310/57013 loss: 6.038591572019017\n",
      "epoch: 0 sentence: 18320/57013 loss: 3.618161627387684\n",
      "epoch: 0 sentence: 18330/57013 loss: 3.8843229138604425\n",
      "epoch: 0 sentence: 18340/57013 loss: 4.58027854744724\n",
      "epoch: 0 sentence: 18350/57013 loss: 5.252359640329018\n",
      "epoch: 0 sentence: 18360/57013 loss: 4.85005631174115\n",
      "epoch: 0 sentence: 18370/57013 loss: 4.5138263774424034\n",
      "epoch: 0 sentence: 18380/57013 loss: 5.400741384493151\n",
      "epoch: 0 sentence: 18390/57013 loss: 6.333832218367243\n",
      "epoch: 0 sentence: 18400/57013 loss: 4.595812601307303\n",
      "epoch: 0 sentence: 18410/57013 loss: 5.374103432354652\n",
      "epoch: 0 sentence: 18420/57013 loss: 4.790090419886438\n",
      "epoch: 0 sentence: 18430/57013 loss: 6.3780011221454025\n",
      "epoch: 0 sentence: 18440/57013 loss: 5.260629144604591\n",
      "epoch: 0 sentence: 18450/57013 loss: 5.755677826246956\n",
      "epoch: 0 sentence: 18460/57013 loss: 4.169255681920713\n",
      "epoch: 0 sentence: 18470/57013 loss: 3.5424392554246618\n",
      "epoch: 0 sentence: 18480/57013 loss: 4.689044220104954\n",
      "epoch: 0 sentence: 18490/57013 loss: 4.022381305472975\n",
      "epoch: 0 sentence: 18500/57013 loss: 3.9050767687325227\n",
      "epoch: 0 sentence: 18510/57013 loss: 4.323025123902531\n",
      "epoch: 0 sentence: 18520/57013 loss: 5.764618937066748\n",
      "epoch: 0 sentence: 18530/57013 loss: 1.4405859054386576\n",
      "epoch: 0 sentence: 18540/57013 loss: 3.6243465360651905\n",
      "epoch: 0 sentence: 18550/57013 loss: 3.359460571578786\n",
      "epoch: 0 sentence: 18560/57013 loss: 4.718437379707079\n",
      "epoch: 0 sentence: 18570/57013 loss: 4.089627115369009\n",
      "epoch: 0 sentence: 18580/57013 loss: 5.249356304976361\n",
      "epoch: 0 sentence: 18590/57013 loss: 5.688114615945842\n",
      "epoch: 0 sentence: 18600/57013 loss: 6.018974814037707\n",
      "epoch: 0 sentence: 18610/57013 loss: 4.244433529181184\n",
      "epoch: 0 sentence: 18620/57013 loss: 6.146641123596087\n",
      "epoch: 0 sentence: 18630/57013 loss: 5.394056843677724\n",
      "epoch: 0 sentence: 18640/57013 loss: 5.144877315128914\n",
      "epoch: 0 sentence: 18650/57013 loss: 5.989683508844166\n",
      "epoch: 0 sentence: 18660/57013 loss: 4.761047734499006\n",
      "epoch: 0 sentence: 18670/57013 loss: 5.167075812765192\n",
      "epoch: 0 sentence: 18680/57013 loss: 5.522225772216919\n",
      "epoch: 0 sentence: 18690/57013 loss: 6.037434909726856\n",
      "epoch: 0 sentence: 18700/57013 loss: 3.8853992969879356\n",
      "epoch: 0 sentence: 18710/57013 loss: 5.010065608483386\n",
      "epoch: 0 sentence: 18720/57013 loss: 6.000550403744682\n",
      "epoch: 0 sentence: 18730/57013 loss: 5.446858481067246\n",
      "epoch: 0 sentence: 18740/57013 loss: 5.23416045958192\n",
      "epoch: 0 sentence: 18750/57013 loss: 5.700531288210751\n",
      "epoch: 0 sentence: 18760/57013 loss: 5.376308183206204\n",
      "epoch: 0 sentence: 18770/57013 loss: 4.662002745545016\n",
      "epoch: 0 sentence: 18780/57013 loss: 6.13871973042735\n",
      "epoch: 0 sentence: 18790/57013 loss: 4.959936133353115\n",
      "epoch: 0 sentence: 18800/57013 loss: 4.601408094682703\n",
      "epoch: 0 sentence: 18810/57013 loss: 5.65411546867098\n",
      "epoch: 0 sentence: 18820/57013 loss: 6.213374261509667\n",
      "epoch: 0 sentence: 18830/57013 loss: 5.753748985153539\n",
      "epoch: 0 sentence: 18840/57013 loss: 3.7806849190245777\n",
      "epoch: 0 sentence: 18850/57013 loss: 3.5469815395848823\n",
      "epoch: 0 sentence: 18860/57013 loss: 6.037849835051063\n",
      "epoch: 0 sentence: 18870/57013 loss: 4.268211545915466\n",
      "epoch: 0 sentence: 18880/57013 loss: 5.174613837743489\n",
      "epoch: 0 sentence: 18890/57013 loss: 3.0853676642944317\n",
      "epoch: 0 sentence: 18900/57013 loss: 5.37953477976708\n",
      "epoch: 0 sentence: 18910/57013 loss: 4.788699970682077\n",
      "epoch: 0 sentence: 18920/57013 loss: 5.01469371588507\n",
      "epoch: 0 sentence: 18930/57013 loss: 2.8753456847794028\n",
      "epoch: 0 sentence: 18940/57013 loss: 5.2403018370779915\n",
      "epoch: 0 sentence: 18950/57013 loss: 4.096730836812468\n",
      "epoch: 0 sentence: 18960/57013 loss: 6.27294394564394\n",
      "epoch: 0 sentence: 18970/57013 loss: 4.0957178867466295\n",
      "epoch: 0 sentence: 18980/57013 loss: 5.174423904674866\n",
      "epoch: 0 sentence: 18990/57013 loss: 5.164535050282077\n",
      "epoch: 0 sentence: 19000/57013 loss: 4.668009724724125\n",
      "epoch: 0 sentence: 19010/57013 loss: 4.919234477917766\n",
      "epoch: 0 sentence: 19020/57013 loss: 5.343392337113846\n",
      "epoch: 0 sentence: 19030/57013 loss: 5.832815175199008\n",
      "epoch: 0 sentence: 19040/57013 loss: 1.9257587480608183\n",
      "epoch: 0 sentence: 19050/57013 loss: 5.872839315045407\n",
      "epoch: 0 sentence: 19060/57013 loss: 2.021822844430027\n",
      "epoch: 0 sentence: 19070/57013 loss: 5.3152252107996905\n",
      "epoch: 0 sentence: 19080/57013 loss: 5.078302284843903\n",
      "epoch: 0 sentence: 19090/57013 loss: 4.301029333871946\n",
      "epoch: 0 sentence: 19100/57013 loss: 3.206339557562317\n",
      "epoch: 0 sentence: 19110/57013 loss: 5.557895437168816\n",
      "epoch: 0 sentence: 19120/57013 loss: 5.103887557727182\n",
      "epoch: 0 sentence: 19130/57013 loss: 4.872912228092524\n",
      "epoch: 0 sentence: 19140/57013 loss: 4.065591608207586\n",
      "epoch: 0 sentence: 19150/57013 loss: 4.847405557426622\n",
      "epoch: 0 sentence: 19160/57013 loss: 6.148489517935984\n",
      "epoch: 0 sentence: 19170/57013 loss: 4.940795181707413\n",
      "epoch: 0 sentence: 19180/57013 loss: 4.108280263601067\n",
      "epoch: 0 sentence: 19190/57013 loss: 5.164780035765952\n",
      "epoch: 0 sentence: 19200/57013 loss: 5.3077755270797375\n",
      "epoch: 0 sentence: 19210/57013 loss: 4.077899178341123\n",
      "epoch: 0 sentence: 19220/57013 loss: 5.266735920665265\n",
      "epoch: 0 sentence: 19230/57013 loss: 4.93190807103359\n",
      "epoch: 0 sentence: 19240/57013 loss: 3.0341276997162785\n",
      "epoch: 0 sentence: 19250/57013 loss: 5.3010642649739435\n",
      "epoch: 0 sentence: 19260/57013 loss: 7.907498562521141\n",
      "epoch: 0 sentence: 19270/57013 loss: 3.3792651651107044\n",
      "epoch: 0 sentence: 19280/57013 loss: 4.933697841643514\n",
      "epoch: 0 sentence: 19290/57013 loss: 5.266376628713316\n",
      "epoch: 0 sentence: 19300/57013 loss: 3.7855791199390003\n",
      "epoch: 0 sentence: 19310/57013 loss: 4.087157516046491\n",
      "epoch: 0 sentence: 19320/57013 loss: 5.157792258014685\n",
      "epoch: 0 sentence: 19330/57013 loss: 5.286004939074614\n",
      "epoch: 0 sentence: 19340/57013 loss: 3.7638305853106067\n",
      "epoch: 0 sentence: 19350/57013 loss: 4.985837008140331\n",
      "epoch: 0 sentence: 19360/57013 loss: 4.623241389642513\n",
      "epoch: 0 sentence: 19370/57013 loss: 4.803917148525805\n",
      "epoch: 0 sentence: 19380/57013 loss: 2.548021615804056\n",
      "epoch: 0 sentence: 19390/57013 loss: 2.1910407363493136\n",
      "epoch: 0 sentence: 19400/57013 loss: 3.459270118965474\n",
      "epoch: 0 sentence: 19410/57013 loss: 4.403279792846528\n",
      "epoch: 0 sentence: 19420/57013 loss: 3.7933681998493785\n",
      "epoch: 0 sentence: 19430/57013 loss: 6.465023793191472\n",
      "epoch: 0 sentence: 19440/57013 loss: 6.281780482743088\n",
      "epoch: 0 sentence: 19450/57013 loss: 3.7158979190552768\n",
      "epoch: 0 sentence: 19460/57013 loss: 5.04419773654334\n",
      "epoch: 0 sentence: 19470/57013 loss: 4.88598140481535\n",
      "epoch: 0 sentence: 19480/57013 loss: 3.2437724899720735\n",
      "epoch: 0 sentence: 19490/57013 loss: 4.215703494217915\n",
      "epoch: 0 sentence: 19500/57013 loss: 3.9298953349201478\n",
      "epoch: 0 sentence: 19510/57013 loss: 5.105699915721165\n",
      "epoch: 0 sentence: 19520/57013 loss: 5.709680020758249\n",
      "epoch: 0 sentence: 19530/57013 loss: 5.6301169242310865\n",
      "epoch: 0 sentence: 19540/57013 loss: 3.86685404749957\n",
      "epoch: 0 sentence: 19550/57013 loss: 3.68206911795244\n",
      "epoch: 0 sentence: 19560/57013 loss: 5.43403606715218\n",
      "epoch: 0 sentence: 19570/57013 loss: 4.3849075437304705\n",
      "epoch: 0 sentence: 19580/57013 loss: 4.221161802219623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 19590/57013 loss: 4.460094839456322\n",
      "epoch: 0 sentence: 19600/57013 loss: 2.7725407397190103\n",
      "epoch: 0 sentence: 19610/57013 loss: 6.231535410349642\n",
      "epoch: 0 sentence: 19620/57013 loss: 2.2815487634977223\n",
      "epoch: 0 sentence: 19630/57013 loss: 4.2978734631102755\n",
      "epoch: 0 sentence: 19640/57013 loss: 4.87992694112466\n",
      "epoch: 0 sentence: 19650/57013 loss: 3.1471498286741144\n",
      "epoch: 0 sentence: 19660/57013 loss: 3.724764165581414\n",
      "epoch: 0 sentence: 19670/57013 loss: 4.290453297486472\n",
      "epoch: 0 sentence: 19680/57013 loss: 4.231985773225399\n",
      "epoch: 0 sentence: 19690/57013 loss: 5.238050848974632\n",
      "epoch: 0 sentence: 19700/57013 loss: 5.270523685880525\n",
      "epoch: 0 sentence: 19710/57013 loss: 4.438903514818604\n",
      "epoch: 0 sentence: 19720/57013 loss: 4.499967201767179\n",
      "epoch: 0 sentence: 19730/57013 loss: 5.584826458634699\n",
      "epoch: 0 sentence: 19740/57013 loss: 5.0127016736596754\n",
      "epoch: 0 sentence: 19750/57013 loss: 3.1907450902150383\n",
      "epoch: 0 sentence: 19760/57013 loss: 5.512505969615577\n",
      "epoch: 0 sentence: 19770/57013 loss: 4.878448150422623\n",
      "epoch: 0 sentence: 19780/57013 loss: 4.246525666595366\n",
      "epoch: 0 sentence: 19790/57013 loss: 4.228092468279778\n",
      "epoch: 0 sentence: 19800/57013 loss: 4.4360768740009275\n",
      "epoch: 0 sentence: 19810/57013 loss: 4.473294302920095\n",
      "epoch: 0 sentence: 19820/57013 loss: 5.131961207354321\n",
      "epoch: 0 sentence: 19830/57013 loss: 4.274672989200006\n",
      "epoch: 0 sentence: 19840/57013 loss: 3.9244231264603298\n",
      "epoch: 0 sentence: 19850/57013 loss: 4.366740886052008\n",
      "epoch: 0 sentence: 19860/57013 loss: 4.026393505477345\n",
      "epoch: 0 sentence: 19870/57013 loss: 5.385693359482712\n",
      "epoch: 0 sentence: 19880/57013 loss: 3.492799349480681\n",
      "epoch: 0 sentence: 19890/57013 loss: 4.998286001335566\n",
      "epoch: 0 sentence: 19900/57013 loss: 4.452046738028944\n",
      "epoch: 0 sentence: 19910/57013 loss: 3.935537572948656\n",
      "epoch: 0 sentence: 19920/57013 loss: 3.632672574439299\n",
      "epoch: 0 sentence: 19930/57013 loss: 5.610865737906171\n",
      "epoch: 0 sentence: 19940/57013 loss: 5.653993065351697\n",
      "epoch: 0 sentence: 19950/57013 loss: 5.0366487850953545\n",
      "epoch: 0 sentence: 19960/57013 loss: 4.181339454713944\n",
      "epoch: 0 sentence: 19970/57013 loss: 4.577097157866017\n",
      "epoch: 0 sentence: 19980/57013 loss: 5.453373426361998\n",
      "epoch: 0 sentence: 19990/57013 loss: 5.083985627873733\n",
      "epoch: 0 sentence: 20000/57013 loss: 4.271206283137269\n",
      "epoch: 0 sentence: 20010/57013 loss: 4.340616155807047\n",
      "epoch: 0 sentence: 20020/57013 loss: 4.194706468850467\n",
      "epoch: 0 sentence: 20030/57013 loss: 4.569381925117785\n",
      "epoch: 0 sentence: 20040/57013 loss: 4.275385654215065\n",
      "epoch: 0 sentence: 20050/57013 loss: 5.392452665499043\n",
      "epoch: 0 sentence: 20060/57013 loss: 5.471215896262809\n",
      "epoch: 0 sentence: 20070/57013 loss: 4.519932527999173\n",
      "epoch: 0 sentence: 20080/57013 loss: 4.70317372100529\n",
      "epoch: 0 sentence: 20090/57013 loss: 4.544121212565156\n",
      "epoch: 0 sentence: 20100/57013 loss: 3.7163339670653106\n",
      "epoch: 0 sentence: 20110/57013 loss: 4.695150055336741\n",
      "epoch: 0 sentence: 20120/57013 loss: 4.6860996367853645\n",
      "epoch: 0 sentence: 20130/57013 loss: 6.58949351554142\n",
      "epoch: 0 sentence: 20140/57013 loss: 5.300256252941102\n",
      "epoch: 0 sentence: 20150/57013 loss: 5.567534258004784\n",
      "epoch: 0 sentence: 20160/57013 loss: 4.1961934558340825\n",
      "epoch: 0 sentence: 20170/57013 loss: 4.125095215705132\n",
      "epoch: 0 sentence: 20180/57013 loss: 5.167172083898096\n",
      "epoch: 0 sentence: 20190/57013 loss: 4.211627363887352\n",
      "epoch: 0 sentence: 20200/57013 loss: 3.766551340839642\n",
      "epoch: 0 sentence: 20210/57013 loss: 4.766099373965567\n",
      "epoch: 0 sentence: 20220/57013 loss: 6.452384494516349\n",
      "epoch: 0 sentence: 20230/57013 loss: 4.843336959001787\n",
      "epoch: 0 sentence: 20240/57013 loss: 5.008545038825621\n",
      "epoch: 0 sentence: 20250/57013 loss: 5.008951850034776\n",
      "epoch: 0 sentence: 20260/57013 loss: 4.101329817029279\n",
      "epoch: 0 sentence: 20270/57013 loss: 4.471068892618135\n",
      "epoch: 0 sentence: 20280/57013 loss: 5.136726356256229\n",
      "epoch: 0 sentence: 20290/57013 loss: 5.97410081024806\n",
      "epoch: 0 sentence: 20300/57013 loss: 4.700737472451208\n",
      "epoch: 0 sentence: 20310/57013 loss: 5.526605291965535\n",
      "epoch: 0 sentence: 20320/57013 loss: 5.115717987156464\n",
      "epoch: 0 sentence: 20330/57013 loss: 6.021212435632607\n",
      "epoch: 0 sentence: 20340/57013 loss: 4.643874247933985\n",
      "epoch: 0 sentence: 20350/57013 loss: 5.528699715083837\n",
      "epoch: 0 sentence: 20360/57013 loss: 4.484309733287783\n",
      "epoch: 0 sentence: 20370/57013 loss: 4.9655214555053755\n",
      "epoch: 0 sentence: 20380/57013 loss: 4.067746007548532\n",
      "epoch: 0 sentence: 20390/57013 loss: 5.577905336393881\n",
      "epoch: 0 sentence: 20400/57013 loss: 4.194069195815373\n",
      "epoch: 0 sentence: 20410/57013 loss: 5.702979785193802\n",
      "epoch: 0 sentence: 20420/57013 loss: 4.326967781766891\n",
      "epoch: 0 sentence: 20430/57013 loss: 3.6201979330604517\n",
      "epoch: 0 sentence: 20440/57013 loss: 4.577367651546765\n",
      "epoch: 0 sentence: 20450/57013 loss: 3.6094604846870393\n",
      "epoch: 0 sentence: 20460/57013 loss: 4.687278564429963\n",
      "epoch: 0 sentence: 20470/57013 loss: 4.641035247015282\n",
      "epoch: 0 sentence: 20480/57013 loss: 2.034971652420423\n",
      "epoch: 0 sentence: 20490/57013 loss: 4.968670820595627\n",
      "epoch: 0 sentence: 20500/57013 loss: 5.182001145592145\n",
      "epoch: 0 sentence: 20510/57013 loss: 4.844548511362654\n",
      "epoch: 0 sentence: 20520/57013 loss: 6.27970633436057\n",
      "epoch: 0 sentence: 20530/57013 loss: 4.4088735317373295\n",
      "epoch: 0 sentence: 20540/57013 loss: 5.571919711952076\n",
      "epoch: 0 sentence: 20550/57013 loss: 3.3120311470037165\n",
      "epoch: 0 sentence: 20560/57013 loss: 3.73216002915131\n",
      "epoch: 0 sentence: 20570/57013 loss: 4.948797410492911\n",
      "epoch: 0 sentence: 20580/57013 loss: 3.7730541890016007\n",
      "epoch: 0 sentence: 20590/57013 loss: 3.0719419524561222\n",
      "epoch: 0 sentence: 20600/57013 loss: 4.616394353305556\n",
      "epoch: 0 sentence: 20610/57013 loss: 3.3925296692763984\n",
      "epoch: 0 sentence: 20620/57013 loss: 4.561942512849941\n",
      "epoch: 0 sentence: 20630/57013 loss: 5.596545932663418\n",
      "epoch: 0 sentence: 20640/57013 loss: 4.460448694502587\n",
      "epoch: 0 sentence: 20650/57013 loss: 4.936601828097745\n",
      "epoch: 0 sentence: 20660/57013 loss: 4.790491625135362\n",
      "epoch: 0 sentence: 20670/57013 loss: 5.298938613709202\n",
      "epoch: 0 sentence: 20680/57013 loss: 5.134022728345939\n",
      "epoch: 0 sentence: 20690/57013 loss: 5.8449784053045635\n",
      "epoch: 0 sentence: 20700/57013 loss: 4.336007447114783\n",
      "epoch: 0 sentence: 20710/57013 loss: 5.3188627202278695\n",
      "epoch: 0 sentence: 20720/57013 loss: 4.804553681407479\n",
      "epoch: 0 sentence: 20730/57013 loss: 2.1186053313505853\n",
      "epoch: 0 sentence: 20740/57013 loss: 4.4406911525252495\n",
      "epoch: 0 sentence: 20750/57013 loss: 5.531701238005805\n",
      "epoch: 0 sentence: 20760/57013 loss: 4.658438037606924\n",
      "epoch: 0 sentence: 20770/57013 loss: 4.966022834503239\n",
      "epoch: 0 sentence: 20780/57013 loss: 5.254659979860401\n",
      "epoch: 0 sentence: 20790/57013 loss: 4.8806759192703915\n",
      "epoch: 0 sentence: 20800/57013 loss: 4.932280418306366\n",
      "epoch: 0 sentence: 20810/57013 loss: 5.184669647732359\n",
      "epoch: 0 sentence: 20820/57013 loss: 4.2400711034534035\n",
      "epoch: 0 sentence: 20830/57013 loss: 5.1726877700076965\n",
      "epoch: 0 sentence: 20840/57013 loss: 3.5996555514261646\n",
      "epoch: 0 sentence: 20850/57013 loss: 4.778664833977485\n",
      "epoch: 0 sentence: 20860/57013 loss: 4.0253770044398545\n",
      "epoch: 0 sentence: 20870/57013 loss: 4.753677141275125\n",
      "epoch: 0 sentence: 20880/57013 loss: 3.550947898438127\n",
      "epoch: 0 sentence: 20890/57013 loss: 4.877787374415697\n",
      "epoch: 0 sentence: 20900/57013 loss: 4.57473571004821\n",
      "epoch: 0 sentence: 20910/57013 loss: 5.673311169302691\n",
      "epoch: 0 sentence: 20920/57013 loss: 5.3306199756586015\n",
      "epoch: 0 sentence: 20930/57013 loss: 3.4466960696296405\n",
      "epoch: 0 sentence: 20940/57013 loss: 4.202637245409902\n",
      "epoch: 0 sentence: 20950/57013 loss: 1.963122583910767\n",
      "epoch: 0 sentence: 20960/57013 loss: 5.777726712214646\n",
      "epoch: 0 sentence: 20970/57013 loss: 3.3513688155004484\n",
      "epoch: 0 sentence: 20980/57013 loss: 5.128190951280414\n",
      "epoch: 0 sentence: 20990/57013 loss: 6.659833116816564\n",
      "epoch: 0 sentence: 21000/57013 loss: 4.836985173612823\n",
      "epoch: 0 sentence: 21010/57013 loss: 3.205410680407231\n",
      "epoch: 0 sentence: 21020/57013 loss: 4.881558627127187\n",
      "epoch: 0 sentence: 21030/57013 loss: 5.1445416355021285\n",
      "epoch: 0 sentence: 21040/57013 loss: 5.561628481989338\n",
      "epoch: 0 sentence: 21050/57013 loss: 4.467363625203704\n",
      "epoch: 0 sentence: 21060/57013 loss: 5.142850721406539\n",
      "epoch: 0 sentence: 21070/57013 loss: 6.041551808384089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 21080/57013 loss: 4.477415760436222\n",
      "epoch: 0 sentence: 21090/57013 loss: 5.969214641352311\n",
      "epoch: 0 sentence: 21100/57013 loss: 4.800382703283385\n",
      "epoch: 0 sentence: 21110/57013 loss: 4.074749421775572\n",
      "epoch: 0 sentence: 21120/57013 loss: 5.901031613085301\n",
      "epoch: 0 sentence: 21130/57013 loss: 4.5173616577062505\n",
      "epoch: 0 sentence: 21140/57013 loss: 3.751694135282986\n",
      "epoch: 0 sentence: 21150/57013 loss: 4.690770130833451\n",
      "epoch: 0 sentence: 21160/57013 loss: 3.300026433364562\n",
      "epoch: 0 sentence: 21170/57013 loss: 4.55489059115848\n",
      "epoch: 0 sentence: 21180/57013 loss: 5.723303448472529\n",
      "epoch: 0 sentence: 21190/57013 loss: 5.041174617874476\n",
      "epoch: 0 sentence: 21200/57013 loss: 4.26640081282036\n",
      "epoch: 0 sentence: 21210/57013 loss: 5.91476766742943\n",
      "epoch: 0 sentence: 21220/57013 loss: 5.724608351908923\n",
      "epoch: 0 sentence: 21230/57013 loss: 5.399675973551847\n",
      "epoch: 0 sentence: 21240/57013 loss: 4.647347382491961\n",
      "epoch: 0 sentence: 21250/57013 loss: 4.18164029596842\n",
      "epoch: 0 sentence: 21260/57013 loss: 4.0642882737829815\n",
      "epoch: 0 sentence: 21270/57013 loss: 4.386959510633519\n",
      "epoch: 0 sentence: 21280/57013 loss: 4.850891116105752\n",
      "epoch: 0 sentence: 21290/57013 loss: 4.50813903561799\n",
      "epoch: 0 sentence: 21300/57013 loss: 5.0676715195170665\n",
      "epoch: 0 sentence: 21310/57013 loss: 5.749555204466185\n",
      "epoch: 0 sentence: 21320/57013 loss: 5.372126622980991\n",
      "epoch: 0 sentence: 21330/57013 loss: 5.45314265627128\n",
      "epoch: 0 sentence: 21340/57013 loss: 6.031783341549487\n",
      "epoch: 0 sentence: 21350/57013 loss: 4.7884647074839135\n",
      "epoch: 0 sentence: 21360/57013 loss: 4.0719325853422665\n",
      "epoch: 0 sentence: 21370/57013 loss: 5.40367754053534\n",
      "epoch: 0 sentence: 21380/57013 loss: 5.105906348134227\n",
      "epoch: 0 sentence: 21390/57013 loss: 4.3891484372258205\n",
      "epoch: 0 sentence: 21400/57013 loss: 5.024254282748835\n",
      "epoch: 0 sentence: 21410/57013 loss: 5.504828907367439\n",
      "epoch: 0 sentence: 21420/57013 loss: 4.493493939960671\n",
      "epoch: 0 sentence: 21430/57013 loss: 5.347683064920216\n",
      "epoch: 0 sentence: 21440/57013 loss: 4.973757781424737\n",
      "epoch: 0 sentence: 21450/57013 loss: 5.287336348469429\n",
      "epoch: 0 sentence: 21460/57013 loss: 5.1689816925741106\n",
      "epoch: 0 sentence: 21470/57013 loss: 4.385521921693208\n",
      "epoch: 0 sentence: 21480/57013 loss: 4.801107639644513\n",
      "epoch: 0 sentence: 21490/57013 loss: 5.233674152024003\n",
      "epoch: 0 sentence: 21500/57013 loss: 5.337090017595483\n",
      "epoch: 0 sentence: 21510/57013 loss: 2.32745195168321\n",
      "epoch: 0 sentence: 21520/57013 loss: 5.027469036907723\n",
      "epoch: 0 sentence: 21530/57013 loss: 5.223835706085944\n",
      "epoch: 0 sentence: 21540/57013 loss: 3.5141879713906037\n",
      "epoch: 0 sentence: 21550/57013 loss: 5.552283342283773\n",
      "epoch: 0 sentence: 21560/57013 loss: 1.4031411519130714\n",
      "epoch: 0 sentence: 21570/57013 loss: 5.090255531290036\n",
      "epoch: 0 sentence: 21580/57013 loss: 3.903347719774397\n",
      "epoch: 0 sentence: 21590/57013 loss: 4.657542612564767\n",
      "epoch: 0 sentence: 21600/57013 loss: 4.933488346785602\n",
      "epoch: 0 sentence: 21610/57013 loss: 5.457721075986035\n",
      "epoch: 0 sentence: 21620/57013 loss: 4.227611851408416\n",
      "epoch: 0 sentence: 21630/57013 loss: 5.25116729292015\n",
      "epoch: 0 sentence: 21640/57013 loss: 3.8792030860876245\n",
      "epoch: 0 sentence: 21650/57013 loss: 3.1718371705927346\n",
      "epoch: 0 sentence: 21660/57013 loss: 2.107101421842541\n",
      "epoch: 0 sentence: 21670/57013 loss: 5.3343492572425\n",
      "epoch: 0 sentence: 21680/57013 loss: 4.3621208374584794\n",
      "epoch: 0 sentence: 21690/57013 loss: 5.426906601813411\n",
      "epoch: 0 sentence: 21700/57013 loss: 5.245143129258799\n",
      "epoch: 0 sentence: 21710/57013 loss: 4.338957107266471\n",
      "epoch: 0 sentence: 21720/57013 loss: 4.035713760502924\n",
      "epoch: 0 sentence: 21730/57013 loss: 5.51980663126673\n",
      "epoch: 0 sentence: 21740/57013 loss: 4.301074840642164\n",
      "epoch: 0 sentence: 21750/57013 loss: 2.3026695753856345\n",
      "epoch: 0 sentence: 21760/57013 loss: 4.951712939882304\n",
      "epoch: 0 sentence: 21770/57013 loss: 3.3640084583627736\n",
      "epoch: 0 sentence: 21780/57013 loss: 4.87202947118857\n",
      "epoch: 0 sentence: 21790/57013 loss: 5.716520020542863\n",
      "epoch: 0 sentence: 21800/57013 loss: 5.357868660984617\n",
      "epoch: 0 sentence: 21810/57013 loss: 5.062267471283241\n",
      "epoch: 0 sentence: 21820/57013 loss: 4.409205827517411\n",
      "epoch: 0 sentence: 21830/57013 loss: 6.167502106560178\n",
      "epoch: 0 sentence: 21840/57013 loss: 5.143228813880232\n",
      "epoch: 0 sentence: 21850/57013 loss: 3.8340422782022068\n",
      "epoch: 0 sentence: 21860/57013 loss: 3.526104625874565\n",
      "epoch: 0 sentence: 21870/57013 loss: 4.532455743832414\n",
      "epoch: 0 sentence: 21880/57013 loss: 3.8397942610021785\n",
      "epoch: 0 sentence: 21890/57013 loss: 3.778489215054437\n",
      "epoch: 0 sentence: 21900/57013 loss: 4.7435336193929505\n",
      "epoch: 0 sentence: 21910/57013 loss: 4.321498549660057\n",
      "epoch: 0 sentence: 21920/57013 loss: 5.44909971491043\n",
      "epoch: 0 sentence: 21930/57013 loss: 4.004026476267482\n",
      "epoch: 0 sentence: 21940/57013 loss: 4.75376055140407\n",
      "epoch: 0 sentence: 21950/57013 loss: 4.077863645739336\n",
      "epoch: 0 sentence: 21960/57013 loss: 5.050165962435331\n",
      "epoch: 0 sentence: 21970/57013 loss: 3.7251298510666104\n",
      "epoch: 0 sentence: 21980/57013 loss: 5.493257581380123\n",
      "epoch: 0 sentence: 21990/57013 loss: 5.83072911221684\n",
      "epoch: 0 sentence: 22000/57013 loss: 7.765095572500073\n",
      "epoch: 0 sentence: 22010/57013 loss: 4.953045251635612\n",
      "epoch: 0 sentence: 22020/57013 loss: 4.279353184165906\n",
      "epoch: 0 sentence: 22030/57013 loss: 4.455775639796469\n",
      "epoch: 0 sentence: 22040/57013 loss: 4.791101901856626\n",
      "epoch: 0 sentence: 22050/57013 loss: 4.098272449476832\n",
      "epoch: 0 sentence: 22060/57013 loss: 2.8799743236430007\n",
      "epoch: 0 sentence: 22070/57013 loss: 3.5245109176188736\n",
      "epoch: 0 sentence: 22080/57013 loss: 5.690571711795564\n",
      "epoch: 0 sentence: 22090/57013 loss: 3.871818191391208\n",
      "epoch: 0 sentence: 22100/57013 loss: 3.215752547081372\n",
      "epoch: 0 sentence: 22110/57013 loss: 4.805543616538837\n",
      "epoch: 0 sentence: 22120/57013 loss: 4.861138437386702\n",
      "epoch: 0 sentence: 22130/57013 loss: 4.582301658086032\n",
      "epoch: 0 sentence: 22140/57013 loss: 4.663825655054279\n",
      "epoch: 0 sentence: 22150/57013 loss: 6.071062637898166\n",
      "epoch: 0 sentence: 22160/57013 loss: 5.69500882372312\n",
      "epoch: 0 sentence: 22170/57013 loss: 5.329103690694121\n",
      "epoch: 0 sentence: 22180/57013 loss: 4.930188966429735\n",
      "epoch: 0 sentence: 22190/57013 loss: 4.7351892716994595\n",
      "epoch: 0 sentence: 22200/57013 loss: 4.6830450225317355\n",
      "epoch: 0 sentence: 22210/57013 loss: 3.5805024105913468\n",
      "epoch: 0 sentence: 22220/57013 loss: 3.438722462647431\n",
      "epoch: 0 sentence: 22230/57013 loss: 5.399071367869675\n",
      "epoch: 0 sentence: 22240/57013 loss: 1.8147732088152129\n",
      "epoch: 0 sentence: 22250/57013 loss: 5.097827296646136\n",
      "epoch: 0 sentence: 22260/57013 loss: 4.767562904396773\n",
      "epoch: 0 sentence: 22270/57013 loss: 4.887080653319917\n",
      "epoch: 0 sentence: 22280/57013 loss: 2.6045095162299825\n",
      "epoch: 0 sentence: 22290/57013 loss: 5.634645653434149\n",
      "epoch: 0 sentence: 22300/57013 loss: 6.069597519807713\n",
      "epoch: 0 sentence: 22310/57013 loss: 2.407433984667896\n",
      "epoch: 0 sentence: 22320/57013 loss: 5.417696901090351\n",
      "epoch: 0 sentence: 22330/57013 loss: 5.31411028457013\n",
      "epoch: 0 sentence: 22340/57013 loss: 5.060717671936802\n",
      "epoch: 0 sentence: 22350/57013 loss: 4.585762008574741\n",
      "epoch: 0 sentence: 22360/57013 loss: 6.05747648226585\n",
      "epoch: 0 sentence: 22370/57013 loss: 4.48000071230099\n",
      "epoch: 0 sentence: 22380/57013 loss: 4.323974341391601\n",
      "epoch: 0 sentence: 22390/57013 loss: 6.0385043192899115\n",
      "epoch: 0 sentence: 22400/57013 loss: 3.7487594656874084\n",
      "epoch: 0 sentence: 22410/57013 loss: 5.909861116920824\n",
      "epoch: 0 sentence: 22420/57013 loss: 5.400853141959777\n",
      "epoch: 0 sentence: 22430/57013 loss: 5.164730098797963\n",
      "epoch: 0 sentence: 22440/57013 loss: 1.9564864621150837\n",
      "epoch: 0 sentence: 22450/57013 loss: 4.798803350796429\n",
      "epoch: 0 sentence: 22460/57013 loss: 5.168518927027207\n",
      "epoch: 0 sentence: 22470/57013 loss: 4.898061222737879\n",
      "epoch: 0 sentence: 22480/57013 loss: 4.986078572895385\n",
      "epoch: 0 sentence: 22490/57013 loss: 4.962536901767795\n",
      "epoch: 0 sentence: 22500/57013 loss: 4.47686975433712\n",
      "epoch: 0 sentence: 22510/57013 loss: 4.46286020315065\n",
      "epoch: 0 sentence: 22520/57013 loss: 5.460639312214954\n",
      "epoch: 0 sentence: 22530/57013 loss: 5.036128960569086\n",
      "epoch: 0 sentence: 22540/57013 loss: 6.050545403474483\n",
      "epoch: 0 sentence: 22550/57013 loss: 4.831447464693889\n",
      "epoch: 0 sentence: 22560/57013 loss: 3.779917238424439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 22570/57013 loss: 4.836252345653157\n",
      "epoch: 0 sentence: 22580/57013 loss: 5.349891000666695\n",
      "epoch: 0 sentence: 22590/57013 loss: 5.147254364939813\n",
      "epoch: 0 sentence: 22600/57013 loss: 5.173467664477288\n",
      "epoch: 0 sentence: 22610/57013 loss: 5.037250219881488\n",
      "epoch: 0 sentence: 22620/57013 loss: 6.13293907716265\n",
      "epoch: 0 sentence: 22630/57013 loss: 4.657926975282001\n",
      "epoch: 0 sentence: 22640/57013 loss: 3.604283560727788\n",
      "epoch: 0 sentence: 22650/57013 loss: 4.541069342973669\n",
      "epoch: 0 sentence: 22660/57013 loss: 5.109871737074886\n",
      "epoch: 0 sentence: 22670/57013 loss: 4.622680405994458\n",
      "epoch: 0 sentence: 22680/57013 loss: 5.0372671374771265\n",
      "epoch: 0 sentence: 22690/57013 loss: 4.561358374074178\n",
      "epoch: 0 sentence: 22700/57013 loss: 3.5771470575105417\n",
      "epoch: 0 sentence: 22710/57013 loss: 4.582149930348786\n",
      "epoch: 0 sentence: 22720/57013 loss: 3.4257041684590335\n",
      "epoch: 0 sentence: 22730/57013 loss: 5.085384751715412\n",
      "epoch: 0 sentence: 22740/57013 loss: 5.497644645152593\n",
      "epoch: 0 sentence: 22750/57013 loss: 3.9874768759319803\n",
      "epoch: 0 sentence: 22760/57013 loss: 5.760828011628178\n",
      "epoch: 0 sentence: 22770/57013 loss: 5.69978734810256\n",
      "epoch: 0 sentence: 22780/57013 loss: 5.615446701244714\n",
      "epoch: 0 sentence: 22790/57013 loss: 5.092918349804917\n",
      "epoch: 0 sentence: 22800/57013 loss: 2.9514625668138077\n",
      "epoch: 0 sentence: 22810/57013 loss: 3.741513857858963\n",
      "epoch: 0 sentence: 22820/57013 loss: 5.541870231155151\n",
      "epoch: 0 sentence: 22830/57013 loss: 5.376492747168004\n",
      "epoch: 0 sentence: 22840/57013 loss: 5.598356084703825\n",
      "epoch: 0 sentence: 22850/57013 loss: 4.997529650711396\n",
      "epoch: 0 sentence: 22860/57013 loss: 4.076685172560767\n",
      "epoch: 0 sentence: 22870/57013 loss: 3.1370440177565033\n",
      "epoch: 0 sentence: 22880/57013 loss: 4.60979479371529\n",
      "epoch: 0 sentence: 22890/57013 loss: 5.599728053019365\n",
      "epoch: 0 sentence: 22900/57013 loss: 5.013914959987901\n",
      "epoch: 0 sentence: 22910/57013 loss: 5.532593840065641\n",
      "epoch: 0 sentence: 22920/57013 loss: 3.441783026724595\n",
      "epoch: 0 sentence: 22930/57013 loss: 5.080381291800058\n",
      "epoch: 0 sentence: 22940/57013 loss: 4.428147861133596\n",
      "epoch: 0 sentence: 22950/57013 loss: 3.212961720016802\n",
      "epoch: 0 sentence: 22960/57013 loss: 3.2539476968236545\n",
      "epoch: 0 sentence: 22970/57013 loss: 5.274736391416589\n",
      "epoch: 0 sentence: 22980/57013 loss: 5.300183518421317\n",
      "epoch: 0 sentence: 22990/57013 loss: 5.077081006762528\n",
      "epoch: 0 sentence: 23000/57013 loss: 3.6299110476336436\n",
      "epoch: 0 sentence: 23010/57013 loss: 4.943961580465215\n",
      "epoch: 0 sentence: 23020/57013 loss: 5.778648982228031\n",
      "epoch: 0 sentence: 23030/57013 loss: 5.245445626432415\n",
      "epoch: 0 sentence: 23040/57013 loss: 4.049233858069474\n",
      "epoch: 0 sentence: 23050/57013 loss: 3.376462400125128\n",
      "epoch: 0 sentence: 23060/57013 loss: 5.6727222123021495\n",
      "epoch: 0 sentence: 23070/57013 loss: 5.597546803231404\n",
      "epoch: 0 sentence: 23080/57013 loss: 5.449266709462029\n",
      "epoch: 0 sentence: 23090/57013 loss: 4.4224261131258915\n",
      "epoch: 0 sentence: 23100/57013 loss: 4.3021867517977\n",
      "epoch: 0 sentence: 23110/57013 loss: 4.316998041909145\n",
      "epoch: 0 sentence: 23120/57013 loss: 4.451380527282287\n",
      "epoch: 0 sentence: 23130/57013 loss: 6.002805072525885\n",
      "epoch: 0 sentence: 23140/57013 loss: 5.613465968321788\n",
      "epoch: 0 sentence: 23150/57013 loss: 5.582530787346594\n",
      "epoch: 0 sentence: 23160/57013 loss: 4.741044680792205\n",
      "epoch: 0 sentence: 23170/57013 loss: 6.357722083220319\n",
      "epoch: 0 sentence: 23180/57013 loss: 3.8021153931082257\n",
      "epoch: 0 sentence: 23190/57013 loss: 5.262424308890744\n",
      "epoch: 0 sentence: 23200/57013 loss: 3.556526423955938\n",
      "epoch: 0 sentence: 23210/57013 loss: 4.722118838595912\n",
      "epoch: 0 sentence: 23220/57013 loss: 6.143287248389701\n",
      "epoch: 0 sentence: 23230/57013 loss: 4.615693039661387\n",
      "epoch: 0 sentence: 23240/57013 loss: 4.809425290294651\n",
      "epoch: 0 sentence: 23250/57013 loss: 2.666109632597359\n",
      "epoch: 0 sentence: 23260/57013 loss: 4.44747287164713\n",
      "epoch: 0 sentence: 23270/57013 loss: 4.858470563744355\n",
      "epoch: 0 sentence: 23280/57013 loss: 4.4157684210221575\n",
      "epoch: 0 sentence: 23290/57013 loss: 5.552925601718351\n",
      "epoch: 0 sentence: 23300/57013 loss: 5.087070907210935\n",
      "epoch: 0 sentence: 23310/57013 loss: 3.2303609563910545\n",
      "epoch: 0 sentence: 23320/57013 loss: 3.6724826000316066\n",
      "epoch: 0 sentence: 23330/57013 loss: 5.803634415918083\n",
      "epoch: 0 sentence: 23340/57013 loss: 3.759051637066223\n",
      "epoch: 0 sentence: 23350/57013 loss: 2.866934974478916\n",
      "epoch: 0 sentence: 23360/57013 loss: 4.688216039867493\n",
      "epoch: 0 sentence: 23370/57013 loss: 5.313944497755218\n",
      "epoch: 0 sentence: 23380/57013 loss: 5.822493105703662\n",
      "epoch: 0 sentence: 23390/57013 loss: 4.936069374849266\n",
      "epoch: 0 sentence: 23400/57013 loss: 4.349873521014042\n",
      "epoch: 0 sentence: 23410/57013 loss: 3.7848992720931616\n",
      "epoch: 0 sentence: 23420/57013 loss: 5.411229503239951\n",
      "epoch: 0 sentence: 23430/57013 loss: 4.992995080290327\n",
      "epoch: 0 sentence: 23440/57013 loss: 4.171212214299536\n",
      "epoch: 0 sentence: 23450/57013 loss: 5.015872218226881\n",
      "epoch: 0 sentence: 23460/57013 loss: 5.636439937660512\n",
      "epoch: 0 sentence: 23470/57013 loss: 4.11575631778061\n",
      "epoch: 0 sentence: 23480/57013 loss: 4.716278499140957\n",
      "epoch: 0 sentence: 23490/57013 loss: 4.879471834796616\n",
      "epoch: 0 sentence: 23500/57013 loss: 3.4944009012092336\n",
      "epoch: 0 sentence: 23510/57013 loss: 4.739263603400032\n",
      "epoch: 0 sentence: 23520/57013 loss: 5.580467102773933\n",
      "epoch: 0 sentence: 23530/57013 loss: 3.691944648523244\n",
      "epoch: 0 sentence: 23540/57013 loss: 4.2420496755649735\n",
      "epoch: 0 sentence: 23550/57013 loss: 4.228964918302966\n",
      "epoch: 0 sentence: 23560/57013 loss: 4.226200226752609\n",
      "epoch: 0 sentence: 23570/57013 loss: 4.249823728849643\n",
      "epoch: 0 sentence: 23580/57013 loss: 5.438126051986341\n",
      "epoch: 0 sentence: 23590/57013 loss: 5.59522105239535\n",
      "epoch: 0 sentence: 23600/57013 loss: 5.560764124121149\n",
      "epoch: 0 sentence: 23610/57013 loss: 4.695203411202628\n",
      "epoch: 0 sentence: 23620/57013 loss: 3.785057742669193\n",
      "epoch: 0 sentence: 23630/57013 loss: 4.2283673413882585\n",
      "epoch: 0 sentence: 23640/57013 loss: 5.533669923011488\n",
      "epoch: 0 sentence: 23650/57013 loss: 3.741885408564833\n",
      "epoch: 0 sentence: 23660/57013 loss: 3.9004788544625213\n",
      "epoch: 0 sentence: 23670/57013 loss: 5.145135152118181\n",
      "epoch: 0 sentence: 23680/57013 loss: 5.282800823086218\n",
      "epoch: 0 sentence: 23690/57013 loss: 2.9686950157673357\n",
      "epoch: 0 sentence: 23700/57013 loss: 4.5876887554057655\n",
      "epoch: 0 sentence: 23710/57013 loss: 5.344341216467307\n",
      "epoch: 0 sentence: 23720/57013 loss: 3.888879936283555\n",
      "epoch: 0 sentence: 23730/57013 loss: 3.009066370851042\n",
      "epoch: 0 sentence: 23740/57013 loss: 4.461100150882349\n",
      "epoch: 0 sentence: 23750/57013 loss: 4.017023285752216\n",
      "epoch: 0 sentence: 23760/57013 loss: 6.788273650210676\n",
      "epoch: 0 sentence: 23770/57013 loss: 6.415102132573252\n",
      "epoch: 0 sentence: 23780/57013 loss: 5.854879367533854\n",
      "epoch: 0 sentence: 23790/57013 loss: 4.374917463485437\n",
      "epoch: 0 sentence: 23800/57013 loss: 4.317131527179829\n",
      "epoch: 0 sentence: 23810/57013 loss: 6.734622152407839\n",
      "epoch: 0 sentence: 23820/57013 loss: 4.168962549684016\n",
      "epoch: 0 sentence: 23830/57013 loss: 4.744519762979136\n",
      "epoch: 0 sentence: 23840/57013 loss: 3.8047124052020838\n",
      "epoch: 0 sentence: 23850/57013 loss: 5.1866037605911\n",
      "epoch: 0 sentence: 23860/57013 loss: 5.802492066943683\n",
      "epoch: 0 sentence: 23870/57013 loss: 4.042688500511534\n",
      "epoch: 0 sentence: 23880/57013 loss: 4.282222778425613\n",
      "epoch: 0 sentence: 23890/57013 loss: 5.287559854725338\n",
      "epoch: 0 sentence: 23900/57013 loss: 3.652830443608375\n",
      "epoch: 0 sentence: 23910/57013 loss: 5.323388454807618\n",
      "epoch: 0 sentence: 23920/57013 loss: 4.213744925413722\n",
      "epoch: 0 sentence: 23930/57013 loss: 6.430543820544777\n",
      "epoch: 0 sentence: 23940/57013 loss: 4.934631853985009\n",
      "epoch: 0 sentence: 23950/57013 loss: 5.1993497860545945\n",
      "epoch: 0 sentence: 23960/57013 loss: 5.285026814374835\n",
      "epoch: 0 sentence: 23970/57013 loss: 4.485995021100669\n",
      "epoch: 0 sentence: 23980/57013 loss: 5.8109646159695885\n",
      "epoch: 0 sentence: 23990/57013 loss: 4.0675818752716255\n",
      "epoch: 0 sentence: 24000/57013 loss: 4.600828906942751\n",
      "epoch: 0 sentence: 24010/57013 loss: 3.4071210823214684\n",
      "epoch: 0 sentence: 24020/57013 loss: 4.454191802776838\n",
      "epoch: 0 sentence: 24030/57013 loss: 4.484504733607563\n",
      "epoch: 0 sentence: 24040/57013 loss: 4.652680697760398\n",
      "epoch: 0 sentence: 24050/57013 loss: 4.759692443603072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 24060/57013 loss: 4.635583014762358\n",
      "epoch: 0 sentence: 24070/57013 loss: 4.675303265764465\n",
      "epoch: 0 sentence: 24080/57013 loss: 4.503610034132\n",
      "epoch: 0 sentence: 24090/57013 loss: 5.995470029862217\n",
      "epoch: 0 sentence: 24100/57013 loss: 3.422240485084474\n",
      "epoch: 0 sentence: 24110/57013 loss: 4.858212687676444\n",
      "epoch: 0 sentence: 24120/57013 loss: 4.721509695061107\n",
      "epoch: 0 sentence: 24130/57013 loss: 1.8931995259119023\n",
      "epoch: 0 sentence: 24140/57013 loss: 4.903094055968222\n",
      "epoch: 0 sentence: 24150/57013 loss: 5.639047873583671\n",
      "epoch: 0 sentence: 24160/57013 loss: 4.5271587345858055\n",
      "epoch: 0 sentence: 24170/57013 loss: 3.08685693333111\n",
      "epoch: 0 sentence: 24180/57013 loss: 5.972579748036435\n",
      "epoch: 0 sentence: 24190/57013 loss: 4.355731564861056\n",
      "epoch: 0 sentence: 24200/57013 loss: 4.055588574573081\n",
      "epoch: 0 sentence: 24210/57013 loss: 5.579046747630308\n",
      "epoch: 0 sentence: 24220/57013 loss: 4.7069090659164345\n",
      "epoch: 0 sentence: 24230/57013 loss: 3.439939576186253\n",
      "epoch: 0 sentence: 24240/57013 loss: 5.162916315191931\n",
      "epoch: 0 sentence: 24250/57013 loss: 3.0478524843019645\n",
      "epoch: 0 sentence: 24260/57013 loss: 5.556144284697699\n",
      "epoch: 0 sentence: 24270/57013 loss: 3.385474518477555\n",
      "epoch: 0 sentence: 24280/57013 loss: 3.924614919197991\n",
      "epoch: 0 sentence: 24290/57013 loss: 3.885039622523154\n",
      "epoch: 0 sentence: 24300/57013 loss: 4.312507925670765\n",
      "epoch: 0 sentence: 24310/57013 loss: 3.6498520683300093\n",
      "epoch: 0 sentence: 24320/57013 loss: 5.27576136357863\n",
      "epoch: 0 sentence: 24330/57013 loss: 4.160062270354892\n",
      "epoch: 0 sentence: 24340/57013 loss: 5.87057406834558\n",
      "epoch: 0 sentence: 24350/57013 loss: 5.475854433231228\n",
      "epoch: 0 sentence: 24360/57013 loss: 3.7977029174472614\n",
      "epoch: 0 sentence: 24370/57013 loss: 4.612047336582768\n",
      "epoch: 0 sentence: 24380/57013 loss: 4.934785411277324\n",
      "epoch: 0 sentence: 24390/57013 loss: 4.286788095004475\n",
      "epoch: 0 sentence: 24400/57013 loss: 3.514217047156475\n",
      "epoch: 0 sentence: 24410/57013 loss: 3.007983926839859\n",
      "epoch: 0 sentence: 24420/57013 loss: 3.9960968089239604\n",
      "epoch: 0 sentence: 24430/57013 loss: 5.684897944047546\n",
      "epoch: 0 sentence: 24440/57013 loss: 5.0780228703866825\n",
      "epoch: 0 sentence: 24450/57013 loss: 4.085817560702777\n",
      "epoch: 0 sentence: 24460/57013 loss: 4.0771663755186225\n",
      "epoch: 0 sentence: 24470/57013 loss: 4.036555912465277\n",
      "epoch: 0 sentence: 24480/57013 loss: 4.759480574428546\n",
      "epoch: 0 sentence: 24490/57013 loss: 5.700946080445062\n",
      "epoch: 0 sentence: 24500/57013 loss: 4.760958499896781\n",
      "epoch: 0 sentence: 24510/57013 loss: 5.142299243563226\n",
      "epoch: 0 sentence: 24520/57013 loss: 4.2084629195999534\n",
      "epoch: 0 sentence: 24530/57013 loss: 4.752591999929432\n",
      "epoch: 0 sentence: 24540/57013 loss: 3.6104540609355964\n",
      "epoch: 0 sentence: 24550/57013 loss: 5.8386053416682255\n",
      "epoch: 0 sentence: 24560/57013 loss: 2.2027350925849416\n",
      "epoch: 0 sentence: 24570/57013 loss: 4.196494381793168\n",
      "epoch: 0 sentence: 24580/57013 loss: 4.6242744318820295\n",
      "epoch: 0 sentence: 24590/57013 loss: 5.029323598850075\n",
      "epoch: 0 sentence: 24600/57013 loss: 4.480915580542808\n",
      "epoch: 0 sentence: 24610/57013 loss: 4.747146588309868\n",
      "epoch: 0 sentence: 24620/57013 loss: 3.4800494091294847\n",
      "epoch: 0 sentence: 24630/57013 loss: 1.5762248250515407\n",
      "epoch: 0 sentence: 24640/57013 loss: 3.8667587795476166\n",
      "epoch: 0 sentence: 24650/57013 loss: 3.863347650254402\n",
      "epoch: 0 sentence: 24660/57013 loss: 4.058270022142833\n",
      "epoch: 0 sentence: 24670/57013 loss: 3.9814569363799674\n",
      "epoch: 0 sentence: 24680/57013 loss: 4.450951107586128\n",
      "epoch: 0 sentence: 24690/57013 loss: 5.2262490927804715\n",
      "epoch: 0 sentence: 24700/57013 loss: 2.936548805119327\n",
      "epoch: 0 sentence: 24710/57013 loss: 5.549791611143535\n",
      "epoch: 0 sentence: 24720/57013 loss: 4.289575226455227\n",
      "epoch: 0 sentence: 24730/57013 loss: 5.634346233450511\n",
      "epoch: 0 sentence: 24740/57013 loss: 5.189247469540178\n",
      "epoch: 0 sentence: 24750/57013 loss: 5.052964276115123\n",
      "epoch: 0 sentence: 24760/57013 loss: 3.700258443018352\n",
      "epoch: 0 sentence: 24770/57013 loss: 4.935653598867132\n",
      "epoch: 0 sentence: 24780/57013 loss: 3.666280277306895\n",
      "epoch: 0 sentence: 24790/57013 loss: 3.2196360605400036\n",
      "epoch: 0 sentence: 24800/57013 loss: 5.054235867398642\n",
      "epoch: 0 sentence: 24810/57013 loss: 4.456462455398498\n",
      "epoch: 0 sentence: 24820/57013 loss: 3.9710617815895493\n",
      "epoch: 0 sentence: 24830/57013 loss: 4.776494782922118\n",
      "epoch: 0 sentence: 24840/57013 loss: 1.4910646560653544\n",
      "epoch: 0 sentence: 24850/57013 loss: 5.0134968790848315\n",
      "epoch: 0 sentence: 24860/57013 loss: 4.431037334733445\n",
      "epoch: 0 sentence: 24870/57013 loss: 4.874555018479604\n",
      "epoch: 0 sentence: 24880/57013 loss: 4.932306406557578\n",
      "epoch: 0 sentence: 24890/57013 loss: 4.403389748886858\n",
      "epoch: 0 sentence: 24900/57013 loss: 4.502035817536063\n",
      "epoch: 0 sentence: 24910/57013 loss: 4.076548341401764\n",
      "epoch: 0 sentence: 24920/57013 loss: 4.882624694454149\n",
      "epoch: 0 sentence: 24930/57013 loss: 4.594535435284964\n",
      "epoch: 0 sentence: 24940/57013 loss: 3.165570385335347\n",
      "epoch: 0 sentence: 24950/57013 loss: 5.050404804129516\n",
      "epoch: 0 sentence: 24960/57013 loss: 4.860307640351918\n",
      "epoch: 0 sentence: 24970/57013 loss: 6.795732137151195\n",
      "epoch: 0 sentence: 24980/57013 loss: 4.88507687922239\n",
      "epoch: 0 sentence: 24990/57013 loss: 4.013890333854118\n",
      "epoch: 0 sentence: 25000/57013 loss: 5.110732654343817\n",
      "epoch: 0 sentence: 25010/57013 loss: 5.463476785946541\n",
      "epoch: 0 sentence: 25020/57013 loss: 5.443794991198623\n",
      "epoch: 0 sentence: 25030/57013 loss: 5.197285942479119\n",
      "epoch: 0 sentence: 25040/57013 loss: 3.658433527818779\n",
      "epoch: 0 sentence: 25050/57013 loss: 4.1534285881661805\n",
      "epoch: 0 sentence: 25060/57013 loss: 5.575245319895418\n",
      "epoch: 0 sentence: 25070/57013 loss: 4.534066520097602\n",
      "epoch: 0 sentence: 25080/57013 loss: 5.068908125127567\n",
      "epoch: 0 sentence: 25090/57013 loss: 2.449630837551375\n",
      "epoch: 0 sentence: 25100/57013 loss: 4.183435957352625\n",
      "epoch: 0 sentence: 25110/57013 loss: 6.034789705660384\n",
      "epoch: 0 sentence: 25120/57013 loss: 5.463554321411753\n",
      "epoch: 0 sentence: 25130/57013 loss: 4.031539356901344\n",
      "epoch: 0 sentence: 25140/57013 loss: 3.450650324076054\n",
      "epoch: 0 sentence: 25150/57013 loss: 5.7634759004325185\n",
      "epoch: 0 sentence: 25160/57013 loss: 4.981761444027044\n",
      "epoch: 0 sentence: 25170/57013 loss: 3.8690655776693608\n",
      "epoch: 0 sentence: 25180/57013 loss: 4.791912816133751\n",
      "epoch: 0 sentence: 25190/57013 loss: 3.2956567558225514\n",
      "epoch: 0 sentence: 25200/57013 loss: 4.183460912504705\n",
      "epoch: 0 sentence: 25210/57013 loss: 5.027433374046679\n",
      "epoch: 0 sentence: 25220/57013 loss: 4.266846360028624\n",
      "epoch: 0 sentence: 25230/57013 loss: 4.385570872958532\n",
      "epoch: 0 sentence: 25240/57013 loss: 5.16700266992848\n",
      "epoch: 0 sentence: 25250/57013 loss: 3.8442071772767186\n",
      "epoch: 0 sentence: 25260/57013 loss: 4.481209162688303\n",
      "epoch: 0 sentence: 25270/57013 loss: 3.903184519137051\n",
      "epoch: 0 sentence: 25280/57013 loss: 4.583757093143295\n",
      "epoch: 0 sentence: 25290/57013 loss: 4.081383327440309\n",
      "epoch: 0 sentence: 25300/57013 loss: 5.743026818833773\n",
      "epoch: 0 sentence: 25310/57013 loss: 5.697772979330602\n",
      "epoch: 0 sentence: 25320/57013 loss: 4.961737037578381\n",
      "epoch: 0 sentence: 25330/57013 loss: 5.292231020340158\n",
      "epoch: 0 sentence: 25340/57013 loss: 5.315896943053779\n",
      "epoch: 0 sentence: 25350/57013 loss: 5.684601946735061\n",
      "epoch: 0 sentence: 25360/57013 loss: 4.922327075662409\n",
      "epoch: 0 sentence: 25370/57013 loss: 5.214284035632232\n",
      "epoch: 0 sentence: 25380/57013 loss: 4.671026114252101\n",
      "epoch: 0 sentence: 25390/57013 loss: 4.8572131362249005\n",
      "epoch: 0 sentence: 25400/57013 loss: 4.53380183685325\n",
      "epoch: 0 sentence: 25410/57013 loss: 4.8078730196442185\n",
      "epoch: 0 sentence: 25420/57013 loss: 5.207033652462074\n",
      "epoch: 0 sentence: 25430/57013 loss: 4.525737741878192\n",
      "epoch: 0 sentence: 25440/57013 loss: 4.779266853393769\n",
      "epoch: 0 sentence: 25450/57013 loss: 4.572551139670368\n",
      "epoch: 0 sentence: 25460/57013 loss: 4.261503966667985\n",
      "epoch: 0 sentence: 25470/57013 loss: 4.795344179017078\n",
      "epoch: 0 sentence: 25480/57013 loss: 4.17162372055294\n",
      "epoch: 0 sentence: 25490/57013 loss: 3.370509054550061\n",
      "epoch: 0 sentence: 25500/57013 loss: 4.163158263346962\n",
      "epoch: 0 sentence: 25510/57013 loss: 4.448236517378588\n",
      "epoch: 0 sentence: 25520/57013 loss: 5.4938755919415625\n",
      "epoch: 0 sentence: 25530/57013 loss: 3.4289028263390837\n",
      "epoch: 0 sentence: 25540/57013 loss: 4.890384358881925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 25550/57013 loss: 4.840755997450914\n",
      "epoch: 0 sentence: 25560/57013 loss: 4.820792203086311\n",
      "epoch: 0 sentence: 25570/57013 loss: 3.981988929504934\n",
      "epoch: 0 sentence: 25580/57013 loss: 5.383210820073203\n",
      "epoch: 0 sentence: 25590/57013 loss: 5.151426800686247\n",
      "epoch: 0 sentence: 25600/57013 loss: 3.524642584941651\n",
      "epoch: 0 sentence: 25610/57013 loss: 3.6041877631916677\n",
      "epoch: 0 sentence: 25620/57013 loss: 4.386865417549727\n",
      "epoch: 0 sentence: 25630/57013 loss: 4.501699184088402\n",
      "epoch: 0 sentence: 25640/57013 loss: 3.0986819565910437\n",
      "epoch: 0 sentence: 25650/57013 loss: 5.522406431575884\n",
      "epoch: 0 sentence: 25660/57013 loss: 5.550055456340041\n",
      "epoch: 0 sentence: 25670/57013 loss: 5.465117327048474\n",
      "epoch: 0 sentence: 25680/57013 loss: 5.721005528934215\n",
      "epoch: 0 sentence: 25690/57013 loss: 5.282874498609375\n",
      "epoch: 0 sentence: 25700/57013 loss: 5.913943328218397\n",
      "epoch: 0 sentence: 25710/57013 loss: 4.601548561470119\n",
      "epoch: 0 sentence: 25720/57013 loss: 4.114324477780434\n",
      "epoch: 0 sentence: 25730/57013 loss: 4.355929057521198\n",
      "epoch: 0 sentence: 25740/57013 loss: 3.8810896873510825\n",
      "epoch: 0 sentence: 25750/57013 loss: 3.649861553170156\n",
      "epoch: 0 sentence: 25760/57013 loss: 5.639967905634077\n",
      "epoch: 0 sentence: 25770/57013 loss: 3.6212964373107317\n",
      "epoch: 0 sentence: 25780/57013 loss: 4.376543923187766\n",
      "epoch: 0 sentence: 25790/57013 loss: 1.452374686189115\n",
      "epoch: 0 sentence: 25800/57013 loss: 4.67606272044052\n",
      "epoch: 0 sentence: 25810/57013 loss: 4.502679814611942\n",
      "epoch: 0 sentence: 25820/57013 loss: 4.160720496258386\n",
      "epoch: 0 sentence: 25830/57013 loss: 3.917087257388422\n",
      "epoch: 0 sentence: 25840/57013 loss: 5.630797788953581\n",
      "epoch: 0 sentence: 25850/57013 loss: 4.251587359303748\n",
      "epoch: 0 sentence: 25860/57013 loss: 4.38821349117522\n",
      "epoch: 0 sentence: 25870/57013 loss: 5.117862911019033\n",
      "epoch: 0 sentence: 25880/57013 loss: 5.017752158117552\n",
      "epoch: 0 sentence: 25890/57013 loss: 4.37276951483482\n",
      "epoch: 0 sentence: 25900/57013 loss: 2.885070409888707\n",
      "epoch: 0 sentence: 25910/57013 loss: 5.3693835689718705\n",
      "epoch: 0 sentence: 25920/57013 loss: 3.5843755449200767\n",
      "epoch: 0 sentence: 25930/57013 loss: 1.9409843539526035\n",
      "epoch: 0 sentence: 25940/57013 loss: 4.710164893924335\n",
      "epoch: 0 sentence: 25950/57013 loss: 1.8272552948494236\n",
      "epoch: 0 sentence: 25960/57013 loss: 4.904288060457119\n",
      "epoch: 0 sentence: 25970/57013 loss: 5.429682106350629\n",
      "epoch: 0 sentence: 25980/57013 loss: 5.399877666731536\n",
      "epoch: 0 sentence: 25990/57013 loss: 5.104696247047597\n",
      "epoch: 0 sentence: 26000/57013 loss: 5.403870014397332\n",
      "epoch: 0 sentence: 26010/57013 loss: 2.2860392140932615\n",
      "epoch: 0 sentence: 26020/57013 loss: 3.389536470153012\n",
      "epoch: 0 sentence: 26030/57013 loss: 3.906766011082132\n",
      "epoch: 0 sentence: 26040/57013 loss: 3.1706620720540886\n",
      "epoch: 0 sentence: 26050/57013 loss: 5.076365913774452\n",
      "epoch: 0 sentence: 26060/57013 loss: 4.435269375110522\n",
      "epoch: 0 sentence: 26070/57013 loss: 4.191054778256765\n",
      "epoch: 0 sentence: 26080/57013 loss: 4.074795590212083\n",
      "epoch: 0 sentence: 26090/57013 loss: 2.93312790264108\n",
      "epoch: 0 sentence: 26100/57013 loss: 4.319302842333753\n",
      "epoch: 0 sentence: 26110/57013 loss: 4.502719225430232\n",
      "epoch: 0 sentence: 26120/57013 loss: 5.276998625386801\n",
      "epoch: 0 sentence: 26130/57013 loss: 4.679156795250862\n",
      "epoch: 0 sentence: 26140/57013 loss: 3.5114464120337376\n",
      "epoch: 0 sentence: 26150/57013 loss: 3.3482051796170538\n",
      "epoch: 0 sentence: 26160/57013 loss: 4.3784192331732825\n",
      "epoch: 0 sentence: 26170/57013 loss: 6.287215836797519\n",
      "epoch: 0 sentence: 26180/57013 loss: 4.013510009076425\n",
      "epoch: 0 sentence: 26190/57013 loss: 5.8444489500340415\n",
      "epoch: 0 sentence: 26200/57013 loss: 3.6223234972624345\n",
      "epoch: 0 sentence: 26210/57013 loss: 4.770804390216017\n",
      "epoch: 0 sentence: 26220/57013 loss: 4.4489060464729695\n",
      "epoch: 0 sentence: 26230/57013 loss: 5.24813589297206\n",
      "epoch: 0 sentence: 26240/57013 loss: 4.968137961733658\n",
      "epoch: 0 sentence: 26250/57013 loss: 4.986318281723689\n",
      "epoch: 0 sentence: 26260/57013 loss: 4.751122607875546\n",
      "epoch: 0 sentence: 26270/57013 loss: 5.285451359974594\n",
      "epoch: 0 sentence: 26280/57013 loss: 4.35239183040185\n",
      "epoch: 0 sentence: 26290/57013 loss: 4.8716660813465325\n",
      "epoch: 0 sentence: 26300/57013 loss: 5.515058271766011\n",
      "epoch: 0 sentence: 26310/57013 loss: 4.241495399863384\n",
      "epoch: 0 sentence: 26320/57013 loss: 5.402349314598665\n",
      "epoch: 0 sentence: 26330/57013 loss: 4.512430811374354\n",
      "epoch: 0 sentence: 26340/57013 loss: 4.88134675983662\n",
      "epoch: 0 sentence: 26350/57013 loss: 4.061135059914016\n",
      "epoch: 0 sentence: 26360/57013 loss: 5.509556821010423\n",
      "epoch: 0 sentence: 26370/57013 loss: 6.19732455952146\n",
      "epoch: 0 sentence: 26380/57013 loss: 3.5216883821510936\n",
      "epoch: 0 sentence: 26390/57013 loss: 3.931292298443193\n",
      "epoch: 0 sentence: 26400/57013 loss: 5.702015028111287\n",
      "epoch: 0 sentence: 26410/57013 loss: 4.586568428101902\n",
      "epoch: 0 sentence: 26420/57013 loss: 4.720880628445587\n",
      "epoch: 0 sentence: 26430/57013 loss: 2.431950349206469\n",
      "epoch: 0 sentence: 26440/57013 loss: 4.145361332930288\n",
      "epoch: 0 sentence: 26450/57013 loss: 4.65522314348456\n",
      "epoch: 0 sentence: 26460/57013 loss: 3.5899862875087036\n",
      "epoch: 0 sentence: 26470/57013 loss: 5.284281889129493\n",
      "epoch: 0 sentence: 26480/57013 loss: 3.9042938742272244\n",
      "epoch: 0 sentence: 26490/57013 loss: 3.048342497393575\n",
      "epoch: 0 sentence: 26500/57013 loss: 4.0636032034657275\n",
      "epoch: 0 sentence: 26510/57013 loss: 4.310730887992964\n",
      "epoch: 0 sentence: 26520/57013 loss: 4.761128710144649\n",
      "epoch: 0 sentence: 26530/57013 loss: 4.380460033564376\n",
      "epoch: 0 sentence: 26540/57013 loss: 3.9182939729574704\n",
      "epoch: 0 sentence: 26550/57013 loss: 5.020223573981574\n",
      "epoch: 0 sentence: 26560/57013 loss: 6.046481992852252\n",
      "epoch: 0 sentence: 26570/57013 loss: 5.3796502266106865\n",
      "epoch: 0 sentence: 26580/57013 loss: 4.853528227137885\n",
      "epoch: 0 sentence: 26590/57013 loss: 4.83359484516155\n",
      "epoch: 0 sentence: 26600/57013 loss: 4.621505982182827\n",
      "epoch: 0 sentence: 26610/57013 loss: 3.917199009523938\n",
      "epoch: 0 sentence: 26620/57013 loss: 4.9904832668586385\n",
      "epoch: 0 sentence: 26630/57013 loss: 3.5904327618512464\n",
      "epoch: 0 sentence: 26640/57013 loss: 6.345273752653771\n",
      "epoch: 0 sentence: 26650/57013 loss: 4.438703596618516\n",
      "epoch: 0 sentence: 26660/57013 loss: 5.5113765846815905\n",
      "epoch: 0 sentence: 26670/57013 loss: 5.246870915881163\n",
      "epoch: 0 sentence: 26680/57013 loss: 4.161514920408145\n",
      "epoch: 0 sentence: 26690/57013 loss: 6.192495736485767\n",
      "epoch: 0 sentence: 26700/57013 loss: 1.7967091847315702\n",
      "epoch: 0 sentence: 26710/57013 loss: 3.5504756537712328\n",
      "epoch: 0 sentence: 26720/57013 loss: 4.356548902155042\n",
      "epoch: 0 sentence: 26730/57013 loss: 3.478914445559044\n",
      "epoch: 0 sentence: 26740/57013 loss: 3.521856231981726\n",
      "epoch: 0 sentence: 26750/57013 loss: 3.5792652754845125\n",
      "epoch: 0 sentence: 26760/57013 loss: 5.043198086948986\n",
      "epoch: 0 sentence: 26770/57013 loss: 4.457612815391163\n",
      "epoch: 0 sentence: 26780/57013 loss: 5.026470705264285\n",
      "epoch: 0 sentence: 26790/57013 loss: 5.396370169109299\n",
      "epoch: 0 sentence: 26800/57013 loss: 3.9210507717513363\n",
      "epoch: 0 sentence: 26810/57013 loss: 4.25492029702879\n",
      "epoch: 0 sentence: 26820/57013 loss: 3.672026367238473\n",
      "epoch: 0 sentence: 26830/57013 loss: 5.392690811798368\n",
      "epoch: 0 sentence: 26840/57013 loss: 2.2633616673610706\n",
      "epoch: 0 sentence: 26850/57013 loss: 6.026510038738268\n",
      "epoch: 0 sentence: 26860/57013 loss: 5.510223823454017\n",
      "epoch: 0 sentence: 26870/57013 loss: 5.4487222941882925\n",
      "epoch: 0 sentence: 26880/57013 loss: 5.435938629731009\n",
      "epoch: 0 sentence: 26890/57013 loss: 3.8424007818915524\n",
      "epoch: 0 sentence: 26900/57013 loss: 3.955398921455874\n",
      "epoch: 0 sentence: 26910/57013 loss: 3.6219098106913727\n",
      "epoch: 0 sentence: 26920/57013 loss: 3.574881436212337\n",
      "epoch: 0 sentence: 26930/57013 loss: 4.213330174252716\n",
      "epoch: 0 sentence: 26940/57013 loss: 4.714454008211454\n",
      "epoch: 0 sentence: 26950/57013 loss: 4.323130960826912\n",
      "epoch: 0 sentence: 26960/57013 loss: 5.460755125129608\n",
      "epoch: 0 sentence: 26970/57013 loss: 4.230736363134306\n",
      "epoch: 0 sentence: 26980/57013 loss: 4.9638856556979585\n",
      "epoch: 0 sentence: 26990/57013 loss: 5.619388113921849\n",
      "epoch: 0 sentence: 27000/57013 loss: 4.965306514874083\n",
      "epoch: 0 sentence: 27010/57013 loss: 4.860626819052078\n",
      "epoch: 0 sentence: 27020/57013 loss: 5.6058062736934415\n",
      "epoch: 0 sentence: 27030/57013 loss: 3.995181383541059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 27040/57013 loss: 4.365221858853904\n",
      "epoch: 0 sentence: 27050/57013 loss: 4.441008965348569\n",
      "epoch: 0 sentence: 27060/57013 loss: 4.6549602420826695\n",
      "epoch: 0 sentence: 27070/57013 loss: 3.7933616414647933\n",
      "epoch: 0 sentence: 27080/57013 loss: 5.462500161187411\n",
      "epoch: 0 sentence: 27090/57013 loss: 4.962199591078619\n",
      "epoch: 0 sentence: 27100/57013 loss: 4.29011322594732\n",
      "epoch: 0 sentence: 27110/57013 loss: 5.7074942206368915\n",
      "epoch: 0 sentence: 27120/57013 loss: 3.9279165841548425\n",
      "epoch: 0 sentence: 27130/57013 loss: 4.066416519287894\n",
      "epoch: 0 sentence: 27140/57013 loss: 3.997672538009598\n",
      "epoch: 0 sentence: 27150/57013 loss: 4.169683955570817\n",
      "epoch: 0 sentence: 27160/57013 loss: 4.963118476587144\n",
      "epoch: 0 sentence: 27170/57013 loss: 5.554841922579511\n",
      "epoch: 0 sentence: 27180/57013 loss: 4.014951905758103\n",
      "epoch: 0 sentence: 27190/57013 loss: 3.1001467062366923\n",
      "epoch: 0 sentence: 27200/57013 loss: 3.8053040484280385\n",
      "epoch: 0 sentence: 27210/57013 loss: 4.5534802350137085\n",
      "epoch: 0 sentence: 27220/57013 loss: 5.428941959979024\n",
      "epoch: 0 sentence: 27230/57013 loss: 5.972856913859988\n",
      "epoch: 0 sentence: 27240/57013 loss: 4.731592855693487\n",
      "epoch: 0 sentence: 27250/57013 loss: 4.367334655727247\n",
      "epoch: 0 sentence: 27260/57013 loss: 4.747290499722766\n",
      "epoch: 0 sentence: 27270/57013 loss: 5.071894514751904\n",
      "epoch: 0 sentence: 27280/57013 loss: 4.866547569205006\n",
      "epoch: 0 sentence: 27290/57013 loss: 2.2428097280135417\n",
      "epoch: 0 sentence: 27300/57013 loss: 4.942330157582795\n",
      "epoch: 0 sentence: 27310/57013 loss: 5.247295899664912\n",
      "epoch: 0 sentence: 27320/57013 loss: 5.22895746109739\n",
      "epoch: 0 sentence: 27330/57013 loss: 4.435767021502832\n",
      "epoch: 0 sentence: 27340/57013 loss: 5.0015308353876895\n",
      "epoch: 0 sentence: 27350/57013 loss: 4.999816440651566\n",
      "epoch: 0 sentence: 27360/57013 loss: 5.31776707194956\n",
      "epoch: 0 sentence: 27370/57013 loss: 4.3298162364525545\n",
      "epoch: 0 sentence: 27380/57013 loss: 6.108099288809258\n",
      "epoch: 0 sentence: 27390/57013 loss: 4.652243979387913\n",
      "epoch: 0 sentence: 27400/57013 loss: 5.723710297711338\n",
      "epoch: 0 sentence: 27410/57013 loss: 3.8667009921438726\n",
      "epoch: 0 sentence: 27420/57013 loss: 3.255721187310298\n",
      "epoch: 0 sentence: 27430/57013 loss: 4.031904289966517\n",
      "epoch: 0 sentence: 27440/57013 loss: 5.090544386723349\n",
      "epoch: 0 sentence: 27450/57013 loss: 5.896430624088178\n",
      "epoch: 0 sentence: 27460/57013 loss: 3.476305006363748\n",
      "epoch: 0 sentence: 27470/57013 loss: 5.415599224443589\n",
      "epoch: 0 sentence: 27480/57013 loss: 5.126533819247332\n",
      "epoch: 0 sentence: 27490/57013 loss: 4.724216618870179\n",
      "epoch: 0 sentence: 27500/57013 loss: 2.608387985776485\n",
      "epoch: 0 sentence: 27510/57013 loss: 5.389728682666248\n",
      "epoch: 0 sentence: 27520/57013 loss: 5.503268284103082\n",
      "epoch: 0 sentence: 27530/57013 loss: 4.543146874122431\n",
      "epoch: 0 sentence: 27540/57013 loss: 4.255401214158405\n",
      "epoch: 0 sentence: 27550/57013 loss: 6.195999348110578\n",
      "epoch: 0 sentence: 27560/57013 loss: 4.293019423264083\n",
      "epoch: 0 sentence: 27570/57013 loss: 3.9605855684621942\n",
      "epoch: 0 sentence: 27580/57013 loss: 4.519601899140552\n",
      "epoch: 0 sentence: 27590/57013 loss: 4.254997045439419\n",
      "epoch: 0 sentence: 27600/57013 loss: 6.171227516958019\n",
      "epoch: 0 sentence: 27610/57013 loss: 3.9795076295626535\n",
      "epoch: 0 sentence: 27620/57013 loss: 3.792232149397923\n",
      "epoch: 0 sentence: 27630/57013 loss: 5.33114635207467\n",
      "epoch: 0 sentence: 27640/57013 loss: 3.814680358414383\n",
      "epoch: 0 sentence: 27650/57013 loss: 4.974807556786352\n",
      "epoch: 0 sentence: 27660/57013 loss: 5.217572164523599\n",
      "epoch: 0 sentence: 27670/57013 loss: 4.324584389807262\n",
      "epoch: 0 sentence: 27680/57013 loss: 3.436658482362082\n",
      "epoch: 0 sentence: 27690/57013 loss: 4.335350543948225\n",
      "epoch: 0 sentence: 27700/57013 loss: 3.782723020555881\n",
      "epoch: 0 sentence: 27710/57013 loss: 4.448688779296817\n",
      "epoch: 0 sentence: 27720/57013 loss: 4.857548884570323\n",
      "epoch: 0 sentence: 27730/57013 loss: 4.411898497971065\n",
      "epoch: 0 sentence: 27740/57013 loss: 6.175330059679998\n",
      "epoch: 0 sentence: 27750/57013 loss: 2.910772111687548\n",
      "epoch: 0 sentence: 27760/57013 loss: 4.020654054510071\n",
      "epoch: 0 sentence: 27770/57013 loss: 5.419140214102058\n",
      "epoch: 0 sentence: 27780/57013 loss: 3.859859324881912\n",
      "epoch: 0 sentence: 27790/57013 loss: 3.2484944231082133\n",
      "epoch: 0 sentence: 27800/57013 loss: 6.034499042481245\n",
      "epoch: 0 sentence: 27810/57013 loss: 3.5490904513654273\n",
      "epoch: 0 sentence: 27820/57013 loss: 5.255498069538448\n",
      "epoch: 0 sentence: 27830/57013 loss: 4.435046251179414\n",
      "epoch: 0 sentence: 27840/57013 loss: 2.772658601227068\n",
      "epoch: 0 sentence: 27850/57013 loss: 5.258670429348388\n",
      "epoch: 0 sentence: 27860/57013 loss: 5.670819215923277\n",
      "epoch: 0 sentence: 27870/57013 loss: 4.456629514691571\n",
      "epoch: 0 sentence: 27880/57013 loss: 4.5345463027065565\n",
      "epoch: 0 sentence: 27890/57013 loss: 1.5170167348965735\n",
      "epoch: 0 sentence: 27900/57013 loss: 4.329453571426631\n",
      "epoch: 0 sentence: 27910/57013 loss: 5.624519763077813\n",
      "epoch: 0 sentence: 27920/57013 loss: 5.498417030251901\n",
      "epoch: 0 sentence: 27930/57013 loss: 3.376800998324051\n",
      "epoch: 0 sentence: 27940/57013 loss: 4.238595559036459\n",
      "epoch: 0 sentence: 27950/57013 loss: 4.675394078422089\n",
      "epoch: 0 sentence: 27960/57013 loss: 4.768997839532114\n",
      "epoch: 0 sentence: 27970/57013 loss: 5.384343046145149\n",
      "epoch: 0 sentence: 27980/57013 loss: 5.523405992925706\n",
      "epoch: 0 sentence: 27990/57013 loss: 2.8334727033801066\n",
      "epoch: 0 sentence: 28000/57013 loss: 4.869098603316767\n",
      "epoch: 0 sentence: 28010/57013 loss: 5.2032276430468745\n",
      "epoch: 0 sentence: 28020/57013 loss: 5.38465386226421\n",
      "epoch: 0 sentence: 28030/57013 loss: 3.8694993697098554\n",
      "epoch: 0 sentence: 28040/57013 loss: 4.3357435552446635\n",
      "epoch: 0 sentence: 28050/57013 loss: 4.944384245355801\n",
      "epoch: 0 sentence: 28060/57013 loss: 3.9562763396831637\n",
      "epoch: 0 sentence: 28070/57013 loss: 6.185979432348726\n",
      "epoch: 0 sentence: 28080/57013 loss: 4.988499120608662\n",
      "epoch: 0 sentence: 28090/57013 loss: 6.175300207824097\n",
      "epoch: 0 sentence: 28100/57013 loss: 6.2576847075369635\n",
      "epoch: 0 sentence: 28110/57013 loss: 4.8666873362018315\n",
      "epoch: 0 sentence: 28120/57013 loss: 5.068610758365313\n",
      "epoch: 0 sentence: 28130/57013 loss: 4.44159125116362\n",
      "epoch: 0 sentence: 28140/57013 loss: 5.453019938237516\n",
      "epoch: 0 sentence: 28150/57013 loss: 4.2284660705457595\n",
      "epoch: 0 sentence: 28160/57013 loss: 4.861539662575525\n",
      "epoch: 0 sentence: 28170/57013 loss: 5.025102879485335\n",
      "epoch: 0 sentence: 28180/57013 loss: 5.190613563482913\n",
      "epoch: 0 sentence: 28190/57013 loss: 3.0451127669159606\n",
      "epoch: 0 sentence: 28200/57013 loss: 4.787364220882796\n",
      "epoch: 0 sentence: 28210/57013 loss: 3.6500078277258496\n",
      "epoch: 0 sentence: 28220/57013 loss: 4.651869561451724\n",
      "epoch: 0 sentence: 28230/57013 loss: 3.6423456383998682\n",
      "epoch: 0 sentence: 28240/57013 loss: 4.2856769198199585\n",
      "epoch: 0 sentence: 28250/57013 loss: 5.769581068956496\n",
      "epoch: 0 sentence: 28260/57013 loss: 4.975222185656598\n",
      "epoch: 0 sentence: 28270/57013 loss: 4.784057315922629\n",
      "epoch: 0 sentence: 28280/57013 loss: 5.334169736235935\n",
      "epoch: 0 sentence: 28290/57013 loss: 5.311762282154568\n",
      "epoch: 0 sentence: 28300/57013 loss: 5.177284622955388\n",
      "epoch: 0 sentence: 28310/57013 loss: 2.1604301471444676\n",
      "epoch: 0 sentence: 28320/57013 loss: 5.5430468015397905\n",
      "epoch: 0 sentence: 28330/57013 loss: 4.307464533195854\n",
      "epoch: 0 sentence: 28340/57013 loss: 3.882106163761307\n",
      "epoch: 0 sentence: 28350/57013 loss: 5.510056418146731\n",
      "epoch: 0 sentence: 28360/57013 loss: 5.730147990827482\n",
      "epoch: 0 sentence: 28370/57013 loss: 4.918443335386822\n",
      "epoch: 0 sentence: 28380/57013 loss: 4.7494768134709675\n",
      "epoch: 0 sentence: 28390/57013 loss: 4.321308492499171\n",
      "epoch: 0 sentence: 28400/57013 loss: 5.7634021905099635\n",
      "epoch: 0 sentence: 28410/57013 loss: 4.360944643158646\n",
      "epoch: 0 sentence: 28420/57013 loss: 4.5207860580241235\n",
      "epoch: 0 sentence: 28430/57013 loss: 5.476803365562302\n",
      "epoch: 0 sentence: 28440/57013 loss: 5.538741632606006\n",
      "epoch: 0 sentence: 28450/57013 loss: 3.129258010999467\n",
      "epoch: 0 sentence: 28460/57013 loss: 3.3830998528135767\n",
      "epoch: 0 sentence: 28470/57013 loss: 5.152852618805208\n",
      "epoch: 0 sentence: 28480/57013 loss: 4.351905520602812\n",
      "epoch: 0 sentence: 28490/57013 loss: 3.3881941691014275\n",
      "epoch: 0 sentence: 28500/57013 loss: 5.023474180413678\n",
      "epoch: 0 sentence: 28510/57013 loss: 4.999082343435537\n",
      "epoch: 0 sentence: 28520/57013 loss: 1.8680319825346259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 28530/57013 loss: 5.270697397019406\n",
      "epoch: 0 sentence: 28540/57013 loss: 5.711719064749015\n",
      "epoch: 0 sentence: 28550/57013 loss: 4.793659868964032\n",
      "epoch: 0 sentence: 28560/57013 loss: 5.060391718358873\n",
      "epoch: 0 sentence: 28570/57013 loss: 5.365643130223507\n",
      "epoch: 0 sentence: 28580/57013 loss: 3.1808115028105255\n",
      "epoch: 0 sentence: 28590/57013 loss: 3.0964271533119456\n",
      "epoch: 0 sentence: 28600/57013 loss: 6.118401721761321\n",
      "epoch: 0 sentence: 28610/57013 loss: 5.437399108147032\n",
      "epoch: 0 sentence: 28620/57013 loss: 4.730116316777237\n",
      "epoch: 0 sentence: 28630/57013 loss: 5.430437549472785\n",
      "epoch: 0 sentence: 28640/57013 loss: 4.7238599041454945\n",
      "epoch: 0 sentence: 28650/57013 loss: 5.9108214169769004\n",
      "epoch: 0 sentence: 28660/57013 loss: 4.6120709980137935\n",
      "epoch: 0 sentence: 28670/57013 loss: 6.681294356177981\n",
      "epoch: 0 sentence: 28680/57013 loss: 2.8700697341658725\n",
      "epoch: 0 sentence: 28690/57013 loss: 4.954023202354159\n",
      "epoch: 0 sentence: 28700/57013 loss: 5.222519410649871\n",
      "epoch: 0 sentence: 28710/57013 loss: 4.185247303357213\n",
      "epoch: 0 sentence: 28720/57013 loss: 4.162315858469015\n",
      "epoch: 0 sentence: 28730/57013 loss: 5.44731279306649\n",
      "epoch: 0 sentence: 28740/57013 loss: 5.301363231570807\n",
      "epoch: 0 sentence: 28750/57013 loss: 4.125344424856077\n",
      "epoch: 0 sentence: 28760/57013 loss: 5.2044973179431455\n",
      "epoch: 0 sentence: 28770/57013 loss: 4.427985843261827\n",
      "epoch: 0 sentence: 28780/57013 loss: 4.950904795606617\n",
      "epoch: 0 sentence: 28790/57013 loss: 5.498274763252905\n",
      "epoch: 0 sentence: 28800/57013 loss: 4.095212508245679\n",
      "epoch: 0 sentence: 28810/57013 loss: 3.86429019547579\n",
      "epoch: 0 sentence: 28820/57013 loss: 4.426510216915992\n",
      "epoch: 0 sentence: 28830/57013 loss: 4.344144084314369\n",
      "epoch: 0 sentence: 28840/57013 loss: 5.037699055491469\n",
      "epoch: 0 sentence: 28850/57013 loss: 5.341739449051861\n",
      "epoch: 0 sentence: 28860/57013 loss: 4.645273126916977\n",
      "epoch: 0 sentence: 28870/57013 loss: 4.957361908835846\n",
      "epoch: 0 sentence: 28880/57013 loss: 5.208921711165606\n",
      "epoch: 0 sentence: 28890/57013 loss: 5.498254012847596\n",
      "epoch: 0 sentence: 28900/57013 loss: 4.774345554846288\n",
      "epoch: 0 sentence: 28910/57013 loss: 5.467742474054345\n",
      "epoch: 0 sentence: 28920/57013 loss: 3.8670831999823134\n",
      "epoch: 0 sentence: 28930/57013 loss: 4.102929511677584\n",
      "epoch: 0 sentence: 28940/57013 loss: 3.7115218497092712\n",
      "epoch: 0 sentence: 28950/57013 loss: 5.017171659625029\n",
      "epoch: 0 sentence: 28960/57013 loss: 4.323515701380801\n",
      "epoch: 0 sentence: 28970/57013 loss: 4.995590879561529\n",
      "epoch: 0 sentence: 28980/57013 loss: 4.561749799304054\n",
      "epoch: 0 sentence: 28990/57013 loss: 4.735158258959099\n",
      "epoch: 0 sentence: 29000/57013 loss: 4.60020036706231\n",
      "epoch: 0 sentence: 29010/57013 loss: 4.798034231401651\n",
      "epoch: 0 sentence: 29020/57013 loss: 4.020217459494511\n",
      "epoch: 0 sentence: 29030/57013 loss: 5.161884490082411\n",
      "epoch: 0 sentence: 29040/57013 loss: 4.767899496246152\n",
      "epoch: 0 sentence: 29050/57013 loss: 4.5265409874704\n",
      "epoch: 0 sentence: 29060/57013 loss: 3.1248639246700494\n",
      "epoch: 0 sentence: 29070/57013 loss: 4.676695002552429\n",
      "epoch: 0 sentence: 29080/57013 loss: 5.559698627164579\n",
      "epoch: 0 sentence: 29090/57013 loss: 5.23861778867041\n",
      "epoch: 0 sentence: 29100/57013 loss: 4.786349903074465\n",
      "epoch: 0 sentence: 29110/57013 loss: 1.958399874538658\n",
      "epoch: 0 sentence: 29120/57013 loss: 3.7531674944966946\n",
      "epoch: 0 sentence: 29130/57013 loss: 5.054750968942412\n",
      "epoch: 0 sentence: 29140/57013 loss: 4.4801503688334\n",
      "epoch: 0 sentence: 29150/57013 loss: 3.863566028166832\n",
      "epoch: 0 sentence: 29160/57013 loss: 4.6941793039512385\n",
      "epoch: 0 sentence: 29170/57013 loss: 5.032985315761671\n",
      "epoch: 0 sentence: 29180/57013 loss: 5.238054227639365\n",
      "epoch: 0 sentence: 29190/57013 loss: 3.5391760802474597\n",
      "epoch: 0 sentence: 29200/57013 loss: 5.36948243581424\n",
      "epoch: 0 sentence: 29210/57013 loss: 5.4785802500284335\n",
      "epoch: 0 sentence: 29220/57013 loss: 5.255349213209098\n",
      "epoch: 0 sentence: 29230/57013 loss: 4.057978910264848\n",
      "epoch: 0 sentence: 29240/57013 loss: 3.8051889660579774\n",
      "epoch: 0 sentence: 29250/57013 loss: 4.262478470744628\n",
      "epoch: 0 sentence: 29260/57013 loss: 5.530596589828749\n",
      "epoch: 0 sentence: 29270/57013 loss: 5.542286512095346\n",
      "epoch: 0 sentence: 29280/57013 loss: 4.583645976398755\n",
      "epoch: 0 sentence: 29290/57013 loss: 6.265281809972896\n",
      "epoch: 0 sentence: 29300/57013 loss: 4.948226439072224\n",
      "epoch: 0 sentence: 29310/57013 loss: 6.02260887791146\n",
      "epoch: 0 sentence: 29320/57013 loss: 4.268583754120313\n",
      "epoch: 0 sentence: 29330/57013 loss: 4.430252693490082\n",
      "epoch: 0 sentence: 29340/57013 loss: 4.717021273340174\n",
      "epoch: 0 sentence: 29350/57013 loss: 4.297217624456135\n",
      "epoch: 0 sentence: 29360/57013 loss: 5.146765346613757\n",
      "epoch: 0 sentence: 29370/57013 loss: 4.624929066322965\n",
      "epoch: 0 sentence: 29380/57013 loss: 3.827285845048396\n",
      "epoch: 0 sentence: 29390/57013 loss: 4.25882773643463\n",
      "epoch: 0 sentence: 29400/57013 loss: 5.724602945944078\n",
      "epoch: 0 sentence: 29410/57013 loss: 5.41888724842064\n",
      "epoch: 0 sentence: 29420/57013 loss: 5.1269351020631815\n",
      "epoch: 0 sentence: 29430/57013 loss: 4.261437533453002\n",
      "epoch: 0 sentence: 29440/57013 loss: 5.347705706156306\n",
      "epoch: 0 sentence: 29450/57013 loss: 5.386175458797208\n",
      "epoch: 0 sentence: 29460/57013 loss: 4.944098662722603\n",
      "epoch: 0 sentence: 29470/57013 loss: 4.346026532933626\n",
      "epoch: 0 sentence: 29480/57013 loss: 5.501457277485694\n",
      "epoch: 0 sentence: 29490/57013 loss: 4.349756711552304\n",
      "epoch: 0 sentence: 29500/57013 loss: 3.574073587893744\n",
      "epoch: 0 sentence: 29510/57013 loss: 4.772524410237078\n",
      "epoch: 0 sentence: 29520/57013 loss: 4.178022587232294\n",
      "epoch: 0 sentence: 29530/57013 loss: 3.619794807701152\n",
      "epoch: 0 sentence: 29540/57013 loss: 4.2122054813034095\n",
      "epoch: 0 sentence: 29550/57013 loss: 4.208724628955042\n",
      "epoch: 0 sentence: 29560/57013 loss: 4.271127585670654\n",
      "epoch: 0 sentence: 29570/57013 loss: 3.891641041697806\n",
      "epoch: 0 sentence: 29580/57013 loss: 3.6602068903654077\n",
      "epoch: 0 sentence: 29590/57013 loss: 5.062096277960938\n",
      "epoch: 0 sentence: 29600/57013 loss: 4.074168348713299\n",
      "epoch: 0 sentence: 29610/57013 loss: 5.29725176248776\n",
      "epoch: 0 sentence: 29620/57013 loss: 4.665303586671138\n",
      "epoch: 0 sentence: 29630/57013 loss: 4.980278737242396\n",
      "epoch: 0 sentence: 29640/57013 loss: 4.218094891196199\n",
      "epoch: 0 sentence: 29650/57013 loss: 7.366280339855784\n",
      "epoch: 0 sentence: 29660/57013 loss: 4.734339898052092\n",
      "epoch: 0 sentence: 29670/57013 loss: 3.3855296272543556\n",
      "epoch: 0 sentence: 29680/57013 loss: 4.640048985996001\n",
      "epoch: 0 sentence: 29690/57013 loss: 4.772216533014593\n",
      "epoch: 0 sentence: 29700/57013 loss: 4.830490193498824\n",
      "epoch: 0 sentence: 29710/57013 loss: 6.45930823569559\n",
      "epoch: 0 sentence: 29720/57013 loss: 5.529596287582547\n",
      "epoch: 0 sentence: 29730/57013 loss: 3.598704240992804\n",
      "epoch: 0 sentence: 29740/57013 loss: 4.120659877015907\n",
      "epoch: 0 sentence: 29750/57013 loss: 3.644012425250989\n",
      "epoch: 0 sentence: 29760/57013 loss: 4.533457644075441\n",
      "epoch: 0 sentence: 29770/57013 loss: 3.1919668698676382\n",
      "epoch: 0 sentence: 29780/57013 loss: 5.246487575334712\n",
      "epoch: 0 sentence: 29790/57013 loss: 4.421230146776474\n",
      "epoch: 0 sentence: 29800/57013 loss: 6.05432322271968\n",
      "epoch: 0 sentence: 29810/57013 loss: 5.15268270919271\n",
      "epoch: 0 sentence: 29820/57013 loss: 3.8430710900626286\n",
      "epoch: 0 sentence: 29830/57013 loss: 4.6502130315169135\n",
      "epoch: 0 sentence: 29840/57013 loss: 5.258487589604882\n",
      "epoch: 0 sentence: 29850/57013 loss: 5.72192970818458\n",
      "epoch: 0 sentence: 29860/57013 loss: 4.54748068262238\n",
      "epoch: 0 sentence: 29870/57013 loss: 4.419849618102197\n",
      "epoch: 0 sentence: 29880/57013 loss: 4.202673215254495\n",
      "epoch: 0 sentence: 29890/57013 loss: 4.438617401199151\n",
      "epoch: 0 sentence: 29900/57013 loss: 4.963316297060219\n",
      "epoch: 0 sentence: 29910/57013 loss: 4.184036227864051\n",
      "epoch: 0 sentence: 29920/57013 loss: 4.111337842008803\n",
      "epoch: 0 sentence: 29930/57013 loss: 5.1330150197404105\n",
      "epoch: 0 sentence: 29940/57013 loss: 4.157125252137263\n",
      "epoch: 0 sentence: 29950/57013 loss: 5.403255235946736\n",
      "epoch: 0 sentence: 29960/57013 loss: 4.436377050143244\n",
      "epoch: 0 sentence: 29970/57013 loss: 4.51638796802503\n",
      "epoch: 0 sentence: 29980/57013 loss: 4.960435818038358\n",
      "epoch: 0 sentence: 29990/57013 loss: 5.170157432770135\n",
      "epoch: 0 sentence: 30000/57013 loss: 5.4894172568271555\n",
      "epoch: 0 sentence: 30010/57013 loss: 4.395537367222787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 30020/57013 loss: 4.149735420036592\n",
      "epoch: 0 sentence: 30030/57013 loss: 4.993042344725915\n",
      "epoch: 0 sentence: 30040/57013 loss: 4.119273549200322\n",
      "epoch: 0 sentence: 30050/57013 loss: 5.974405070922482\n",
      "epoch: 0 sentence: 30060/57013 loss: 4.918848015552232\n",
      "epoch: 0 sentence: 30070/57013 loss: 4.489573922906576\n",
      "epoch: 0 sentence: 30080/57013 loss: 5.484076549206752\n",
      "epoch: 0 sentence: 30090/57013 loss: 5.878555731967285\n",
      "epoch: 0 sentence: 30100/57013 loss: 4.217868005785709\n",
      "epoch: 0 sentence: 30110/57013 loss: 5.12236013939481\n",
      "epoch: 0 sentence: 30120/57013 loss: 3.8218348579088737\n",
      "epoch: 0 sentence: 30130/57013 loss: 3.873966131282926\n",
      "epoch: 0 sentence: 30140/57013 loss: 5.529120534140246\n",
      "epoch: 0 sentence: 30150/57013 loss: 5.6617156932289125\n",
      "epoch: 0 sentence: 30160/57013 loss: 4.7836216924940524\n",
      "epoch: 0 sentence: 30170/57013 loss: 3.8479394788215693\n",
      "epoch: 0 sentence: 30180/57013 loss: 4.398422193708643\n",
      "epoch: 0 sentence: 30190/57013 loss: 5.376137969992199\n",
      "epoch: 0 sentence: 30200/57013 loss: 1.6956419341486368\n",
      "epoch: 0 sentence: 30210/57013 loss: 5.3222639856444784\n",
      "epoch: 0 sentence: 30220/57013 loss: 2.6289850314510987\n",
      "epoch: 0 sentence: 30230/57013 loss: 5.805146890664177\n",
      "epoch: 0 sentence: 30240/57013 loss: 4.982889070477331\n",
      "epoch: 0 sentence: 30250/57013 loss: 3.790934055810353\n",
      "epoch: 0 sentence: 30260/57013 loss: 4.787723574960923\n",
      "epoch: 0 sentence: 30270/57013 loss: 3.380984299370175\n",
      "epoch: 0 sentence: 30280/57013 loss: 4.101633029417106\n",
      "epoch: 0 sentence: 30290/57013 loss: 5.298372177812777\n",
      "epoch: 0 sentence: 30300/57013 loss: 5.818994817685537\n",
      "epoch: 0 sentence: 30310/57013 loss: 4.214267548363024\n",
      "epoch: 0 sentence: 30320/57013 loss: 4.783724992729258\n",
      "epoch: 0 sentence: 30330/57013 loss: 4.533506490147754\n",
      "epoch: 0 sentence: 30340/57013 loss: 3.2297668571683698\n",
      "epoch: 0 sentence: 30350/57013 loss: 5.586467350266367\n",
      "epoch: 0 sentence: 30360/57013 loss: 4.95083633549358\n",
      "epoch: 0 sentence: 30370/57013 loss: 5.053688247128064\n",
      "epoch: 0 sentence: 30380/57013 loss: 3.8606541966207932\n",
      "epoch: 0 sentence: 30390/57013 loss: 4.62857975347379\n",
      "epoch: 0 sentence: 30400/57013 loss: 3.099002104883969\n",
      "epoch: 0 sentence: 30410/57013 loss: 2.9483647312823904\n",
      "epoch: 0 sentence: 30420/57013 loss: 4.809490161267859\n",
      "epoch: 0 sentence: 30430/57013 loss: 3.705989975225966\n",
      "epoch: 0 sentence: 30440/57013 loss: 4.956918248548483\n",
      "epoch: 0 sentence: 30450/57013 loss: 3.598117537940332\n",
      "epoch: 0 sentence: 30460/57013 loss: 4.871953482245827\n",
      "epoch: 0 sentence: 30470/57013 loss: 5.2807885278721445\n",
      "epoch: 0 sentence: 30480/57013 loss: 6.070149826091787\n",
      "epoch: 0 sentence: 30490/57013 loss: 4.4961327603169945\n",
      "epoch: 0 sentence: 30500/57013 loss: 4.788036259217543\n",
      "epoch: 0 sentence: 30510/57013 loss: 4.956234792607724\n",
      "epoch: 0 sentence: 30520/57013 loss: 5.323692518165388\n",
      "epoch: 0 sentence: 30530/57013 loss: 5.556431849485045\n",
      "epoch: 0 sentence: 30540/57013 loss: 3.9678551163536637\n",
      "epoch: 0 sentence: 30550/57013 loss: 5.895021338536251\n",
      "epoch: 0 sentence: 30560/57013 loss: 3.793116961459272\n",
      "epoch: 0 sentence: 30570/57013 loss: 4.7483011326983995\n",
      "epoch: 0 sentence: 30580/57013 loss: 6.043303796496701\n",
      "epoch: 0 sentence: 30590/57013 loss: 4.050166741884724\n",
      "epoch: 0 sentence: 30600/57013 loss: 4.48705976363751\n",
      "epoch: 0 sentence: 30610/57013 loss: 5.00264557974293\n",
      "epoch: 0 sentence: 30620/57013 loss: 5.848827387687061\n",
      "epoch: 0 sentence: 30630/57013 loss: 4.190035831508528\n",
      "epoch: 0 sentence: 30640/57013 loss: 3.5102775211266874\n",
      "epoch: 0 sentence: 30650/57013 loss: 4.668658543558558\n",
      "epoch: 0 sentence: 30660/57013 loss: 4.7165417680661115\n",
      "epoch: 0 sentence: 30670/57013 loss: 4.708135200012699\n",
      "epoch: 0 sentence: 30680/57013 loss: 3.8673264266219007\n",
      "epoch: 0 sentence: 30690/57013 loss: 5.149298907779964\n",
      "epoch: 0 sentence: 30700/57013 loss: 5.259713125193465\n",
      "epoch: 0 sentence: 30710/57013 loss: 4.157280123364466\n",
      "epoch: 0 sentence: 30720/57013 loss: 4.275734185441742\n",
      "epoch: 0 sentence: 30730/57013 loss: 5.422321424831627\n",
      "epoch: 0 sentence: 30740/57013 loss: 6.269691752919654\n",
      "epoch: 0 sentence: 30750/57013 loss: 5.046144112646727\n",
      "epoch: 0 sentence: 30760/57013 loss: 5.1234222478496845\n",
      "epoch: 0 sentence: 30770/57013 loss: 4.563810378461777\n",
      "epoch: 0 sentence: 30780/57013 loss: 4.45538362235667\n",
      "epoch: 0 sentence: 30790/57013 loss: 4.5117165763166565\n",
      "epoch: 0 sentence: 30800/57013 loss: 3.724712498219938\n",
      "epoch: 0 sentence: 30810/57013 loss: 3.1962470919235835\n",
      "epoch: 0 sentence: 30820/57013 loss: 7.984233251708545\n",
      "epoch: 0 sentence: 30830/57013 loss: 4.477372686493722\n",
      "epoch: 0 sentence: 30840/57013 loss: 6.199151785012219\n",
      "epoch: 0 sentence: 30850/57013 loss: 6.081877969470863\n",
      "epoch: 0 sentence: 30860/57013 loss: 3.8174115793657695\n",
      "epoch: 0 sentence: 30870/57013 loss: 5.196747029888835\n",
      "epoch: 0 sentence: 30880/57013 loss: 6.52550686852693\n",
      "epoch: 0 sentence: 30890/57013 loss: 4.995132138467514\n",
      "epoch: 0 sentence: 30900/57013 loss: 5.701307998791311\n",
      "epoch: 0 sentence: 30910/57013 loss: 4.140632909474433\n",
      "epoch: 0 sentence: 30920/57013 loss: 5.117020424038754\n",
      "epoch: 0 sentence: 30930/57013 loss: 3.8726065213568805\n",
      "epoch: 0 sentence: 30940/57013 loss: 5.311882480465119\n",
      "epoch: 0 sentence: 30950/57013 loss: 4.97462772684488\n",
      "epoch: 0 sentence: 30960/57013 loss: 4.653496476666573\n",
      "epoch: 0 sentence: 30970/57013 loss: 3.555278058168168\n",
      "epoch: 0 sentence: 30980/57013 loss: 4.045214595493764\n",
      "epoch: 0 sentence: 30990/57013 loss: 3.484614296149697\n",
      "epoch: 0 sentence: 31000/57013 loss: 4.730257868583883\n",
      "epoch: 0 sentence: 31010/57013 loss: 3.1652935253622383\n",
      "epoch: 0 sentence: 31020/57013 loss: 4.086569582542808\n",
      "epoch: 0 sentence: 31030/57013 loss: 5.451272744838636\n",
      "epoch: 0 sentence: 31040/57013 loss: 5.155486403293996\n",
      "epoch: 0 sentence: 31050/57013 loss: 4.847936145731172\n",
      "epoch: 0 sentence: 31060/57013 loss: 3.754593016039797\n",
      "epoch: 0 sentence: 31070/57013 loss: 3.9176565712558826\n",
      "epoch: 0 sentence: 31080/57013 loss: 4.161740649464679\n",
      "epoch: 0 sentence: 31090/57013 loss: 4.805634994325969\n",
      "epoch: 0 sentence: 31100/57013 loss: 3.726183537096745\n",
      "epoch: 0 sentence: 31110/57013 loss: 4.189396731564857\n",
      "epoch: 0 sentence: 31120/57013 loss: 6.083418394707832\n",
      "epoch: 0 sentence: 31130/57013 loss: 3.4360589728513156\n",
      "epoch: 0 sentence: 31140/57013 loss: 3.1694727143822177\n",
      "epoch: 0 sentence: 31150/57013 loss: 5.821642999621593\n",
      "epoch: 0 sentence: 31160/57013 loss: 5.664444863055336\n",
      "epoch: 0 sentence: 31170/57013 loss: 4.061140791448348\n",
      "epoch: 0 sentence: 31180/57013 loss: 5.311983842491639\n",
      "epoch: 0 sentence: 31190/57013 loss: 5.950288046007042\n",
      "epoch: 0 sentence: 31200/57013 loss: 4.607570336554964\n",
      "epoch: 0 sentence: 31210/57013 loss: 4.423737682063685\n",
      "epoch: 0 sentence: 31220/57013 loss: 4.238401017712634\n",
      "epoch: 0 sentence: 31230/57013 loss: 4.727523080401149\n",
      "epoch: 0 sentence: 31240/57013 loss: 4.400226620406251\n",
      "epoch: 0 sentence: 31250/57013 loss: 4.226253787754206\n",
      "epoch: 0 sentence: 31260/57013 loss: 3.6685089116899063\n",
      "epoch: 0 sentence: 31270/57013 loss: 5.91206719215544\n",
      "epoch: 0 sentence: 31280/57013 loss: 4.207226131983763\n",
      "epoch: 0 sentence: 31290/57013 loss: 4.67295570309338\n",
      "epoch: 0 sentence: 31300/57013 loss: 4.801292635814088\n",
      "epoch: 0 sentence: 31310/57013 loss: 5.161875161347148\n",
      "epoch: 0 sentence: 31320/57013 loss: 5.156335319777257\n",
      "epoch: 0 sentence: 31330/57013 loss: 2.330024775641521\n",
      "epoch: 0 sentence: 31340/57013 loss: 4.776584744416904\n",
      "epoch: 0 sentence: 31350/57013 loss: 4.0169784282545065\n",
      "epoch: 0 sentence: 31360/57013 loss: 5.484613392136799\n",
      "epoch: 0 sentence: 31370/57013 loss: 3.838451651452854\n",
      "epoch: 0 sentence: 31380/57013 loss: 5.247588667181927\n",
      "epoch: 0 sentence: 31390/57013 loss: 4.066760518891737\n",
      "epoch: 0 sentence: 31400/57013 loss: 4.629995982668803\n",
      "epoch: 0 sentence: 31410/57013 loss: 2.696364745490804\n",
      "epoch: 0 sentence: 31420/57013 loss: 3.5532618303461128\n",
      "epoch: 0 sentence: 31430/57013 loss: 3.717117168037332\n",
      "epoch: 0 sentence: 31440/57013 loss: 3.72651266547391\n",
      "epoch: 0 sentence: 31450/57013 loss: 3.7559347887258667\n",
      "epoch: 0 sentence: 31460/57013 loss: 4.008442191316039\n",
      "epoch: 0 sentence: 31470/57013 loss: 4.4920820780034845\n",
      "epoch: 0 sentence: 31480/57013 loss: 5.448618979203443\n",
      "epoch: 0 sentence: 31490/57013 loss: 4.693778348858461\n",
      "epoch: 0 sentence: 31500/57013 loss: 3.498090314719033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 31510/57013 loss: 4.896180346702645\n",
      "epoch: 0 sentence: 31520/57013 loss: 4.62042011558765\n",
      "epoch: 0 sentence: 31530/57013 loss: 4.502796915531677\n",
      "epoch: 0 sentence: 31540/57013 loss: 4.749769610869238\n",
      "epoch: 0 sentence: 31550/57013 loss: 4.994174631685027\n",
      "epoch: 0 sentence: 31560/57013 loss: 5.321479730843291\n",
      "epoch: 0 sentence: 31570/57013 loss: 3.8514079273215693\n",
      "epoch: 0 sentence: 31580/57013 loss: 4.366763784430808\n",
      "epoch: 0 sentence: 31590/57013 loss: 5.261844201868171\n",
      "epoch: 0 sentence: 31600/57013 loss: 4.216457481066984\n",
      "epoch: 0 sentence: 31610/57013 loss: 3.2249319410775947\n",
      "epoch: 0 sentence: 31620/57013 loss: 4.895857689410239\n",
      "epoch: 0 sentence: 31630/57013 loss: 4.65869384251659\n",
      "epoch: 0 sentence: 31640/57013 loss: 4.8509102349503\n",
      "epoch: 0 sentence: 31650/57013 loss: 5.638784968495342\n",
      "epoch: 0 sentence: 31660/57013 loss: 3.195757319191994\n",
      "epoch: 0 sentence: 31670/57013 loss: 5.328913853565912\n",
      "epoch: 0 sentence: 31680/57013 loss: 4.986486813636598\n",
      "epoch: 0 sentence: 31690/57013 loss: 4.7508844521793385\n",
      "epoch: 0 sentence: 31700/57013 loss: 4.523080255336522\n",
      "epoch: 0 sentence: 31710/57013 loss: 5.249311867551458\n",
      "epoch: 0 sentence: 31720/57013 loss: 3.7790992312237024\n",
      "epoch: 0 sentence: 31730/57013 loss: 5.656918222510852\n",
      "epoch: 0 sentence: 31740/57013 loss: 4.920954802414368\n",
      "epoch: 0 sentence: 31750/57013 loss: 5.472494958460803\n",
      "epoch: 0 sentence: 31760/57013 loss: 5.115242602070534\n",
      "epoch: 0 sentence: 31770/57013 loss: 4.142711558820991\n",
      "epoch: 0 sentence: 31780/57013 loss: 4.832711553164259\n",
      "epoch: 0 sentence: 31790/57013 loss: 3.29819998089312\n",
      "epoch: 0 sentence: 31800/57013 loss: 3.1630361511290404\n",
      "epoch: 0 sentence: 31810/57013 loss: 5.384360213868331\n",
      "epoch: 0 sentence: 31820/57013 loss: 5.3533219528805835\n",
      "epoch: 0 sentence: 31830/57013 loss: 5.0011550170628\n",
      "epoch: 0 sentence: 31840/57013 loss: 4.681820963156519\n",
      "epoch: 0 sentence: 31850/57013 loss: 1.6179898303928846\n",
      "epoch: 0 sentence: 31860/57013 loss: 2.4483239076915813\n",
      "epoch: 0 sentence: 31870/57013 loss: 3.360033023379455\n",
      "epoch: 0 sentence: 31880/57013 loss: 4.7998351728965245\n",
      "epoch: 0 sentence: 31890/57013 loss: 5.366843619022826\n",
      "epoch: 0 sentence: 31900/57013 loss: 6.710753108970309\n",
      "epoch: 0 sentence: 31910/57013 loss: 5.7243710259753575\n",
      "epoch: 0 sentence: 31920/57013 loss: 4.598205300432508\n",
      "epoch: 0 sentence: 31930/57013 loss: 5.0558490168313535\n",
      "epoch: 0 sentence: 31940/57013 loss: 4.367572247326682\n",
      "epoch: 0 sentence: 31950/57013 loss: 5.216083098725943\n",
      "epoch: 0 sentence: 31960/57013 loss: 3.171429085952546\n",
      "epoch: 0 sentence: 31970/57013 loss: 4.355982474060391\n",
      "epoch: 0 sentence: 31980/57013 loss: 4.864813309372799\n",
      "epoch: 0 sentence: 31990/57013 loss: 5.424606859665063\n",
      "epoch: 0 sentence: 32000/57013 loss: 4.725198875124988\n",
      "epoch: 0 sentence: 32010/57013 loss: 3.4929496223429717\n",
      "epoch: 0 sentence: 32020/57013 loss: 4.1475208701414354\n",
      "epoch: 0 sentence: 32030/57013 loss: 4.5167504327517625\n",
      "epoch: 0 sentence: 32040/57013 loss: 3.6346004768246885\n",
      "epoch: 0 sentence: 32050/57013 loss: 5.372303034112496\n",
      "epoch: 0 sentence: 32060/57013 loss: 3.987326844755286\n",
      "epoch: 0 sentence: 32070/57013 loss: 4.539203841901049\n",
      "epoch: 0 sentence: 32080/57013 loss: 4.311397547801506\n",
      "epoch: 0 sentence: 32090/57013 loss: 4.564689640490058\n",
      "epoch: 0 sentence: 32100/57013 loss: 5.30298528740947\n",
      "epoch: 0 sentence: 32110/57013 loss: 5.15450802723127\n",
      "epoch: 0 sentence: 32120/57013 loss: 3.414393308480042\n",
      "epoch: 0 sentence: 32130/57013 loss: 5.454425637695245\n",
      "epoch: 0 sentence: 32140/57013 loss: 4.050393521423506\n",
      "epoch: 0 sentence: 32150/57013 loss: 4.734924115710374\n",
      "epoch: 0 sentence: 32160/57013 loss: 4.906951783073222\n",
      "epoch: 0 sentence: 32170/57013 loss: 4.834602520405968\n",
      "epoch: 0 sentence: 32180/57013 loss: 4.882733929377357\n",
      "epoch: 0 sentence: 32190/57013 loss: 3.816700280958893\n",
      "epoch: 0 sentence: 32200/57013 loss: 4.353900952057392\n",
      "epoch: 0 sentence: 32210/57013 loss: 4.017417368765462\n",
      "epoch: 0 sentence: 32220/57013 loss: 4.734106912949433\n",
      "epoch: 0 sentence: 32230/57013 loss: 5.288352203099985\n",
      "epoch: 0 sentence: 32240/57013 loss: 3.148647035206709\n",
      "epoch: 0 sentence: 32250/57013 loss: 5.178710470679153\n",
      "epoch: 0 sentence: 32260/57013 loss: 4.624002822756226\n",
      "epoch: 0 sentence: 32270/57013 loss: 4.158133860266968\n",
      "epoch: 0 sentence: 32280/57013 loss: 4.107353574720828\n",
      "epoch: 0 sentence: 32290/57013 loss: 5.025276760235846\n",
      "epoch: 0 sentence: 32300/57013 loss: 4.004749343810036\n",
      "epoch: 0 sentence: 32310/57013 loss: 5.180223910680571\n",
      "epoch: 0 sentence: 32320/57013 loss: 3.520374604704505\n",
      "epoch: 0 sentence: 32330/57013 loss: 4.818354861127205\n",
      "epoch: 0 sentence: 32340/57013 loss: 5.161236389794058\n",
      "epoch: 0 sentence: 32350/57013 loss: 5.965335444307942\n",
      "epoch: 0 sentence: 32360/57013 loss: 5.601754275586295\n",
      "epoch: 0 sentence: 32370/57013 loss: 5.044883421809464\n",
      "epoch: 0 sentence: 32380/57013 loss: 5.436932462022761\n",
      "epoch: 0 sentence: 32390/57013 loss: 4.612255906775782\n",
      "epoch: 0 sentence: 32400/57013 loss: 3.9009519667822774\n",
      "epoch: 0 sentence: 32410/57013 loss: 4.376222025840452\n",
      "epoch: 0 sentence: 32420/57013 loss: 4.262855640313709\n",
      "epoch: 0 sentence: 32430/57013 loss: 4.489045024487624\n",
      "epoch: 0 sentence: 32440/57013 loss: 2.7319417691743006\n",
      "epoch: 0 sentence: 32450/57013 loss: 4.320813653141371\n",
      "epoch: 0 sentence: 32460/57013 loss: 5.865292781195602\n",
      "epoch: 0 sentence: 32470/57013 loss: 4.341548989322365\n",
      "epoch: 0 sentence: 32480/57013 loss: 5.471690612902723\n",
      "epoch: 0 sentence: 32490/57013 loss: 6.257948003297344\n",
      "epoch: 0 sentence: 32500/57013 loss: 5.251388656513109\n",
      "epoch: 0 sentence: 32510/57013 loss: 5.253478192208441\n",
      "epoch: 0 sentence: 32520/57013 loss: 4.93716087623497\n",
      "epoch: 0 sentence: 32530/57013 loss: 2.9251839365281382\n",
      "epoch: 0 sentence: 32540/57013 loss: 4.476147561947908\n",
      "epoch: 0 sentence: 32550/57013 loss: 4.174795544040635\n",
      "epoch: 0 sentence: 32560/57013 loss: 5.048930491319541\n",
      "epoch: 0 sentence: 32570/57013 loss: 4.177041758981159\n",
      "epoch: 0 sentence: 32580/57013 loss: 3.1105183556403255\n",
      "epoch: 0 sentence: 32590/57013 loss: 2.9858756814272343\n",
      "epoch: 0 sentence: 32600/57013 loss: 5.717368461837404\n",
      "epoch: 0 sentence: 32610/57013 loss: 5.438144045116376\n",
      "epoch: 0 sentence: 32620/57013 loss: 4.325269832610601\n",
      "epoch: 0 sentence: 32630/57013 loss: 3.5880359608586536\n",
      "epoch: 0 sentence: 32640/57013 loss: 4.66503157125398\n",
      "epoch: 0 sentence: 32650/57013 loss: 3.7528870984598095\n",
      "epoch: 0 sentence: 32660/57013 loss: 5.343142683572513\n",
      "epoch: 0 sentence: 32670/57013 loss: 5.9646678337055095\n",
      "epoch: 0 sentence: 32680/57013 loss: 6.4560373131970765\n",
      "epoch: 0 sentence: 32690/57013 loss: 4.668355630752061\n",
      "epoch: 0 sentence: 32700/57013 loss: 4.6718728713161815\n",
      "epoch: 0 sentence: 32710/57013 loss: 5.1216857532875055\n",
      "epoch: 0 sentence: 32720/57013 loss: 4.781805196368962\n",
      "epoch: 0 sentence: 32730/57013 loss: 4.566075137778511\n",
      "epoch: 0 sentence: 32740/57013 loss: 4.534331834623125\n",
      "epoch: 0 sentence: 32750/57013 loss: 4.731911861315075\n",
      "epoch: 0 sentence: 32760/57013 loss: 4.328904278582974\n",
      "epoch: 0 sentence: 32770/57013 loss: 5.210795552727901\n",
      "epoch: 0 sentence: 32780/57013 loss: 4.9346333360663905\n",
      "epoch: 0 sentence: 32790/57013 loss: 4.422537715318035\n",
      "epoch: 0 sentence: 32800/57013 loss: 5.4954817111060725\n",
      "epoch: 0 sentence: 32810/57013 loss: 5.303005331352171\n",
      "epoch: 0 sentence: 32820/57013 loss: 5.2344184262932325\n",
      "epoch: 0 sentence: 32830/57013 loss: 3.687705304494002\n",
      "epoch: 0 sentence: 32840/57013 loss: 3.7004199850110617\n",
      "epoch: 0 sentence: 32850/57013 loss: 5.195265536603324\n",
      "epoch: 0 sentence: 32860/57013 loss: 4.275785547675717\n",
      "epoch: 0 sentence: 32870/57013 loss: 4.677920568415899\n",
      "epoch: 0 sentence: 32880/57013 loss: 4.34246431353565\n",
      "epoch: 0 sentence: 32890/57013 loss: 3.4355896830002437\n",
      "epoch: 0 sentence: 32900/57013 loss: 4.575862168731857\n",
      "epoch: 0 sentence: 32910/57013 loss: 4.568013060923692\n",
      "epoch: 0 sentence: 32920/57013 loss: 5.2215001445861855\n",
      "epoch: 0 sentence: 32930/57013 loss: 4.259963175887323\n",
      "epoch: 0 sentence: 32940/57013 loss: 4.262218612883696\n",
      "epoch: 0 sentence: 32950/57013 loss: 5.525116233810553\n",
      "epoch: 0 sentence: 32960/57013 loss: 5.72515767654687\n",
      "epoch: 0 sentence: 32970/57013 loss: 3.9773477907692847\n",
      "epoch: 0 sentence: 32980/57013 loss: 5.283421615163077\n",
      "epoch: 0 sentence: 32990/57013 loss: 3.7228839954973623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 33000/57013 loss: 3.392842246700773\n",
      "epoch: 0 sentence: 33010/57013 loss: 4.060946820171729\n",
      "epoch: 0 sentence: 33020/57013 loss: 4.519310736384171\n",
      "epoch: 0 sentence: 33030/57013 loss: 2.984873409095621\n",
      "epoch: 0 sentence: 33040/57013 loss: 2.721477046262785\n",
      "epoch: 0 sentence: 33050/57013 loss: 5.805257441822876\n",
      "epoch: 0 sentence: 33060/57013 loss: 3.3256645405160152\n",
      "epoch: 0 sentence: 33070/57013 loss: 3.9541990493549193\n",
      "epoch: 0 sentence: 33080/57013 loss: 4.015285208585719\n",
      "epoch: 0 sentence: 33090/57013 loss: 5.634391394096517\n",
      "epoch: 0 sentence: 33100/57013 loss: 4.752716283914823\n",
      "epoch: 0 sentence: 33110/57013 loss: 5.033907527178291\n",
      "epoch: 0 sentence: 33120/57013 loss: 5.433595242332622\n",
      "epoch: 0 sentence: 33130/57013 loss: 3.990312979309907\n",
      "epoch: 0 sentence: 33140/57013 loss: 5.342855937527368\n",
      "epoch: 0 sentence: 33150/57013 loss: 5.350243173913057\n",
      "epoch: 0 sentence: 33160/57013 loss: 2.2241041281948535\n",
      "epoch: 0 sentence: 33170/57013 loss: 5.699127149246386\n",
      "epoch: 0 sentence: 33180/57013 loss: 4.737911711694287\n",
      "epoch: 0 sentence: 33190/57013 loss: 2.943796813720426\n",
      "epoch: 0 sentence: 33200/57013 loss: 5.239630889579725\n",
      "epoch: 0 sentence: 33210/57013 loss: 4.921041864185423\n",
      "epoch: 0 sentence: 33220/57013 loss: 5.213706001722962\n",
      "epoch: 0 sentence: 33230/57013 loss: 4.546692148790297\n",
      "epoch: 0 sentence: 33240/57013 loss: 3.315247708063526\n",
      "epoch: 0 sentence: 33250/57013 loss: 3.7913504910482247\n",
      "epoch: 0 sentence: 33260/57013 loss: 4.607644113609149\n",
      "epoch: 0 sentence: 33270/57013 loss: 4.220765657739758\n",
      "epoch: 0 sentence: 33280/57013 loss: 4.332339914999723\n",
      "epoch: 0 sentence: 33290/57013 loss: 5.5361468042472355\n",
      "epoch: 0 sentence: 33300/57013 loss: 4.113487301398741\n",
      "epoch: 0 sentence: 33310/57013 loss: 3.734018192616019\n",
      "epoch: 0 sentence: 33320/57013 loss: 6.1460682838689005\n",
      "epoch: 0 sentence: 33330/57013 loss: 5.002365982106618\n",
      "epoch: 0 sentence: 33340/57013 loss: 5.119672682960843\n",
      "epoch: 0 sentence: 33350/57013 loss: 5.49109659106659\n",
      "epoch: 0 sentence: 33360/57013 loss: 5.167250339619778\n",
      "epoch: 0 sentence: 33370/57013 loss: 2.812927834861702\n",
      "epoch: 0 sentence: 33380/57013 loss: 5.395169860506098\n",
      "epoch: 0 sentence: 33390/57013 loss: 3.4527841799319696\n",
      "epoch: 0 sentence: 33400/57013 loss: 6.7988952515224295\n",
      "epoch: 0 sentence: 33410/57013 loss: 4.747983663092229\n",
      "epoch: 0 sentence: 33420/57013 loss: 5.396223325388583\n",
      "epoch: 0 sentence: 33430/57013 loss: 3.4055735745278453\n",
      "epoch: 0 sentence: 33440/57013 loss: 4.153532712206871\n",
      "epoch: 0 sentence: 33450/57013 loss: 5.505542257550394\n",
      "epoch: 0 sentence: 33460/57013 loss: 3.5516354803062677\n",
      "epoch: 0 sentence: 33470/57013 loss: 3.7897946353758347\n",
      "epoch: 0 sentence: 33480/57013 loss: 4.877601196008637\n",
      "epoch: 0 sentence: 33490/57013 loss: 6.591887597881006\n",
      "epoch: 0 sentence: 33500/57013 loss: 5.82681671453449\n",
      "epoch: 0 sentence: 33510/57013 loss: 4.629685634803583\n",
      "epoch: 0 sentence: 33520/57013 loss: 5.621788868964546\n",
      "epoch: 0 sentence: 33530/57013 loss: 5.22101306343937\n",
      "epoch: 0 sentence: 33540/57013 loss: 4.983503397632488\n",
      "epoch: 0 sentence: 33550/57013 loss: 5.23110052944277\n",
      "epoch: 0 sentence: 33560/57013 loss: 3.860030516603697\n",
      "epoch: 0 sentence: 33570/57013 loss: 3.798068514707507\n",
      "epoch: 0 sentence: 33580/57013 loss: 4.642509192738494\n",
      "epoch: 0 sentence: 33590/57013 loss: 3.472011083885576\n",
      "epoch: 0 sentence: 33600/57013 loss: 2.8418172163471853\n",
      "epoch: 0 sentence: 33610/57013 loss: 4.123807088269435\n",
      "epoch: 0 sentence: 33620/57013 loss: 3.652719526906565\n",
      "epoch: 0 sentence: 33630/57013 loss: 5.237614375416396\n",
      "epoch: 0 sentence: 33640/57013 loss: 5.901640441913123\n",
      "epoch: 0 sentence: 33650/57013 loss: 2.8972865503315455\n",
      "epoch: 0 sentence: 33660/57013 loss: 4.381300454921745\n",
      "epoch: 0 sentence: 33670/57013 loss: 4.698294458996056\n",
      "epoch: 0 sentence: 33680/57013 loss: 5.446886756686545\n",
      "epoch: 0 sentence: 33690/57013 loss: 4.331321750851366\n",
      "epoch: 0 sentence: 33700/57013 loss: 4.275971765472031\n",
      "epoch: 0 sentence: 33710/57013 loss: 5.041070035473856\n",
      "epoch: 0 sentence: 33720/57013 loss: 3.9045561160274693\n",
      "epoch: 0 sentence: 33730/57013 loss: 4.935960687485824\n",
      "epoch: 0 sentence: 33740/57013 loss: 4.544180973770228\n",
      "epoch: 0 sentence: 33750/57013 loss: 4.968188599966845\n",
      "epoch: 0 sentence: 33760/57013 loss: 3.64770071745057\n",
      "epoch: 0 sentence: 33770/57013 loss: 5.120033081997972\n",
      "epoch: 0 sentence: 33780/57013 loss: 2.957271018193498\n",
      "epoch: 0 sentence: 33790/57013 loss: 5.009944703131875\n",
      "epoch: 0 sentence: 33800/57013 loss: 4.088840356458337\n",
      "epoch: 0 sentence: 33810/57013 loss: 5.113770505182474\n",
      "epoch: 0 sentence: 33820/57013 loss: 6.132106442550259\n",
      "epoch: 0 sentence: 33830/57013 loss: 5.127015292227473\n",
      "epoch: 0 sentence: 33840/57013 loss: 4.532960985983093\n",
      "epoch: 0 sentence: 33850/57013 loss: 5.957510402084591\n",
      "epoch: 0 sentence: 33860/57013 loss: 4.61623286304398\n",
      "epoch: 0 sentence: 33870/57013 loss: 4.957751354409404\n",
      "epoch: 0 sentence: 33880/57013 loss: 3.1819199958505613\n",
      "epoch: 0 sentence: 33890/57013 loss: 6.492097057695607\n",
      "epoch: 0 sentence: 33900/57013 loss: 3.7662798510040547\n",
      "epoch: 0 sentence: 33910/57013 loss: 4.532667980566552\n",
      "epoch: 0 sentence: 33920/57013 loss: 5.189555232835295\n",
      "epoch: 0 sentence: 33930/57013 loss: 4.571339681452536\n",
      "epoch: 0 sentence: 33940/57013 loss: 4.182867275915799\n",
      "epoch: 0 sentence: 33950/57013 loss: 2.0659508975534187\n",
      "epoch: 0 sentence: 33960/57013 loss: 4.760913768503379\n",
      "epoch: 0 sentence: 33970/57013 loss: 5.383655840041223\n",
      "epoch: 0 sentence: 33980/57013 loss: 4.137478647131073\n",
      "epoch: 0 sentence: 33990/57013 loss: 4.032993944088113\n",
      "epoch: 0 sentence: 34000/57013 loss: 5.209541666814923\n",
      "epoch: 0 sentence: 34010/57013 loss: 4.3126390162841295\n",
      "epoch: 0 sentence: 34020/57013 loss: 5.430974062444405\n",
      "epoch: 0 sentence: 34030/57013 loss: 4.711737573865515\n",
      "epoch: 0 sentence: 34040/57013 loss: 4.771610522409304\n",
      "epoch: 0 sentence: 34050/57013 loss: 3.0236969468126436\n",
      "epoch: 0 sentence: 34060/57013 loss: 4.416125518191192\n",
      "epoch: 0 sentence: 34070/57013 loss: 4.89847602628213\n",
      "epoch: 0 sentence: 34080/57013 loss: 4.280069735590067\n",
      "epoch: 0 sentence: 34090/57013 loss: 3.995628196152649\n",
      "epoch: 0 sentence: 34100/57013 loss: 5.3758223882751475\n",
      "epoch: 0 sentence: 34110/57013 loss: 5.039512478512006\n",
      "epoch: 0 sentence: 34120/57013 loss: 4.99265168574176\n",
      "epoch: 0 sentence: 34130/57013 loss: 3.7073850211605337\n",
      "epoch: 0 sentence: 34140/57013 loss: 4.104982353638737\n",
      "epoch: 0 sentence: 34150/57013 loss: 4.78193299376735\n",
      "epoch: 0 sentence: 34160/57013 loss: 2.694104304164842\n",
      "epoch: 0 sentence: 34170/57013 loss: 4.840150916269519\n",
      "epoch: 0 sentence: 34180/57013 loss: 3.9476619987984165\n",
      "epoch: 0 sentence: 34190/57013 loss: 5.533155228389566\n",
      "epoch: 0 sentence: 34200/57013 loss: 4.5540650920464225\n",
      "epoch: 0 sentence: 34210/57013 loss: 5.404917024220477\n",
      "epoch: 0 sentence: 34220/57013 loss: 5.567849916409232\n",
      "epoch: 0 sentence: 34230/57013 loss: 4.743817455159733\n",
      "epoch: 0 sentence: 34240/57013 loss: 5.504030570610454\n",
      "epoch: 0 sentence: 34250/57013 loss: 4.437111195471387\n",
      "epoch: 0 sentence: 34260/57013 loss: 5.209023964944381\n",
      "epoch: 0 sentence: 34270/57013 loss: 4.360450150733637\n",
      "epoch: 0 sentence: 34280/57013 loss: 4.412398015802739\n",
      "epoch: 0 sentence: 34290/57013 loss: 3.708934890952621\n",
      "epoch: 0 sentence: 34300/57013 loss: 3.425158241222915\n",
      "epoch: 0 sentence: 34310/57013 loss: 4.880636544848329\n",
      "epoch: 0 sentence: 34320/57013 loss: 4.918420534800574\n",
      "epoch: 0 sentence: 34330/57013 loss: 5.780673573109809\n",
      "epoch: 0 sentence: 34340/57013 loss: 5.145023731904523\n",
      "epoch: 0 sentence: 34350/57013 loss: 4.501066143276928\n",
      "epoch: 0 sentence: 34360/57013 loss: 4.454886513068368\n",
      "epoch: 0 sentence: 34370/57013 loss: 3.7931159570616875\n",
      "epoch: 0 sentence: 34380/57013 loss: 3.3661805522083896\n",
      "epoch: 0 sentence: 34390/57013 loss: 3.1777794715890773\n",
      "epoch: 0 sentence: 34400/57013 loss: 3.2132178571633303\n",
      "epoch: 0 sentence: 34410/57013 loss: 4.718744228657293\n",
      "epoch: 0 sentence: 34420/57013 loss: 4.669680585427281\n",
      "epoch: 0 sentence: 34430/57013 loss: 3.207899901065286\n",
      "epoch: 0 sentence: 34440/57013 loss: 5.635457838815102\n",
      "epoch: 0 sentence: 34450/57013 loss: 4.963950326510217\n",
      "epoch: 0 sentence: 34460/57013 loss: 4.1967336976501635\n",
      "epoch: 0 sentence: 34470/57013 loss: 5.3117877448370585\n",
      "epoch: 0 sentence: 34480/57013 loss: 4.930110170742252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 34490/57013 loss: 3.5555205282909457\n",
      "epoch: 0 sentence: 34500/57013 loss: 4.890990516621174\n",
      "epoch: 0 sentence: 34510/57013 loss: 5.647933525009421\n",
      "epoch: 0 sentence: 34520/57013 loss: 4.460420538101644\n",
      "epoch: 0 sentence: 34530/57013 loss: 4.230037293089751\n",
      "epoch: 0 sentence: 34540/57013 loss: 5.685489917416104\n",
      "epoch: 0 sentence: 34550/57013 loss: 4.90434916392798\n",
      "epoch: 0 sentence: 34560/57013 loss: 3.764057821881789\n",
      "epoch: 0 sentence: 34570/57013 loss: 5.628749744696179\n",
      "epoch: 0 sentence: 34580/57013 loss: 3.8002861722420964\n",
      "epoch: 0 sentence: 34590/57013 loss: 4.555915296387074\n",
      "epoch: 0 sentence: 34600/57013 loss: 5.391954700639148\n",
      "epoch: 0 sentence: 34610/57013 loss: 4.621204139053379\n",
      "epoch: 0 sentence: 34620/57013 loss: 5.03846515413593\n",
      "epoch: 0 sentence: 34630/57013 loss: 4.79563757413684\n",
      "epoch: 0 sentence: 34640/57013 loss: 4.027471085729174\n",
      "epoch: 0 sentence: 34650/57013 loss: 4.661414488219519\n",
      "epoch: 0 sentence: 34660/57013 loss: 5.901128574230805\n",
      "epoch: 0 sentence: 34670/57013 loss: 4.516919820862437\n",
      "epoch: 0 sentence: 34680/57013 loss: 6.3799990782077165\n",
      "epoch: 0 sentence: 34690/57013 loss: 4.8076239677420824\n",
      "epoch: 0 sentence: 34700/57013 loss: 4.698623387810325\n",
      "epoch: 0 sentence: 34710/57013 loss: 4.062957279915257\n",
      "epoch: 0 sentence: 34720/57013 loss: 4.3060111705985324\n",
      "epoch: 0 sentence: 34730/57013 loss: 4.796945201053118\n",
      "epoch: 0 sentence: 34740/57013 loss: 3.3334616984486867\n",
      "epoch: 0 sentence: 34750/57013 loss: 5.207757248700541\n",
      "epoch: 0 sentence: 34760/57013 loss: 6.050575470654021\n",
      "epoch: 0 sentence: 34770/57013 loss: 4.327563226354698\n",
      "epoch: 0 sentence: 34780/57013 loss: 4.994247483541837\n",
      "epoch: 0 sentence: 34790/57013 loss: 4.831925350552079\n",
      "epoch: 0 sentence: 34800/57013 loss: 5.8588584539961275\n",
      "epoch: 0 sentence: 34810/57013 loss: 5.700648822614773\n",
      "epoch: 0 sentence: 34820/57013 loss: 4.948577174478148\n",
      "epoch: 0 sentence: 34830/57013 loss: 5.48553189333774\n",
      "epoch: 0 sentence: 34840/57013 loss: 4.965584414166279\n",
      "epoch: 0 sentence: 34850/57013 loss: 4.688538946870316\n",
      "epoch: 0 sentence: 34860/57013 loss: 5.076993266510449\n",
      "epoch: 0 sentence: 34870/57013 loss: 4.200731230463623\n",
      "epoch: 0 sentence: 34880/57013 loss: 5.107757277953318\n",
      "epoch: 0 sentence: 34890/57013 loss: 4.189328490067971\n",
      "epoch: 0 sentence: 34900/57013 loss: 4.814659801354289\n",
      "epoch: 0 sentence: 34910/57013 loss: 5.19323169683344\n",
      "epoch: 0 sentence: 34920/57013 loss: 3.863976722196684\n",
      "epoch: 0 sentence: 34930/57013 loss: 4.567041711491253\n",
      "epoch: 0 sentence: 34940/57013 loss: 4.3735690185402545\n",
      "epoch: 0 sentence: 34950/57013 loss: 3.8015178215814562\n",
      "epoch: 0 sentence: 34960/57013 loss: 5.749172903812282\n",
      "epoch: 0 sentence: 34970/57013 loss: 5.49045657266678\n",
      "epoch: 0 sentence: 34980/57013 loss: 5.1379986830333735\n",
      "epoch: 0 sentence: 34990/57013 loss: 4.2424620194009135\n",
      "epoch: 0 sentence: 35000/57013 loss: 4.315895852933799\n",
      "epoch: 0 sentence: 35010/57013 loss: 4.734222312134115\n",
      "epoch: 0 sentence: 35020/57013 loss: 5.41400435532703\n",
      "epoch: 0 sentence: 35030/57013 loss: 3.033632696228135\n",
      "epoch: 0 sentence: 35040/57013 loss: 3.253577889332506\n",
      "epoch: 0 sentence: 35050/57013 loss: 2.658534149491551\n",
      "epoch: 0 sentence: 35060/57013 loss: 3.3345247671296168\n",
      "epoch: 0 sentence: 35070/57013 loss: 3.521447039436723\n",
      "epoch: 0 sentence: 35080/57013 loss: 4.859779636633066\n",
      "epoch: 0 sentence: 35090/57013 loss: 5.8292981645381055\n",
      "epoch: 0 sentence: 35100/57013 loss: 4.998301965343982\n",
      "epoch: 0 sentence: 35110/57013 loss: 5.141519018344202\n",
      "epoch: 0 sentence: 35120/57013 loss: 5.108985387661441\n",
      "epoch: 0 sentence: 35130/57013 loss: 5.166571021969632\n",
      "epoch: 0 sentence: 35140/57013 loss: 4.828467538727419\n",
      "epoch: 0 sentence: 35150/57013 loss: 4.0349888240890985\n",
      "epoch: 0 sentence: 35160/57013 loss: 1.9423465459071054\n",
      "epoch: 0 sentence: 35170/57013 loss: 4.548185526006967\n",
      "epoch: 0 sentence: 35180/57013 loss: 4.349165444343681\n",
      "epoch: 0 sentence: 35190/57013 loss: 5.090241906748709\n",
      "epoch: 0 sentence: 35200/57013 loss: 3.2943800889508754\n",
      "epoch: 0 sentence: 35210/57013 loss: 4.445594959032528\n",
      "epoch: 0 sentence: 35220/57013 loss: 5.3991826364451505\n",
      "epoch: 0 sentence: 35230/57013 loss: 3.648366147173455\n",
      "epoch: 0 sentence: 35240/57013 loss: 4.34865147113823\n",
      "epoch: 0 sentence: 35250/57013 loss: 4.8128580566261006\n",
      "epoch: 0 sentence: 35260/57013 loss: 4.433615917532397\n",
      "epoch: 0 sentence: 35270/57013 loss: 4.949422207969629\n",
      "epoch: 0 sentence: 35280/57013 loss: 4.022063088861757\n",
      "epoch: 0 sentence: 35290/57013 loss: 3.289238496776804\n",
      "epoch: 0 sentence: 35300/57013 loss: 5.148446064180736\n",
      "epoch: 0 sentence: 35310/57013 loss: 3.673191997981882\n",
      "epoch: 0 sentence: 35320/57013 loss: 5.085801823274723\n",
      "epoch: 0 sentence: 35330/57013 loss: 5.19549509730879\n",
      "epoch: 0 sentence: 35340/57013 loss: 5.369374351870315\n",
      "epoch: 0 sentence: 35350/57013 loss: 2.5505068047674997\n",
      "epoch: 0 sentence: 35360/57013 loss: 4.857488877628495\n",
      "epoch: 0 sentence: 35370/57013 loss: 4.6563221514728435\n",
      "epoch: 0 sentence: 35380/57013 loss: 4.90986153988103\n",
      "epoch: 0 sentence: 35390/57013 loss: 3.6956009582450386\n",
      "epoch: 0 sentence: 35400/57013 loss: 5.757394708557743\n",
      "epoch: 0 sentence: 35410/57013 loss: 4.704489550670149\n",
      "epoch: 0 sentence: 35420/57013 loss: 3.798373119364256\n",
      "epoch: 0 sentence: 35430/57013 loss: 4.9542032034356\n",
      "epoch: 0 sentence: 35440/57013 loss: 2.9276884380483383\n",
      "epoch: 0 sentence: 35450/57013 loss: 4.40042935032466\n",
      "epoch: 0 sentence: 35460/57013 loss: 3.914326799346391\n",
      "epoch: 0 sentence: 35470/57013 loss: 5.247704155054982\n",
      "epoch: 0 sentence: 35480/57013 loss: 4.437938936638239\n",
      "epoch: 0 sentence: 35490/57013 loss: 5.023504119684162\n",
      "epoch: 0 sentence: 35500/57013 loss: 4.161744816060742\n",
      "epoch: 0 sentence: 35510/57013 loss: 3.3251119932973734\n",
      "epoch: 0 sentence: 35520/57013 loss: 5.79201037166859\n",
      "epoch: 0 sentence: 35530/57013 loss: 5.622938505933506\n",
      "epoch: 0 sentence: 35540/57013 loss: 3.3087254876901175\n",
      "epoch: 0 sentence: 35550/57013 loss: 2.5803331717562936\n",
      "epoch: 0 sentence: 35560/57013 loss: 4.637529387943698\n",
      "epoch: 0 sentence: 35570/57013 loss: 5.07114790380488\n",
      "epoch: 0 sentence: 35580/57013 loss: 6.627409919644506\n",
      "epoch: 0 sentence: 35590/57013 loss: 5.059610065955563\n",
      "epoch: 0 sentence: 35600/57013 loss: 5.084614485919557\n",
      "epoch: 0 sentence: 35610/57013 loss: 4.638417165337097\n",
      "epoch: 0 sentence: 35620/57013 loss: 2.0775233067880494\n",
      "epoch: 0 sentence: 35630/57013 loss: 3.930783611377685\n",
      "epoch: 0 sentence: 35640/57013 loss: 4.773682726738162\n",
      "epoch: 0 sentence: 35650/57013 loss: 3.1794052782864552\n",
      "epoch: 0 sentence: 35660/57013 loss: 2.9804777005698733\n",
      "epoch: 0 sentence: 35670/57013 loss: 6.80076511327677\n",
      "epoch: 0 sentence: 35680/57013 loss: 3.9602753278366825\n",
      "epoch: 0 sentence: 35690/57013 loss: 5.0115932195899555\n",
      "epoch: 0 sentence: 35700/57013 loss: 4.526656802711208\n",
      "epoch: 0 sentence: 35710/57013 loss: 4.246308566435951\n",
      "epoch: 0 sentence: 35720/57013 loss: 4.261548561230808\n",
      "epoch: 0 sentence: 35730/57013 loss: 5.37856367500608\n",
      "epoch: 0 sentence: 35740/57013 loss: 5.763076636336694\n",
      "epoch: 0 sentence: 35750/57013 loss: 4.571313322800856\n",
      "epoch: 0 sentence: 35760/57013 loss: 3.6782377146996685\n",
      "epoch: 0 sentence: 35770/57013 loss: 5.804998543936381\n",
      "epoch: 0 sentence: 35780/57013 loss: 4.558937465146412\n",
      "epoch: 0 sentence: 35790/57013 loss: 3.7148417828711784\n",
      "epoch: 0 sentence: 35800/57013 loss: 5.224999201918627\n",
      "epoch: 0 sentence: 35810/57013 loss: 4.806049519430726\n",
      "epoch: 0 sentence: 35820/57013 loss: 2.7715743147384235\n",
      "epoch: 0 sentence: 35830/57013 loss: 1.950471257066543\n",
      "epoch: 0 sentence: 35840/57013 loss: 4.583367784959966\n",
      "epoch: 0 sentence: 35850/57013 loss: 5.523466621930678\n",
      "epoch: 0 sentence: 35860/57013 loss: 4.403162497263989\n",
      "epoch: 0 sentence: 35870/57013 loss: 4.738600887995986\n",
      "epoch: 0 sentence: 35880/57013 loss: 4.201472597068704\n",
      "epoch: 0 sentence: 35890/57013 loss: 5.533818649897823\n",
      "epoch: 0 sentence: 35900/57013 loss: 4.76478439012076\n",
      "epoch: 0 sentence: 35910/57013 loss: 4.713614794999953\n",
      "epoch: 0 sentence: 35920/57013 loss: 4.126214245000503\n",
      "epoch: 0 sentence: 35930/57013 loss: 5.06602456409588\n",
      "epoch: 0 sentence: 35940/57013 loss: 4.8446281745127004\n",
      "epoch: 0 sentence: 35950/57013 loss: 4.767090758839973\n",
      "epoch: 0 sentence: 35960/57013 loss: 5.570632711381639\n",
      "epoch: 0 sentence: 35970/57013 loss: 4.133402460895069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 35980/57013 loss: 4.897402421593875\n",
      "epoch: 0 sentence: 35990/57013 loss: 5.423794075657133\n",
      "epoch: 0 sentence: 36000/57013 loss: 5.035854249296722\n",
      "epoch: 0 sentence: 36010/57013 loss: 4.038674824171407\n",
      "epoch: 0 sentence: 36020/57013 loss: 4.001250914506133\n",
      "epoch: 0 sentence: 36030/57013 loss: 4.581444802854189\n",
      "epoch: 0 sentence: 36040/57013 loss: 5.257305336635175\n",
      "epoch: 0 sentence: 36050/57013 loss: 3.951156272718346\n",
      "epoch: 0 sentence: 36060/57013 loss: 4.366245874047587\n",
      "epoch: 0 sentence: 36070/57013 loss: 2.2606284267818975\n",
      "epoch: 0 sentence: 36080/57013 loss: 5.766887658991673\n",
      "epoch: 0 sentence: 36090/57013 loss: 3.7109447651544185\n",
      "epoch: 0 sentence: 36100/57013 loss: 5.104694464141026\n",
      "epoch: 0 sentence: 36110/57013 loss: 5.168393584983867\n",
      "epoch: 0 sentence: 36120/57013 loss: 4.248693142868333\n",
      "epoch: 0 sentence: 36130/57013 loss: 5.261200272927555\n",
      "epoch: 0 sentence: 36140/57013 loss: 3.762652761965676\n",
      "epoch: 0 sentence: 36150/57013 loss: 4.386881440834589\n",
      "epoch: 0 sentence: 36160/57013 loss: 4.442680482648232\n",
      "epoch: 0 sentence: 36170/57013 loss: 5.550652566264929\n",
      "epoch: 0 sentence: 36180/57013 loss: 5.098441901022293\n",
      "epoch: 0 sentence: 36190/57013 loss: 3.987958462997956\n",
      "epoch: 0 sentence: 36200/57013 loss: 4.49450481813049\n",
      "epoch: 0 sentence: 36210/57013 loss: 4.424316983057614\n",
      "epoch: 0 sentence: 36220/57013 loss: 3.543318366132701\n",
      "epoch: 0 sentence: 36230/57013 loss: 3.559016347013118\n",
      "epoch: 0 sentence: 36240/57013 loss: 4.23678836054922\n",
      "epoch: 0 sentence: 36250/57013 loss: 5.187100258642383\n",
      "epoch: 0 sentence: 36260/57013 loss: 3.3130581613202903\n",
      "epoch: 0 sentence: 36270/57013 loss: 4.36539085763759\n",
      "epoch: 0 sentence: 36280/57013 loss: 1.9448303343681639\n",
      "epoch: 0 sentence: 36290/57013 loss: 6.1796024525750575\n",
      "epoch: 0 sentence: 36300/57013 loss: 4.030872911098686\n",
      "epoch: 0 sentence: 36310/57013 loss: 6.326248506360937\n",
      "epoch: 0 sentence: 36320/57013 loss: 5.5096315623249\n",
      "epoch: 0 sentence: 36330/57013 loss: 5.867590110630581\n",
      "epoch: 0 sentence: 36340/57013 loss: 4.779104028590382\n",
      "epoch: 0 sentence: 36350/57013 loss: 4.466130583382163\n",
      "epoch: 0 sentence: 36360/57013 loss: 4.4893600658234805\n",
      "epoch: 0 sentence: 36370/57013 loss: 4.519461459149771\n",
      "epoch: 0 sentence: 36380/57013 loss: 5.666131905867112\n",
      "epoch: 0 sentence: 36390/57013 loss: 5.091070448204373\n",
      "epoch: 0 sentence: 36400/57013 loss: 4.2472924510476355\n",
      "epoch: 0 sentence: 36410/57013 loss: 2.6995310863636\n",
      "epoch: 0 sentence: 36420/57013 loss: 4.889039747215187\n",
      "epoch: 0 sentence: 36430/57013 loss: 5.115258464935871\n",
      "epoch: 0 sentence: 36440/57013 loss: 5.147953669382299\n",
      "epoch: 0 sentence: 36450/57013 loss: 3.7582118923885726\n",
      "epoch: 0 sentence: 36460/57013 loss: 4.720540437937376\n",
      "epoch: 0 sentence: 36470/57013 loss: 5.314051351432705\n",
      "epoch: 0 sentence: 36480/57013 loss: 3.1440087965476144\n",
      "epoch: 0 sentence: 36490/57013 loss: 4.782180033185449\n",
      "epoch: 0 sentence: 36500/57013 loss: 5.149949271839596\n",
      "epoch: 0 sentence: 36510/57013 loss: 4.6040928095011076\n",
      "epoch: 0 sentence: 36520/57013 loss: 4.542763452924089\n",
      "epoch: 0 sentence: 36530/57013 loss: 5.628696000955657\n",
      "epoch: 0 sentence: 36540/57013 loss: 5.634963519600593\n",
      "epoch: 0 sentence: 36550/57013 loss: 5.453260838702822\n",
      "epoch: 0 sentence: 36560/57013 loss: 4.862952035203784\n",
      "epoch: 0 sentence: 36570/57013 loss: 2.5681288936720335\n",
      "epoch: 0 sentence: 36580/57013 loss: 4.685012753056323\n",
      "epoch: 0 sentence: 36590/57013 loss: 4.5319745078731755\n",
      "epoch: 0 sentence: 36600/57013 loss: 4.755391387836893\n",
      "epoch: 0 sentence: 36610/57013 loss: 4.151335508119472\n",
      "epoch: 0 sentence: 36620/57013 loss: 4.133386287335034\n",
      "epoch: 0 sentence: 36630/57013 loss: 4.750616411776758\n",
      "epoch: 0 sentence: 36640/57013 loss: 4.82582661952901\n",
      "epoch: 0 sentence: 36650/57013 loss: 5.747747475428689\n",
      "epoch: 0 sentence: 36660/57013 loss: 5.023611866539164\n",
      "epoch: 0 sentence: 36670/57013 loss: 4.764459722325264\n",
      "epoch: 0 sentence: 36680/57013 loss: 3.741817763342979\n",
      "epoch: 0 sentence: 36690/57013 loss: 4.993711832132292\n",
      "epoch: 0 sentence: 36700/57013 loss: 1.9038629730365182\n",
      "epoch: 0 sentence: 36710/57013 loss: 4.597739144979798\n",
      "epoch: 0 sentence: 36720/57013 loss: 5.130617991270393\n",
      "epoch: 0 sentence: 36730/57013 loss: 4.342952397426536\n",
      "epoch: 0 sentence: 36740/57013 loss: 2.623054915699038\n",
      "epoch: 0 sentence: 36750/57013 loss: 5.5011217177795535\n",
      "epoch: 0 sentence: 36760/57013 loss: 8.518453855291044\n",
      "epoch: 0 sentence: 36770/57013 loss: 4.39666160582304\n",
      "epoch: 0 sentence: 36780/57013 loss: 4.492656509842177\n",
      "epoch: 0 sentence: 36790/57013 loss: 3.922848724560076\n",
      "epoch: 0 sentence: 36800/57013 loss: 4.772021784399679\n",
      "epoch: 0 sentence: 36810/57013 loss: 4.682174510889887\n",
      "epoch: 0 sentence: 36820/57013 loss: 4.410579249914271\n",
      "epoch: 0 sentence: 36830/57013 loss: 4.6562229375261674\n",
      "epoch: 0 sentence: 36840/57013 loss: 6.441076256655257\n",
      "epoch: 0 sentence: 36850/57013 loss: 3.342684620560393\n",
      "epoch: 0 sentence: 36860/57013 loss: 4.595878645394262\n",
      "epoch: 0 sentence: 36870/57013 loss: 4.1046771130521895\n",
      "epoch: 0 sentence: 36880/57013 loss: 4.633822969335406\n",
      "epoch: 0 sentence: 36890/57013 loss: 4.2877462796732155\n",
      "epoch: 0 sentence: 36900/57013 loss: 4.416644861721195\n",
      "epoch: 0 sentence: 36910/57013 loss: 4.592827071209855\n",
      "epoch: 0 sentence: 36920/57013 loss: 4.017591558644174\n",
      "epoch: 0 sentence: 36930/57013 loss: 5.030891664871377\n",
      "epoch: 0 sentence: 36940/57013 loss: 4.541069787914333\n",
      "epoch: 0 sentence: 36950/57013 loss: 4.369247120455702\n",
      "epoch: 0 sentence: 36960/57013 loss: 4.3390941193361465\n",
      "epoch: 0 sentence: 36970/57013 loss: 5.55056167100172\n",
      "epoch: 0 sentence: 36980/57013 loss: 5.525466122361937\n",
      "epoch: 0 sentence: 36990/57013 loss: 5.0108080114970965\n",
      "epoch: 0 sentence: 37000/57013 loss: 4.772666249577283\n",
      "epoch: 0 sentence: 37010/57013 loss: 5.663918682334268\n",
      "epoch: 0 sentence: 37020/57013 loss: 3.90818028642675\n",
      "epoch: 0 sentence: 37030/57013 loss: 4.93670635975886\n",
      "epoch: 0 sentence: 37040/57013 loss: 4.576720666161769\n",
      "epoch: 0 sentence: 37050/57013 loss: 3.7205484323607525\n",
      "epoch: 0 sentence: 37060/57013 loss: 3.8513809146437614\n",
      "epoch: 0 sentence: 37070/57013 loss: 5.4852796564630095\n",
      "epoch: 0 sentence: 37080/57013 loss: 4.297314955625827\n",
      "epoch: 0 sentence: 37090/57013 loss: 5.704051411814493\n",
      "epoch: 0 sentence: 37100/57013 loss: 3.2744036042719897\n",
      "epoch: 0 sentence: 37110/57013 loss: 4.897871035524179\n",
      "epoch: 0 sentence: 37120/57013 loss: 4.461229560969346\n",
      "epoch: 0 sentence: 37130/57013 loss: 5.919611732546729\n",
      "epoch: 0 sentence: 37140/57013 loss: 4.742454409297276\n",
      "epoch: 0 sentence: 37150/57013 loss: 4.945796103758717\n",
      "epoch: 0 sentence: 37160/57013 loss: 4.99774321378079\n",
      "epoch: 0 sentence: 37170/57013 loss: 3.0168491572709537\n",
      "epoch: 0 sentence: 37180/57013 loss: 5.23553417153136\n",
      "epoch: 0 sentence: 37190/57013 loss: 5.588599525933287\n",
      "epoch: 0 sentence: 37200/57013 loss: 5.0788415620177\n",
      "epoch: 0 sentence: 37210/57013 loss: 5.497548047413703\n",
      "epoch: 0 sentence: 37220/57013 loss: 4.9062448042526015\n",
      "epoch: 0 sentence: 37230/57013 loss: 5.617186743289329\n",
      "epoch: 0 sentence: 37240/57013 loss: 3.286766467035007\n",
      "epoch: 0 sentence: 37250/57013 loss: 4.8347630969367446\n",
      "epoch: 0 sentence: 37260/57013 loss: 5.435399315448739\n",
      "epoch: 0 sentence: 37270/57013 loss: 5.787759870619851\n",
      "epoch: 0 sentence: 37280/57013 loss: 5.542659078068358\n",
      "epoch: 0 sentence: 37290/57013 loss: 4.088390966184307\n",
      "epoch: 0 sentence: 37300/57013 loss: 4.295683167628366\n",
      "epoch: 0 sentence: 37310/57013 loss: 5.5535769954056216\n",
      "epoch: 0 sentence: 37320/57013 loss: 4.927462131322004\n",
      "epoch: 0 sentence: 37330/57013 loss: 3.618056681625512\n",
      "epoch: 0 sentence: 37340/57013 loss: 4.663060569665289\n",
      "epoch: 0 sentence: 37350/57013 loss: 5.059467719994688\n",
      "epoch: 0 sentence: 37360/57013 loss: 4.784736045432926\n",
      "epoch: 0 sentence: 37370/57013 loss: 5.415662965056639\n",
      "epoch: 0 sentence: 37380/57013 loss: 4.605811821627292\n",
      "epoch: 0 sentence: 37390/57013 loss: 2.3682214121934213\n",
      "epoch: 0 sentence: 37400/57013 loss: 4.593281748750885\n",
      "epoch: 0 sentence: 37410/57013 loss: 5.156143777479021\n",
      "epoch: 0 sentence: 37420/57013 loss: 4.694466997077883\n",
      "epoch: 0 sentence: 37430/57013 loss: 4.723087122928627\n",
      "epoch: 0 sentence: 37440/57013 loss: 3.202822903540521\n",
      "epoch: 0 sentence: 37450/57013 loss: 3.919097581855745\n",
      "epoch: 0 sentence: 37460/57013 loss: 5.11137257743709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 37470/57013 loss: 4.384308169885831\n",
      "epoch: 0 sentence: 37480/57013 loss: 6.813721757403963\n",
      "epoch: 0 sentence: 37490/57013 loss: 4.928353845844566\n",
      "epoch: 0 sentence: 37500/57013 loss: 5.321211969882081\n",
      "epoch: 0 sentence: 37510/57013 loss: 4.594183227665082\n",
      "epoch: 0 sentence: 37520/57013 loss: 2.785337928914051\n",
      "epoch: 0 sentence: 37530/57013 loss: 4.409870773735674\n",
      "epoch: 0 sentence: 37540/57013 loss: 3.370598966060982\n",
      "epoch: 0 sentence: 37550/57013 loss: 4.61603681024573\n",
      "epoch: 0 sentence: 37560/57013 loss: 4.00524506125417\n",
      "epoch: 0 sentence: 37570/57013 loss: 6.455431993005888\n",
      "epoch: 0 sentence: 37580/57013 loss: 4.080885847533791\n",
      "epoch: 0 sentence: 37590/57013 loss: 4.116179252077307\n",
      "epoch: 0 sentence: 37600/57013 loss: 5.08820939154339\n",
      "epoch: 0 sentence: 37610/57013 loss: 3.4294203048766785\n",
      "epoch: 0 sentence: 37620/57013 loss: 3.592668043279941\n",
      "epoch: 0 sentence: 37630/57013 loss: 6.132500535752328\n",
      "epoch: 0 sentence: 37640/57013 loss: 4.2199669785596665\n",
      "epoch: 0 sentence: 37650/57013 loss: 3.5929696545794862\n",
      "epoch: 0 sentence: 37660/57013 loss: 5.5006986774428155\n",
      "epoch: 0 sentence: 37670/57013 loss: 5.404776889393063\n",
      "epoch: 0 sentence: 37680/57013 loss: 3.2321135178678064\n",
      "epoch: 0 sentence: 37690/57013 loss: 5.012268447012736\n",
      "epoch: 0 sentence: 37700/57013 loss: 3.073499241535722\n",
      "epoch: 0 sentence: 37710/57013 loss: 3.692970095746372\n",
      "epoch: 0 sentence: 37720/57013 loss: 2.8676827711826145\n",
      "epoch: 0 sentence: 37730/57013 loss: 4.883335273937007\n",
      "epoch: 0 sentence: 37740/57013 loss: 4.119373501114667\n",
      "epoch: 0 sentence: 37750/57013 loss: 4.312896048953841\n",
      "epoch: 0 sentence: 37760/57013 loss: 4.673718836343343\n",
      "epoch: 0 sentence: 37770/57013 loss: 4.484085495459109\n",
      "epoch: 0 sentence: 37780/57013 loss: 4.627957300233496\n",
      "epoch: 0 sentence: 37790/57013 loss: 4.338958511488028\n",
      "epoch: 0 sentence: 37800/57013 loss: 4.957675922878014\n",
      "epoch: 0 sentence: 37810/57013 loss: 3.8997748337609117\n",
      "epoch: 0 sentence: 37820/57013 loss: 3.828578045694468\n",
      "epoch: 0 sentence: 37830/57013 loss: 4.47879789580373\n",
      "epoch: 0 sentence: 37840/57013 loss: 3.9723355377616834\n",
      "epoch: 0 sentence: 37850/57013 loss: 3.606574472282274\n",
      "epoch: 0 sentence: 37860/57013 loss: 5.103728807935305\n",
      "epoch: 0 sentence: 37870/57013 loss: 4.49640961894934\n",
      "epoch: 0 sentence: 37880/57013 loss: 5.600743909427806\n",
      "epoch: 0 sentence: 37890/57013 loss: 5.562574310568384\n",
      "epoch: 0 sentence: 37900/57013 loss: 4.404978057207424\n",
      "epoch: 0 sentence: 37910/57013 loss: 4.973767331898064\n",
      "epoch: 0 sentence: 37920/57013 loss: 3.21287176283848\n",
      "epoch: 0 sentence: 37930/57013 loss: 2.127218322458404\n",
      "epoch: 0 sentence: 37940/57013 loss: 4.1809780043832365\n",
      "epoch: 0 sentence: 37950/57013 loss: 1.9708954581639535\n",
      "epoch: 0 sentence: 37960/57013 loss: 4.047430218670503\n",
      "epoch: 0 sentence: 37970/57013 loss: 3.8595393469776833\n",
      "epoch: 0 sentence: 37980/57013 loss: 4.211222202599829\n",
      "epoch: 0 sentence: 37990/57013 loss: 4.184614877713656\n",
      "epoch: 0 sentence: 38000/57013 loss: 4.601209390940016\n",
      "epoch: 0 sentence: 38010/57013 loss: 4.016415562322662\n",
      "epoch: 0 sentence: 38020/57013 loss: 6.549240556671176\n",
      "epoch: 0 sentence: 38030/57013 loss: 3.5219438269642427\n",
      "epoch: 0 sentence: 38040/57013 loss: 4.399222714599761\n",
      "epoch: 0 sentence: 38050/57013 loss: 3.4701017755414445\n",
      "epoch: 0 sentence: 38060/57013 loss: 5.441936169994302\n",
      "epoch: 0 sentence: 38070/57013 loss: 4.0212705622925\n",
      "epoch: 0 sentence: 38080/57013 loss: 4.140584639184206\n",
      "epoch: 0 sentence: 38090/57013 loss: 4.798317299272967\n",
      "epoch: 0 sentence: 38100/57013 loss: 5.236896558159915\n",
      "epoch: 0 sentence: 38110/57013 loss: 4.717196817943363\n",
      "epoch: 0 sentence: 38120/57013 loss: 3.9690431089221145\n",
      "epoch: 0 sentence: 38130/57013 loss: 5.353356113165095\n",
      "epoch: 0 sentence: 38140/57013 loss: 4.809056568935862\n",
      "epoch: 0 sentence: 38150/57013 loss: 4.153311032590962\n",
      "epoch: 0 sentence: 38160/57013 loss: 4.242604359577566\n",
      "epoch: 0 sentence: 38170/57013 loss: 4.624557118086535\n",
      "epoch: 0 sentence: 38180/57013 loss: 4.990996018894644\n",
      "epoch: 0 sentence: 38190/57013 loss: 5.389993859860893\n",
      "epoch: 0 sentence: 38200/57013 loss: 4.499571609807596\n",
      "epoch: 0 sentence: 38210/57013 loss: 5.314228068310784\n",
      "epoch: 0 sentence: 38220/57013 loss: 5.470988893012242\n",
      "epoch: 0 sentence: 38230/57013 loss: 4.236648580838347\n",
      "epoch: 0 sentence: 38240/57013 loss: 3.936725733256808\n",
      "epoch: 0 sentence: 38250/57013 loss: 5.0021241589334515\n",
      "epoch: 0 sentence: 38260/57013 loss: 3.8562301434324335\n",
      "epoch: 0 sentence: 38270/57013 loss: 5.0657231380391385\n",
      "epoch: 0 sentence: 38280/57013 loss: 3.9312201728890317\n",
      "epoch: 0 sentence: 38290/57013 loss: 5.109998121958425\n",
      "epoch: 0 sentence: 38300/57013 loss: 4.233613714961973\n",
      "epoch: 0 sentence: 38310/57013 loss: 4.8494037927639075\n",
      "epoch: 0 sentence: 38320/57013 loss: 5.072897219389494\n",
      "epoch: 0 sentence: 38330/57013 loss: 3.702647022999067\n",
      "epoch: 0 sentence: 38340/57013 loss: 5.164712179098468\n",
      "epoch: 0 sentence: 38350/57013 loss: 4.554058724792719\n",
      "epoch: 0 sentence: 38360/57013 loss: 3.771444985960286\n",
      "epoch: 0 sentence: 38370/57013 loss: 4.392979513217856\n",
      "epoch: 0 sentence: 38380/57013 loss: 3.827443848257138\n",
      "epoch: 0 sentence: 38390/57013 loss: 4.432721526294625\n",
      "epoch: 0 sentence: 38400/57013 loss: 3.0100927750223097\n",
      "epoch: 0 sentence: 38410/57013 loss: 4.955392164196224\n",
      "epoch: 0 sentence: 38420/57013 loss: 4.9592500190752915\n",
      "epoch: 0 sentence: 38430/57013 loss: 3.807825701324076\n",
      "epoch: 0 sentence: 38440/57013 loss: 4.745782519568246\n",
      "epoch: 0 sentence: 38450/57013 loss: 6.279678178189989\n",
      "epoch: 0 sentence: 38460/57013 loss: 5.083649022195226\n",
      "epoch: 0 sentence: 38470/57013 loss: 3.1723799349301602\n",
      "epoch: 0 sentence: 38480/57013 loss: 5.412577723858425\n",
      "epoch: 0 sentence: 38490/57013 loss: 5.16300135903411\n",
      "epoch: 0 sentence: 38500/57013 loss: 3.4882226184216782\n",
      "epoch: 0 sentence: 38510/57013 loss: 4.302137017673385\n",
      "epoch: 0 sentence: 38520/57013 loss: 4.818311539687322\n",
      "epoch: 0 sentence: 38530/57013 loss: 3.2883383026426762\n",
      "epoch: 0 sentence: 38540/57013 loss: 5.349384274041926\n",
      "epoch: 0 sentence: 38550/57013 loss: 4.669871780472883\n",
      "epoch: 0 sentence: 38560/57013 loss: 4.1583228418623\n",
      "epoch: 0 sentence: 38570/57013 loss: 5.043431721278051\n",
      "epoch: 0 sentence: 38580/57013 loss: 4.5640087934382585\n",
      "epoch: 0 sentence: 38590/57013 loss: 3.3648312002201886\n",
      "epoch: 0 sentence: 38600/57013 loss: 4.195008356678952\n",
      "epoch: 0 sentence: 38610/57013 loss: 5.624919270364131\n",
      "epoch: 0 sentence: 38620/57013 loss: 4.480397763386259\n",
      "epoch: 0 sentence: 38630/57013 loss: 4.011903295474419\n",
      "epoch: 0 sentence: 38640/57013 loss: 3.703630885244225\n",
      "epoch: 0 sentence: 38650/57013 loss: 5.132102350338522\n",
      "epoch: 0 sentence: 38660/57013 loss: 4.4840760908336\n",
      "epoch: 0 sentence: 38670/57013 loss: 3.930866706651715\n",
      "epoch: 0 sentence: 38680/57013 loss: 3.076405885764812\n",
      "epoch: 0 sentence: 38690/57013 loss: 3.572686035861474\n",
      "epoch: 0 sentence: 38700/57013 loss: 4.625059426213761\n",
      "epoch: 0 sentence: 38710/57013 loss: 3.9093536288033666\n",
      "epoch: 0 sentence: 38720/57013 loss: 4.353914653933539\n",
      "epoch: 0 sentence: 38730/57013 loss: 5.911235193196974\n",
      "epoch: 0 sentence: 38740/57013 loss: 5.624383047769284\n",
      "epoch: 0 sentence: 38750/57013 loss: 5.3389700199639325\n",
      "epoch: 0 sentence: 38760/57013 loss: 2.225085327411499\n",
      "epoch: 0 sentence: 38770/57013 loss: 3.6083098736742323\n",
      "epoch: 0 sentence: 38780/57013 loss: 4.36394665810744\n",
      "epoch: 0 sentence: 38790/57013 loss: 4.994883912546578\n",
      "epoch: 0 sentence: 38800/57013 loss: 4.974816436375431\n",
      "epoch: 0 sentence: 38810/57013 loss: 4.415106719104043\n",
      "epoch: 0 sentence: 38820/57013 loss: 4.939534637457546\n",
      "epoch: 0 sentence: 38830/57013 loss: 5.156174520501305\n",
      "epoch: 0 sentence: 38840/57013 loss: 4.470409167587303\n",
      "epoch: 0 sentence: 38850/57013 loss: 4.275045866447714\n",
      "epoch: 0 sentence: 38860/57013 loss: 4.157540287304456\n",
      "epoch: 0 sentence: 38870/57013 loss: 3.8963144506128744\n",
      "epoch: 0 sentence: 38880/57013 loss: 4.917838513074427\n",
      "epoch: 0 sentence: 38890/57013 loss: 3.8281019114520616\n",
      "epoch: 0 sentence: 38900/57013 loss: 4.932188426212431\n",
      "epoch: 0 sentence: 38910/57013 loss: 3.4922770463708983\n",
      "epoch: 0 sentence: 38920/57013 loss: 4.6421762257966135\n",
      "epoch: 0 sentence: 38930/57013 loss: 4.520856401904175\n",
      "epoch: 0 sentence: 38940/57013 loss: 4.634450180577493\n",
      "epoch: 0 sentence: 38950/57013 loss: 3.9126962548330813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 38960/57013 loss: 5.518185177711527\n",
      "epoch: 0 sentence: 38970/57013 loss: 4.824087441629579\n",
      "epoch: 0 sentence: 38980/57013 loss: 4.190396997235469\n",
      "epoch: 0 sentence: 38990/57013 loss: 4.392003373317237\n",
      "epoch: 0 sentence: 39000/57013 loss: 4.933187571314665\n",
      "epoch: 0 sentence: 39010/57013 loss: 5.323111665158428\n",
      "epoch: 0 sentence: 39020/57013 loss: 4.16322693594577\n",
      "epoch: 0 sentence: 39030/57013 loss: 4.260532686394177\n",
      "epoch: 0 sentence: 39040/57013 loss: 3.6260124279375514\n",
      "epoch: 0 sentence: 39050/57013 loss: 3.7456631357764656\n",
      "epoch: 0 sentence: 39060/57013 loss: 4.958110841683682\n",
      "epoch: 0 sentence: 39070/57013 loss: 4.669172197244774\n",
      "epoch: 0 sentence: 39080/57013 loss: 5.211295090012982\n",
      "epoch: 0 sentence: 39090/57013 loss: 4.832719438804446\n",
      "epoch: 0 sentence: 39100/57013 loss: 4.575346644494649\n",
      "epoch: 0 sentence: 39110/57013 loss: 5.4210654815426835\n",
      "epoch: 0 sentence: 39120/57013 loss: 3.8775442102316546\n",
      "epoch: 0 sentence: 39130/57013 loss: 4.792498043099387\n",
      "epoch: 0 sentence: 39140/57013 loss: 4.934994569278626\n",
      "epoch: 0 sentence: 39150/57013 loss: 4.90081643238061\n",
      "epoch: 0 sentence: 39160/57013 loss: 3.9913571239574557\n",
      "epoch: 0 sentence: 39170/57013 loss: 4.505002246579634\n",
      "epoch: 0 sentence: 39180/57013 loss: 4.053836176606612\n",
      "epoch: 0 sentence: 39190/57013 loss: 2.8552567025488114\n",
      "epoch: 0 sentence: 39200/57013 loss: 3.914888415026921\n",
      "epoch: 0 sentence: 39210/57013 loss: 5.156535213848418\n",
      "epoch: 0 sentence: 39220/57013 loss: 3.868654155971589\n",
      "epoch: 0 sentence: 39230/57013 loss: 5.392463688492073\n",
      "epoch: 0 sentence: 39240/57013 loss: 4.263147686809182\n",
      "epoch: 0 sentence: 39250/57013 loss: 3.56644675610663\n",
      "epoch: 0 sentence: 39260/57013 loss: 2.923766517055086\n",
      "epoch: 0 sentence: 39270/57013 loss: 4.599366249394512\n",
      "epoch: 0 sentence: 39280/57013 loss: 3.2232555705405677\n",
      "epoch: 0 sentence: 39290/57013 loss: 5.250603754252966\n",
      "epoch: 0 sentence: 39300/57013 loss: 4.999968052189577\n",
      "epoch: 0 sentence: 39310/57013 loss: 4.301242774362449\n",
      "epoch: 0 sentence: 39320/57013 loss: 4.652922149528343\n",
      "epoch: 0 sentence: 39330/57013 loss: 4.090545264155508\n",
      "epoch: 0 sentence: 39340/57013 loss: 5.0698284794244435\n",
      "epoch: 0 sentence: 39350/57013 loss: 4.280111513627877\n",
      "epoch: 0 sentence: 39360/57013 loss: 4.563652014585224\n",
      "epoch: 0 sentence: 39370/57013 loss: 4.866838727840438\n",
      "epoch: 0 sentence: 39380/57013 loss: 4.341139421002516\n",
      "epoch: 0 sentence: 39390/57013 loss: 4.913058369902019\n",
      "epoch: 0 sentence: 39400/57013 loss: 5.39855793064531\n",
      "epoch: 0 sentence: 39410/57013 loss: 4.5905095698696705\n",
      "epoch: 0 sentence: 39420/57013 loss: 5.038949707009516\n",
      "epoch: 0 sentence: 39430/57013 loss: 3.6177565651892176\n",
      "epoch: 0 sentence: 39440/57013 loss: 5.135572694731548\n",
      "epoch: 0 sentence: 39450/57013 loss: 3.545142276963271\n",
      "epoch: 0 sentence: 39460/57013 loss: 5.228513841127012\n",
      "epoch: 0 sentence: 39470/57013 loss: 3.888497329091964\n",
      "epoch: 0 sentence: 39480/57013 loss: 5.147320989684378\n",
      "epoch: 0 sentence: 39490/57013 loss: 4.972629243900303\n",
      "epoch: 0 sentence: 39500/57013 loss: 3.124060063243805\n",
      "epoch: 0 sentence: 39510/57013 loss: 4.605183525490699\n",
      "epoch: 0 sentence: 39520/57013 loss: 8.089801561999744\n",
      "epoch: 0 sentence: 39530/57013 loss: 4.269359754763979\n",
      "epoch: 0 sentence: 39540/57013 loss: 2.4386485787018626\n",
      "epoch: 0 sentence: 39550/57013 loss: 2.662526739314051\n",
      "epoch: 0 sentence: 39560/57013 loss: 4.0423703588005795\n",
      "epoch: 0 sentence: 39570/57013 loss: 4.620357806394606\n",
      "epoch: 0 sentence: 39580/57013 loss: 4.093067954667867\n",
      "epoch: 0 sentence: 39590/57013 loss: 4.69930174753351\n",
      "epoch: 0 sentence: 39600/57013 loss: 3.8838935447680996\n",
      "epoch: 0 sentence: 39610/57013 loss: 4.369652232733936\n",
      "epoch: 0 sentence: 39620/57013 loss: 4.6723662629842275\n",
      "epoch: 0 sentence: 39630/57013 loss: 4.722241308529212\n",
      "epoch: 0 sentence: 39640/57013 loss: 5.332961655410729\n",
      "epoch: 0 sentence: 39650/57013 loss: 5.284171123391649\n",
      "epoch: 0 sentence: 39660/57013 loss: 5.170880363379799\n",
      "epoch: 0 sentence: 39670/57013 loss: 6.4670803322026185\n",
      "epoch: 0 sentence: 39680/57013 loss: 4.9971235052448275\n",
      "epoch: 0 sentence: 39690/57013 loss: 3.8310331259413206\n",
      "epoch: 0 sentence: 39700/57013 loss: 6.481991540298497\n",
      "epoch: 0 sentence: 39710/57013 loss: 5.361285518450691\n",
      "epoch: 0 sentence: 39720/57013 loss: 3.134161361101898\n",
      "epoch: 0 sentence: 39730/57013 loss: 5.407342112729814\n",
      "epoch: 0 sentence: 39740/57013 loss: 3.7153601601978026\n",
      "epoch: 0 sentence: 39750/57013 loss: 5.0386875745651585\n",
      "epoch: 0 sentence: 39760/57013 loss: 4.473349978272602\n",
      "epoch: 0 sentence: 39770/57013 loss: 3.8718709886982032\n",
      "epoch: 0 sentence: 39780/57013 loss: 4.410693798897044\n",
      "epoch: 0 sentence: 39790/57013 loss: 3.998027461943462\n",
      "epoch: 0 sentence: 39800/57013 loss: 4.620341647410187\n",
      "epoch: 0 sentence: 39810/57013 loss: 5.748387930922489\n",
      "epoch: 0 sentence: 39820/57013 loss: 6.07852962195434\n",
      "epoch: 0 sentence: 39830/57013 loss: 6.009091219413599\n",
      "epoch: 0 sentence: 39840/57013 loss: 4.414207579945832\n",
      "epoch: 0 sentence: 39850/57013 loss: 4.1919593038758185\n",
      "epoch: 0 sentence: 39860/57013 loss: 4.500334968879859\n",
      "epoch: 0 sentence: 39870/57013 loss: 4.566053857858938\n",
      "epoch: 0 sentence: 39880/57013 loss: 5.276024848587195\n",
      "epoch: 0 sentence: 39890/57013 loss: 5.244887721478452\n",
      "epoch: 0 sentence: 39900/57013 loss: 5.248659565031326\n",
      "epoch: 0 sentence: 39910/57013 loss: 4.345990217469621\n",
      "epoch: 0 sentence: 39920/57013 loss: 4.961874737423557\n",
      "epoch: 0 sentence: 39930/57013 loss: 7.750947902384191\n",
      "epoch: 0 sentence: 39940/57013 loss: 5.689021253688084\n",
      "epoch: 0 sentence: 39950/57013 loss: 3.6087264343525782\n",
      "epoch: 0 sentence: 39960/57013 loss: 1.5403480805740208\n",
      "epoch: 0 sentence: 39970/57013 loss: 5.924951629957698\n",
      "epoch: 0 sentence: 39980/57013 loss: 4.485929622430175\n",
      "epoch: 0 sentence: 39990/57013 loss: 4.198740895170001\n",
      "epoch: 0 sentence: 40000/57013 loss: 4.907975670814287\n",
      "epoch: 0 sentence: 40010/57013 loss: 3.7101273214793054\n",
      "epoch: 0 sentence: 40020/57013 loss: 3.345127080895961\n",
      "epoch: 0 sentence: 40030/57013 loss: 4.848987259954569\n",
      "epoch: 0 sentence: 40040/57013 loss: 5.2914434882163315\n",
      "epoch: 0 sentence: 40050/57013 loss: 4.651449184220092\n",
      "epoch: 0 sentence: 40060/57013 loss: 4.333878950108332\n",
      "epoch: 0 sentence: 40070/57013 loss: 5.01488746834197\n",
      "epoch: 0 sentence: 40080/57013 loss: 5.802419883474192\n",
      "epoch: 0 sentence: 40090/57013 loss: 6.38956706850351\n",
      "epoch: 0 sentence: 40100/57013 loss: 4.6410317886779255\n",
      "epoch: 0 sentence: 40110/57013 loss: 3.967506767702991\n",
      "epoch: 0 sentence: 40120/57013 loss: 3.4455818177899453\n",
      "epoch: 0 sentence: 40130/57013 loss: 4.682152210963043\n",
      "epoch: 0 sentence: 40140/57013 loss: 4.556868912999371\n",
      "epoch: 0 sentence: 40150/57013 loss: 5.239768785545393\n",
      "epoch: 0 sentence: 40160/57013 loss: 4.969342123269709\n",
      "epoch: 0 sentence: 40170/57013 loss: 4.428348377457407\n",
      "epoch: 0 sentence: 40180/57013 loss: 5.165409297862478\n",
      "epoch: 0 sentence: 40190/57013 loss: 5.132083271993596\n",
      "epoch: 0 sentence: 40200/57013 loss: 3.653562175524825\n",
      "epoch: 0 sentence: 40210/57013 loss: 4.7688251067736465\n",
      "epoch: 0 sentence: 40220/57013 loss: 5.033005224264422\n",
      "epoch: 0 sentence: 40230/57013 loss: 3.700987009185347\n",
      "epoch: 0 sentence: 40240/57013 loss: 4.609579345801248\n",
      "epoch: 0 sentence: 40250/57013 loss: 4.202745364962567\n",
      "epoch: 0 sentence: 40260/57013 loss: 3.655717293374506\n",
      "epoch: 0 sentence: 40270/57013 loss: 3.861630998943652\n",
      "epoch: 0 sentence: 40280/57013 loss: 4.453660896732331\n",
      "epoch: 0 sentence: 40290/57013 loss: 3.901402446772965\n",
      "epoch: 0 sentence: 40300/57013 loss: 6.129847705254904\n",
      "epoch: 0 sentence: 40310/57013 loss: 3.7342161402420206\n",
      "epoch: 0 sentence: 40320/57013 loss: 2.345277641866509\n",
      "epoch: 0 sentence: 40330/57013 loss: 5.172332927485621\n",
      "epoch: 0 sentence: 40340/57013 loss: 4.026146885290627\n",
      "epoch: 0 sentence: 40350/57013 loss: 4.9863280704041\n",
      "epoch: 0 sentence: 40360/57013 loss: 3.7726828748770416\n",
      "epoch: 0 sentence: 40370/57013 loss: 4.4673821016451605\n",
      "epoch: 0 sentence: 40380/57013 loss: 4.399187555012933\n",
      "epoch: 0 sentence: 40390/57013 loss: 5.272983289245451\n",
      "epoch: 0 sentence: 40400/57013 loss: 4.850630431154924\n",
      "epoch: 0 sentence: 40410/57013 loss: 5.3444962470872115\n",
      "epoch: 0 sentence: 40420/57013 loss: 4.066004720070833\n",
      "epoch: 0 sentence: 40430/57013 loss: 4.695672834324009\n",
      "epoch: 0 sentence: 40440/57013 loss: 3.245099504838162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 40450/57013 loss: 3.4814882158833282\n",
      "epoch: 0 sentence: 40460/57013 loss: 5.409248873340307\n",
      "epoch: 0 sentence: 40470/57013 loss: 5.019378241870629\n",
      "epoch: 0 sentence: 40480/57013 loss: 5.304671135126377\n",
      "epoch: 0 sentence: 40490/57013 loss: 4.919635693562492\n",
      "epoch: 0 sentence: 40500/57013 loss: 4.406748331090511\n",
      "epoch: 0 sentence: 40510/57013 loss: 4.1397176891731835\n",
      "epoch: 0 sentence: 40520/57013 loss: 4.070908753998461\n",
      "epoch: 0 sentence: 40530/57013 loss: 5.119445597688464\n",
      "epoch: 0 sentence: 40540/57013 loss: 5.080728763407457\n",
      "epoch: 0 sentence: 40550/57013 loss: 5.078721010105575\n",
      "epoch: 0 sentence: 40560/57013 loss: 4.886462019441011\n",
      "epoch: 0 sentence: 40570/57013 loss: 4.808837295599647\n",
      "epoch: 0 sentence: 40580/57013 loss: 4.7129216403313565\n",
      "epoch: 0 sentence: 40590/57013 loss: 4.59887397586067\n",
      "epoch: 0 sentence: 40600/57013 loss: 5.431706726474404\n",
      "epoch: 0 sentence: 40610/57013 loss: 3.9436575078284553\n",
      "epoch: 0 sentence: 40620/57013 loss: 4.173752025602625\n",
      "epoch: 0 sentence: 40630/57013 loss: 4.481289005835601\n",
      "epoch: 0 sentence: 40640/57013 loss: 4.9879337785541455\n",
      "epoch: 0 sentence: 40650/57013 loss: 4.00502996058328\n",
      "epoch: 0 sentence: 40660/57013 loss: 4.095181831094488\n",
      "epoch: 0 sentence: 40670/57013 loss: 4.631687842649438\n",
      "epoch: 0 sentence: 40680/57013 loss: 3.3675186042184144\n",
      "epoch: 0 sentence: 40690/57013 loss: 4.7331619763816555\n",
      "epoch: 0 sentence: 40700/57013 loss: 6.225336056525289\n",
      "epoch: 0 sentence: 40710/57013 loss: 5.20989511013666\n",
      "epoch: 0 sentence: 40720/57013 loss: 4.751600321692246\n",
      "epoch: 0 sentence: 40730/57013 loss: 4.851548870950802\n",
      "epoch: 0 sentence: 40740/57013 loss: 5.663906823142208\n",
      "epoch: 0 sentence: 40750/57013 loss: 4.4691768842979664\n",
      "epoch: 0 sentence: 40760/57013 loss: 4.93311420999229\n",
      "epoch: 0 sentence: 40770/57013 loss: 4.405179786374414\n",
      "epoch: 0 sentence: 40780/57013 loss: 4.853822592713856\n",
      "epoch: 0 sentence: 40790/57013 loss: 5.297540699258218\n",
      "epoch: 0 sentence: 40800/57013 loss: 4.901620010239421\n",
      "epoch: 0 sentence: 40810/57013 loss: 3.663179121403332\n",
      "epoch: 0 sentence: 40820/57013 loss: 4.784833230219357\n",
      "epoch: 0 sentence: 40830/57013 loss: 5.597830204453849\n",
      "epoch: 0 sentence: 40840/57013 loss: 5.031973086758281\n",
      "epoch: 0 sentence: 40850/57013 loss: 3.507963696736716\n",
      "epoch: 0 sentence: 40860/57013 loss: 4.331975046827792\n",
      "epoch: 0 sentence: 40870/57013 loss: 5.13658824354473\n",
      "epoch: 0 sentence: 40880/57013 loss: 4.656439891373854\n",
      "epoch: 0 sentence: 40890/57013 loss: 4.943771970765085\n",
      "epoch: 0 sentence: 40900/57013 loss: 4.558577113118136\n",
      "epoch: 0 sentence: 40910/57013 loss: 2.67593601636477\n",
      "epoch: 0 sentence: 40920/57013 loss: 2.1855281569692884\n",
      "epoch: 0 sentence: 40930/57013 loss: 4.7827599354938615\n",
      "epoch: 0 sentence: 40940/57013 loss: 4.7970913105542206\n",
      "epoch: 0 sentence: 40950/57013 loss: 3.5499508329981726\n",
      "epoch: 0 sentence: 40960/57013 loss: 3.142880545496035\n",
      "epoch: 0 sentence: 40970/57013 loss: 3.9677427095309725\n",
      "epoch: 0 sentence: 40980/57013 loss: 3.8539780148906506\n",
      "epoch: 0 sentence: 40990/57013 loss: 4.937393976661981\n",
      "epoch: 0 sentence: 41000/57013 loss: 4.986331356300564\n",
      "epoch: 0 sentence: 41010/57013 loss: 4.873343129660976\n",
      "epoch: 0 sentence: 41020/57013 loss: 5.03825009335891\n",
      "epoch: 0 sentence: 41030/57013 loss: 6.344901527692805\n",
      "epoch: 0 sentence: 41040/57013 loss: 4.160809332875064\n",
      "epoch: 0 sentence: 41050/57013 loss: 5.29610935408415\n",
      "epoch: 0 sentence: 41060/57013 loss: 3.0397337155419346\n",
      "epoch: 0 sentence: 41070/57013 loss: 4.805466333163636\n",
      "epoch: 0 sentence: 41080/57013 loss: 5.095352491181751\n",
      "epoch: 0 sentence: 41090/57013 loss: 3.8399511617257622\n",
      "epoch: 0 sentence: 41100/57013 loss: 3.1564605482615655\n",
      "epoch: 0 sentence: 41110/57013 loss: 4.789414158638892\n",
      "epoch: 0 sentence: 41120/57013 loss: 4.731570634661231\n",
      "epoch: 0 sentence: 41130/57013 loss: 5.469347090755725\n",
      "epoch: 0 sentence: 41140/57013 loss: 4.808972998236568\n",
      "epoch: 0 sentence: 41150/57013 loss: 5.126011024690574\n",
      "epoch: 0 sentence: 41160/57013 loss: 2.404421952090708\n",
      "epoch: 0 sentence: 41170/57013 loss: 4.499420148747917\n",
      "epoch: 0 sentence: 41180/57013 loss: 5.422572742190996\n",
      "epoch: 0 sentence: 41190/57013 loss: 3.888712048264803\n",
      "epoch: 0 sentence: 41200/57013 loss: 4.592871127022106\n",
      "epoch: 0 sentence: 41210/57013 loss: 5.797990054705862\n",
      "epoch: 0 sentence: 41220/57013 loss: 4.9798799418069475\n",
      "epoch: 0 sentence: 41230/57013 loss: 5.934318005483374\n",
      "epoch: 0 sentence: 41240/57013 loss: 3.005447667465831\n",
      "epoch: 0 sentence: 41250/57013 loss: 3.945915495547113\n",
      "epoch: 0 sentence: 41260/57013 loss: 3.874115651667373\n",
      "epoch: 0 sentence: 41270/57013 loss: 5.114967687535361\n",
      "epoch: 0 sentence: 41280/57013 loss: 3.699144542891873\n",
      "epoch: 0 sentence: 41290/57013 loss: 3.933816094215675\n",
      "epoch: 0 sentence: 41300/57013 loss: 1.8292394921652315\n",
      "epoch: 0 sentence: 41310/57013 loss: 5.201206560419584\n",
      "epoch: 0 sentence: 41320/57013 loss: 5.199527547301903\n",
      "epoch: 0 sentence: 41330/57013 loss: 5.27053527185708\n",
      "epoch: 0 sentence: 41340/57013 loss: 5.1249497184793436\n",
      "epoch: 0 sentence: 41350/57013 loss: 4.931515484655702\n",
      "epoch: 0 sentence: 41360/57013 loss: 3.7093881148648897\n",
      "epoch: 0 sentence: 41370/57013 loss: 3.737574985703041\n",
      "epoch: 0 sentence: 41380/57013 loss: 4.0705922041447575\n",
      "epoch: 0 sentence: 41390/57013 loss: 4.63610454111423\n",
      "epoch: 0 sentence: 41400/57013 loss: 2.81732040170654\n",
      "epoch: 0 sentence: 41410/57013 loss: 3.575319706267553\n",
      "epoch: 0 sentence: 41420/57013 loss: 5.853062787635135\n",
      "epoch: 0 sentence: 41430/57013 loss: 2.060128583065134\n",
      "epoch: 0 sentence: 41440/57013 loss: 4.009336628502285\n",
      "epoch: 0 sentence: 41450/57013 loss: 4.507298492060133\n",
      "epoch: 0 sentence: 41460/57013 loss: 3.2364122125841934\n",
      "epoch: 0 sentence: 41470/57013 loss: 4.950624002529719\n",
      "epoch: 0 sentence: 41480/57013 loss: 4.765762504099291\n",
      "epoch: 0 sentence: 41490/57013 loss: 4.260672959946453\n",
      "epoch: 0 sentence: 41500/57013 loss: 3.239499941692474\n",
      "epoch: 0 sentence: 41510/57013 loss: 4.071454996008562\n",
      "epoch: 0 sentence: 41520/57013 loss: 4.405440963154389\n",
      "epoch: 0 sentence: 41530/57013 loss: 5.094687075244837\n",
      "epoch: 0 sentence: 41540/57013 loss: 4.1753065974731465\n",
      "epoch: 0 sentence: 41550/57013 loss: 5.053779815051928\n",
      "epoch: 0 sentence: 41560/57013 loss: 3.65869539139021\n",
      "epoch: 0 sentence: 41570/57013 loss: 5.2012888435592295\n",
      "epoch: 0 sentence: 41580/57013 loss: 4.374711437984901\n",
      "epoch: 0 sentence: 41590/57013 loss: 4.071248332695021\n",
      "epoch: 0 sentence: 41600/57013 loss: 5.343975515879252\n",
      "epoch: 0 sentence: 41610/57013 loss: 3.9434081742927654\n",
      "epoch: 0 sentence: 41620/57013 loss: 1.5589592817496096\n",
      "epoch: 0 sentence: 41630/57013 loss: 4.267774198107964\n",
      "epoch: 0 sentence: 41640/57013 loss: 5.045724940453039\n",
      "epoch: 0 sentence: 41650/57013 loss: 4.439139486068434\n",
      "epoch: 0 sentence: 41660/57013 loss: 3.516242810150673\n",
      "epoch: 0 sentence: 41670/57013 loss: 5.465816774261832\n",
      "epoch: 0 sentence: 41680/57013 loss: 4.851250819214329\n",
      "epoch: 0 sentence: 41690/57013 loss: 4.865945256963334\n",
      "epoch: 0 sentence: 41700/57013 loss: 5.023902713806863\n",
      "epoch: 0 sentence: 41710/57013 loss: 1.5473892654207317\n",
      "epoch: 0 sentence: 41720/57013 loss: 4.506171952293913\n",
      "epoch: 0 sentence: 41730/57013 loss: 5.263939432310423\n",
      "epoch: 0 sentence: 41740/57013 loss: 4.333880334511902\n",
      "epoch: 0 sentence: 41750/57013 loss: 4.24561325087532\n",
      "epoch: 0 sentence: 41760/57013 loss: 5.839632604193226\n",
      "epoch: 0 sentence: 41770/57013 loss: 4.856482985609005\n",
      "epoch: 0 sentence: 41780/57013 loss: 5.1163928324193435\n",
      "epoch: 0 sentence: 41790/57013 loss: 4.780679831402831\n",
      "epoch: 0 sentence: 41800/57013 loss: 5.255147837693508\n",
      "epoch: 0 sentence: 41810/57013 loss: 4.599311438865032\n",
      "epoch: 0 sentence: 41820/57013 loss: 3.902864120070908\n",
      "epoch: 0 sentence: 41830/57013 loss: 5.420033357453792\n",
      "epoch: 0 sentence: 41840/57013 loss: 5.302107761349034\n",
      "epoch: 0 sentence: 41850/57013 loss: 6.214412764774511\n",
      "epoch: 0 sentence: 41860/57013 loss: 4.467153112888792\n",
      "epoch: 0 sentence: 41870/57013 loss: 4.130458744087807\n",
      "epoch: 0 sentence: 41880/57013 loss: 4.973762951697765\n",
      "epoch: 0 sentence: 41890/57013 loss: 4.708968564012974\n",
      "epoch: 0 sentence: 41900/57013 loss: 5.1092529389657155\n",
      "epoch: 0 sentence: 41910/57013 loss: 5.969867879318048\n",
      "epoch: 0 sentence: 41920/57013 loss: 4.252863018811498\n",
      "epoch: 0 sentence: 41930/57013 loss: 5.237068899257162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 41940/57013 loss: 4.456140886778492\n",
      "epoch: 0 sentence: 41950/57013 loss: 4.722358674392537\n",
      "epoch: 0 sentence: 41960/57013 loss: 5.222967516825416\n",
      "epoch: 0 sentence: 41970/57013 loss: 5.175983552850143\n",
      "epoch: 0 sentence: 41980/57013 loss: 2.699102854390144\n",
      "epoch: 0 sentence: 41990/57013 loss: 5.090134462688812\n",
      "epoch: 0 sentence: 42000/57013 loss: 4.846223846389214\n",
      "epoch: 0 sentence: 42010/57013 loss: 4.913113892118101\n",
      "epoch: 0 sentence: 42020/57013 loss: 3.9557529271266265\n",
      "epoch: 0 sentence: 42030/57013 loss: 5.266337246837828\n",
      "epoch: 0 sentence: 42040/57013 loss: 5.898103297414591\n",
      "epoch: 0 sentence: 42050/57013 loss: 5.202503890829289\n",
      "epoch: 0 sentence: 42060/57013 loss: 3.4395335288545703\n",
      "epoch: 0 sentence: 42070/57013 loss: 4.318478356068278\n",
      "epoch: 0 sentence: 42080/57013 loss: 4.855954611414931\n",
      "epoch: 0 sentence: 42090/57013 loss: 5.2755583632024265\n",
      "epoch: 0 sentence: 42100/57013 loss: 4.48068779855081\n",
      "epoch: 0 sentence: 42110/57013 loss: 5.821345244132713\n",
      "epoch: 0 sentence: 42120/57013 loss: 4.6704141868324065\n",
      "epoch: 0 sentence: 42130/57013 loss: 4.945723460506939\n",
      "epoch: 0 sentence: 42140/57013 loss: 4.557825311048949\n",
      "epoch: 0 sentence: 42150/57013 loss: 5.540686237711191\n",
      "epoch: 0 sentence: 42160/57013 loss: 4.5409370416889825\n",
      "epoch: 0 sentence: 42170/57013 loss: 3.2473240639352703\n",
      "epoch: 0 sentence: 42180/57013 loss: 4.378935238873661\n",
      "epoch: 0 sentence: 42190/57013 loss: 5.0204942406406285\n",
      "epoch: 0 sentence: 42200/57013 loss: 5.611637025402816\n",
      "epoch: 0 sentence: 42210/57013 loss: 2.186755673287743\n",
      "epoch: 0 sentence: 42220/57013 loss: 5.083198319355034\n",
      "epoch: 0 sentence: 42230/57013 loss: 1.5612756974808402\n",
      "epoch: 0 sentence: 42240/57013 loss: 5.218871141916468\n",
      "epoch: 0 sentence: 42250/57013 loss: 5.30072855744871\n",
      "epoch: 0 sentence: 42260/57013 loss: 4.487924566778906\n",
      "epoch: 0 sentence: 42270/57013 loss: 5.081243834222128\n",
      "epoch: 0 sentence: 42280/57013 loss: 5.625624504795275\n",
      "epoch: 0 sentence: 42290/57013 loss: 4.525315082153872\n",
      "epoch: 0 sentence: 42300/57013 loss: 4.874929844308084\n",
      "epoch: 0 sentence: 42310/57013 loss: 4.555933354070263\n",
      "epoch: 0 sentence: 42320/57013 loss: 4.843750243880171\n",
      "epoch: 0 sentence: 42330/57013 loss: 4.5209094717575535\n",
      "epoch: 0 sentence: 42340/57013 loss: 4.365920308035283\n",
      "epoch: 0 sentence: 42350/57013 loss: 4.515193652262268\n",
      "epoch: 0 sentence: 42360/57013 loss: 5.787030441739342\n",
      "epoch: 0 sentence: 42370/57013 loss: 4.768984328403802\n",
      "epoch: 0 sentence: 42380/57013 loss: 5.2095974349907435\n",
      "epoch: 0 sentence: 42390/57013 loss: 4.997032258746381\n",
      "epoch: 0 sentence: 42400/57013 loss: 5.675456117953675\n",
      "epoch: 0 sentence: 42410/57013 loss: 3.9857846671128354\n",
      "epoch: 0 sentence: 42420/57013 loss: 5.787098219493979\n",
      "epoch: 0 sentence: 42430/57013 loss: 5.859589553945554\n",
      "epoch: 0 sentence: 42440/57013 loss: 2.7935931488409835\n",
      "epoch: 0 sentence: 42450/57013 loss: 4.723738718417608\n",
      "epoch: 0 sentence: 42460/57013 loss: 5.143731271131655\n",
      "epoch: 0 sentence: 42470/57013 loss: 5.586869264793952\n",
      "epoch: 0 sentence: 42480/57013 loss: 4.865018018466875\n",
      "epoch: 0 sentence: 42490/57013 loss: 5.503878553672911\n",
      "epoch: 0 sentence: 42500/57013 loss: 4.451125100831413\n",
      "epoch: 0 sentence: 42510/57013 loss: 4.085640051094401\n",
      "epoch: 0 sentence: 42520/57013 loss: 4.570140205201607\n",
      "epoch: 0 sentence: 42530/57013 loss: 4.7062412410859915\n",
      "epoch: 0 sentence: 42540/57013 loss: 4.249677446827677\n",
      "epoch: 0 sentence: 42550/57013 loss: 3.756793221672551\n",
      "epoch: 0 sentence: 42560/57013 loss: 3.228812309293311\n",
      "epoch: 0 sentence: 42570/57013 loss: 5.408263427406803\n",
      "epoch: 0 sentence: 42580/57013 loss: 4.52627433669626\n",
      "epoch: 0 sentence: 42590/57013 loss: 5.325807170794277\n",
      "epoch: 0 sentence: 42600/57013 loss: 4.5082000885494935\n",
      "epoch: 0 sentence: 42610/57013 loss: 5.219662384736453\n",
      "epoch: 0 sentence: 42620/57013 loss: 4.563357017616369\n",
      "epoch: 0 sentence: 42630/57013 loss: 5.163073052799202\n",
      "epoch: 0 sentence: 42640/57013 loss: 4.848889468491681\n",
      "epoch: 0 sentence: 42650/57013 loss: 4.629846736694729\n",
      "epoch: 0 sentence: 42660/57013 loss: 3.264344074672677\n",
      "epoch: 0 sentence: 42670/57013 loss: 3.7811028308604904\n",
      "epoch: 0 sentence: 42680/57013 loss: 4.550890060042531\n",
      "epoch: 0 sentence: 42690/57013 loss: 4.901191222918129\n",
      "epoch: 0 sentence: 42700/57013 loss: 6.0407432634157265\n",
      "epoch: 0 sentence: 42710/57013 loss: 3.3125018560158086\n",
      "epoch: 0 sentence: 42720/57013 loss: 5.584514457976393\n",
      "epoch: 0 sentence: 42730/57013 loss: 3.818103767317136\n",
      "epoch: 0 sentence: 42740/57013 loss: 5.405053106879714\n",
      "epoch: 0 sentence: 42750/57013 loss: 5.589366837840656\n",
      "epoch: 0 sentence: 42760/57013 loss: 3.803298809789493\n",
      "epoch: 0 sentence: 42770/57013 loss: 3.755902718562045\n",
      "epoch: 0 sentence: 42780/57013 loss: 4.70699595107928\n",
      "epoch: 0 sentence: 42790/57013 loss: 4.7276896357624505\n",
      "epoch: 0 sentence: 42800/57013 loss: 4.919927052769861\n",
      "epoch: 0 sentence: 42810/57013 loss: 4.507075439605327\n",
      "epoch: 0 sentence: 42820/57013 loss: 3.5723033452655035\n",
      "epoch: 0 sentence: 42830/57013 loss: 5.29801166018536\n",
      "epoch: 0 sentence: 42840/57013 loss: 4.37764689140203\n",
      "epoch: 0 sentence: 42850/57013 loss: 4.846702021866887\n",
      "epoch: 0 sentence: 42860/57013 loss: 4.75664805358167\n",
      "epoch: 0 sentence: 42870/57013 loss: 5.560357924527521\n",
      "epoch: 0 sentence: 42880/57013 loss: 4.9314307369935815\n",
      "epoch: 0 sentence: 42890/57013 loss: 4.017501121212409\n",
      "epoch: 0 sentence: 42900/57013 loss: 3.9179826473032513\n",
      "epoch: 0 sentence: 42910/57013 loss: 5.175029885548768\n",
      "epoch: 0 sentence: 42920/57013 loss: 3.126842002437412\n",
      "epoch: 0 sentence: 42930/57013 loss: 4.440573699075754\n",
      "epoch: 0 sentence: 42940/57013 loss: 5.7958152390831765\n",
      "epoch: 0 sentence: 42950/57013 loss: 6.296507451257379\n",
      "epoch: 0 sentence: 42960/57013 loss: 5.67891973016745\n",
      "epoch: 0 sentence: 42970/57013 loss: 4.181044362559793\n",
      "epoch: 0 sentence: 42980/57013 loss: 3.966735961816599\n",
      "epoch: 0 sentence: 42990/57013 loss: 5.077857124136123\n",
      "epoch: 0 sentence: 43000/57013 loss: 4.3561782773926145\n",
      "epoch: 0 sentence: 43010/57013 loss: 5.142552127059195\n",
      "epoch: 0 sentence: 43020/57013 loss: 3.880097916146559\n",
      "epoch: 0 sentence: 43030/57013 loss: 6.447118447895752\n",
      "epoch: 0 sentence: 43040/57013 loss: 3.3770390180742442\n",
      "epoch: 0 sentence: 43050/57013 loss: 4.062864233567774\n",
      "epoch: 0 sentence: 43060/57013 loss: 4.717696536653134\n",
      "epoch: 0 sentence: 43070/57013 loss: 5.104955555340019\n",
      "epoch: 0 sentence: 43080/57013 loss: 4.190313580622665\n",
      "epoch: 0 sentence: 43090/57013 loss: 6.2970895635222845\n",
      "epoch: 0 sentence: 43100/57013 loss: 4.613536008432903\n",
      "epoch: 0 sentence: 43110/57013 loss: 5.279701724275278\n",
      "epoch: 0 sentence: 43120/57013 loss: 4.6592056283800645\n",
      "epoch: 0 sentence: 43130/57013 loss: 4.921265967563245\n",
      "epoch: 0 sentence: 43140/57013 loss: 5.574953334098334\n",
      "epoch: 0 sentence: 43150/57013 loss: 5.185804534903672\n",
      "epoch: 0 sentence: 43160/57013 loss: 5.447373279696506\n",
      "epoch: 0 sentence: 43170/57013 loss: 4.02907223416939\n",
      "epoch: 0 sentence: 43180/57013 loss: 3.5964247557562254\n",
      "epoch: 0 sentence: 43190/57013 loss: 4.697671950259518\n",
      "epoch: 0 sentence: 43200/57013 loss: 4.761016515782239\n",
      "epoch: 0 sentence: 43210/57013 loss: 4.1394555421171635\n",
      "epoch: 0 sentence: 43220/57013 loss: 3.940945701765775\n",
      "epoch: 0 sentence: 43230/57013 loss: 4.8861462487728975\n",
      "epoch: 0 sentence: 43240/57013 loss: 4.656151537798883\n",
      "epoch: 0 sentence: 43250/57013 loss: 5.28312937020984\n",
      "epoch: 0 sentence: 43260/57013 loss: 3.6155363334601525\n",
      "epoch: 0 sentence: 43270/57013 loss: 4.8014745434020005\n",
      "epoch: 0 sentence: 43280/57013 loss: 3.4007940105196184\n",
      "epoch: 0 sentence: 43290/57013 loss: 6.199454402001777\n",
      "epoch: 0 sentence: 43300/57013 loss: 2.256098005895638\n",
      "epoch: 0 sentence: 43310/57013 loss: 4.900600786941694\n",
      "epoch: 0 sentence: 43320/57013 loss: 4.664832230157285\n",
      "epoch: 0 sentence: 43330/57013 loss: 5.296017920754476\n",
      "epoch: 0 sentence: 43340/57013 loss: 5.339669500267656\n",
      "epoch: 0 sentence: 43350/57013 loss: 4.561504581115802\n",
      "epoch: 0 sentence: 43360/57013 loss: 4.186699670300122\n",
      "epoch: 0 sentence: 43370/57013 loss: 4.019902653578732\n",
      "epoch: 0 sentence: 43380/57013 loss: 5.350168347084805\n",
      "epoch: 0 sentence: 43390/57013 loss: 5.090794979468984\n",
      "epoch: 0 sentence: 43400/57013 loss: 3.975982796272328\n",
      "epoch: 0 sentence: 43410/57013 loss: 4.103341133549506\n",
      "epoch: 0 sentence: 43420/57013 loss: 5.103836668420583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 43430/57013 loss: 5.828068164727106\n",
      "epoch: 0 sentence: 43440/57013 loss: 3.7538835474315686\n",
      "epoch: 0 sentence: 43450/57013 loss: 3.192090573351668\n",
      "epoch: 0 sentence: 43460/57013 loss: 5.096191501636845\n",
      "epoch: 0 sentence: 43470/57013 loss: 2.2817559611083427\n",
      "epoch: 0 sentence: 43480/57013 loss: 2.1196819805244793\n",
      "epoch: 0 sentence: 43490/57013 loss: 3.1239709215278957\n",
      "epoch: 0 sentence: 43500/57013 loss: 4.337533920708697\n",
      "epoch: 0 sentence: 43510/57013 loss: 4.657073182245296\n",
      "epoch: 0 sentence: 43520/57013 loss: 3.9649619745111897\n",
      "epoch: 0 sentence: 43530/57013 loss: 4.787220411373669\n",
      "epoch: 0 sentence: 43540/57013 loss: 4.780359889719785\n",
      "epoch: 0 sentence: 43550/57013 loss: 3.5754304427615953\n",
      "epoch: 0 sentence: 43560/57013 loss: 3.0671079808289505\n",
      "epoch: 0 sentence: 43570/57013 loss: 5.1042500259756665\n",
      "epoch: 0 sentence: 43580/57013 loss: 5.2869085866151195\n",
      "epoch: 0 sentence: 43590/57013 loss: 5.216058370547976\n",
      "epoch: 0 sentence: 43600/57013 loss: 3.3741270046783467\n",
      "epoch: 0 sentence: 43610/57013 loss: 4.6162224932111116\n",
      "epoch: 0 sentence: 43620/57013 loss: 4.914978982143736\n",
      "epoch: 0 sentence: 43630/57013 loss: 4.407585411223536\n",
      "epoch: 0 sentence: 43640/57013 loss: 3.600555260439972\n",
      "epoch: 0 sentence: 43650/57013 loss: 2.635461654165959\n",
      "epoch: 0 sentence: 43660/57013 loss: 4.983274328531069\n",
      "epoch: 0 sentence: 43670/57013 loss: 4.501135102005124\n",
      "epoch: 0 sentence: 43680/57013 loss: 2.064376069123546\n",
      "epoch: 0 sentence: 43690/57013 loss: 4.672057354529563\n",
      "epoch: 0 sentence: 43700/57013 loss: 4.908769785017502\n",
      "epoch: 0 sentence: 43710/57013 loss: 4.147050085033957\n",
      "epoch: 0 sentence: 43720/57013 loss: 3.8114970402512793\n",
      "epoch: 0 sentence: 43730/57013 loss: 5.18101929567402\n",
      "epoch: 0 sentence: 43740/57013 loss: 4.731894050999795\n",
      "epoch: 0 sentence: 43750/57013 loss: 6.16286515546676\n",
      "epoch: 0 sentence: 43760/57013 loss: 3.7780289404373777\n",
      "epoch: 0 sentence: 43770/57013 loss: 4.131430091937367\n",
      "epoch: 0 sentence: 43780/57013 loss: 3.754044289064336\n",
      "epoch: 0 sentence: 43790/57013 loss: 5.476630783163945\n",
      "epoch: 0 sentence: 43800/57013 loss: 5.095460391713578\n",
      "epoch: 0 sentence: 43810/57013 loss: 5.108516727816439\n",
      "epoch: 0 sentence: 43820/57013 loss: 4.47860136799668\n",
      "epoch: 0 sentence: 43830/57013 loss: 5.218284795133353\n",
      "epoch: 0 sentence: 43840/57013 loss: 3.942541482733986\n",
      "epoch: 0 sentence: 43850/57013 loss: 4.447384160671813\n",
      "epoch: 0 sentence: 43860/57013 loss: 3.141402884402934\n",
      "epoch: 0 sentence: 43870/57013 loss: 2.7697204513149405\n",
      "epoch: 0 sentence: 43880/57013 loss: 4.130883213729588\n",
      "epoch: 0 sentence: 43890/57013 loss: 5.267843769799263\n",
      "epoch: 0 sentence: 43900/57013 loss: 4.24568142002017\n",
      "epoch: 0 sentence: 43910/57013 loss: 5.1552341680845135\n",
      "epoch: 0 sentence: 43920/57013 loss: 3.679201343613679\n",
      "epoch: 0 sentence: 43930/57013 loss: 4.154909168680377\n",
      "epoch: 0 sentence: 43940/57013 loss: 3.878449276157151\n",
      "epoch: 0 sentence: 43950/57013 loss: 3.887825037598778\n",
      "epoch: 0 sentence: 43960/57013 loss: 4.919619034997131\n",
      "epoch: 0 sentence: 43970/57013 loss: 4.481413833525642\n",
      "epoch: 0 sentence: 43980/57013 loss: 2.4321709911428324\n",
      "epoch: 0 sentence: 43990/57013 loss: 1.5876514974563025\n",
      "epoch: 0 sentence: 44000/57013 loss: 3.403007647591233\n",
      "epoch: 0 sentence: 44010/57013 loss: 4.417482458520334\n",
      "epoch: 0 sentence: 44020/57013 loss: 4.335163941728232\n",
      "epoch: 0 sentence: 44030/57013 loss: 5.524641371201575\n",
      "epoch: 0 sentence: 44040/57013 loss: 4.015559363172448\n",
      "epoch: 0 sentence: 44050/57013 loss: 4.7480820142256945\n",
      "epoch: 0 sentence: 44060/57013 loss: 4.746232869236847\n",
      "epoch: 0 sentence: 44070/57013 loss: 5.342551654878016\n",
      "epoch: 0 sentence: 44080/57013 loss: 3.6319071854321434\n",
      "epoch: 0 sentence: 44090/57013 loss: 4.341647118171485\n",
      "epoch: 0 sentence: 44100/57013 loss: 3.85185075807225\n",
      "epoch: 0 sentence: 44110/57013 loss: 5.2770893586542265\n",
      "epoch: 0 sentence: 44120/57013 loss: 4.409116056741859\n",
      "epoch: 0 sentence: 44130/57013 loss: 4.336292061473768\n",
      "epoch: 0 sentence: 44140/57013 loss: 4.074838217030075\n",
      "epoch: 0 sentence: 44150/57013 loss: 5.4006075106960525\n",
      "epoch: 0 sentence: 44160/57013 loss: 5.443257286650506\n",
      "epoch: 0 sentence: 44170/57013 loss: 2.7771716660799686\n",
      "epoch: 0 sentence: 44180/57013 loss: 4.333977385765377\n",
      "epoch: 0 sentence: 44190/57013 loss: 4.432363317924489\n",
      "epoch: 0 sentence: 44200/57013 loss: 4.233790790541838\n",
      "epoch: 0 sentence: 44210/57013 loss: 5.452038543521093\n",
      "epoch: 0 sentence: 44220/57013 loss: 4.317878566278561\n",
      "epoch: 0 sentence: 44230/57013 loss: 4.702983104706724\n",
      "epoch: 0 sentence: 44240/57013 loss: 4.596892063076697\n",
      "epoch: 0 sentence: 44250/57013 loss: 4.185421441182774\n",
      "epoch: 0 sentence: 44260/57013 loss: 4.948297293154836\n",
      "epoch: 0 sentence: 44270/57013 loss: 3.98766541055864\n",
      "epoch: 0 sentence: 44280/57013 loss: 5.2096358281363715\n",
      "epoch: 0 sentence: 44290/57013 loss: 4.630886985986946\n",
      "epoch: 0 sentence: 44300/57013 loss: 4.0296644471504\n",
      "epoch: 0 sentence: 44310/57013 loss: 5.683284403927371\n",
      "epoch: 0 sentence: 44320/57013 loss: 3.677345102778101\n",
      "epoch: 0 sentence: 44330/57013 loss: 5.01106967447872\n",
      "epoch: 0 sentence: 44340/57013 loss: 5.038425352104412\n",
      "epoch: 0 sentence: 44350/57013 loss: 5.838008762724371\n",
      "epoch: 0 sentence: 44360/57013 loss: 5.685246648843193\n",
      "epoch: 0 sentence: 44370/57013 loss: 4.285021425751645\n",
      "epoch: 0 sentence: 44380/57013 loss: 5.0144428489672075\n",
      "epoch: 0 sentence: 44390/57013 loss: 5.665458745454626\n",
      "epoch: 0 sentence: 44400/57013 loss: 5.044883486233068\n",
      "epoch: 0 sentence: 44410/57013 loss: 5.275133322755065\n",
      "epoch: 0 sentence: 44420/57013 loss: 3.344078946948847\n",
      "epoch: 0 sentence: 44430/57013 loss: 4.655775905479504\n",
      "epoch: 0 sentence: 44440/57013 loss: 3.9571157287220107\n",
      "epoch: 0 sentence: 44450/57013 loss: 4.874497130611916\n",
      "epoch: 0 sentence: 44460/57013 loss: 3.8109355550573047\n",
      "epoch: 0 sentence: 44470/57013 loss: 4.337252331564558\n",
      "epoch: 0 sentence: 44480/57013 loss: 5.955869872160197\n",
      "epoch: 0 sentence: 44490/57013 loss: 4.188114702263512\n",
      "epoch: 0 sentence: 44500/57013 loss: 3.104235012315306\n",
      "epoch: 0 sentence: 44510/57013 loss: 4.686881900644262\n",
      "epoch: 0 sentence: 44520/57013 loss: 5.110074424947609\n",
      "epoch: 0 sentence: 44530/57013 loss: 4.1366038676182475\n",
      "epoch: 0 sentence: 44540/57013 loss: 4.012211631210403\n",
      "epoch: 0 sentence: 44550/57013 loss: 5.391977552187753\n",
      "epoch: 0 sentence: 44560/57013 loss: 4.7906938711857565\n",
      "epoch: 0 sentence: 44570/57013 loss: 5.225145490688949\n",
      "epoch: 0 sentence: 44580/57013 loss: 5.354193313951404\n",
      "epoch: 0 sentence: 44590/57013 loss: 3.1552129279806174\n",
      "epoch: 0 sentence: 44600/57013 loss: 4.538236285341246\n",
      "epoch: 0 sentence: 44610/57013 loss: 3.636684437395568\n",
      "epoch: 0 sentence: 44620/57013 loss: 5.172321728520313\n",
      "epoch: 0 sentence: 44630/57013 loss: 3.2293321119882736\n",
      "epoch: 0 sentence: 44640/57013 loss: 4.343451890581859\n",
      "epoch: 0 sentence: 44650/57013 loss: 5.053493969692362\n",
      "epoch: 0 sentence: 44660/57013 loss: 6.03030591963745\n",
      "epoch: 0 sentence: 44670/57013 loss: 4.065919997605028\n",
      "epoch: 0 sentence: 44680/57013 loss: 3.53769205684489\n",
      "epoch: 0 sentence: 44690/57013 loss: 3.9593624038622153\n",
      "epoch: 0 sentence: 44700/57013 loss: 4.200882295491879\n",
      "epoch: 0 sentence: 44710/57013 loss: 5.173984361448957\n",
      "epoch: 0 sentence: 44720/57013 loss: 4.430122053128603\n",
      "epoch: 0 sentence: 44730/57013 loss: 5.307267795261518\n",
      "epoch: 0 sentence: 44740/57013 loss: 4.9656057542206735\n",
      "epoch: 0 sentence: 44750/57013 loss: 4.00908148757897\n",
      "epoch: 0 sentence: 44760/57013 loss: 4.175927933823066\n",
      "epoch: 0 sentence: 44770/57013 loss: 4.090807058243095\n",
      "epoch: 0 sentence: 44780/57013 loss: 4.7118709925729005\n",
      "epoch: 0 sentence: 44790/57013 loss: 3.9455835305463567\n",
      "epoch: 0 sentence: 44800/57013 loss: 4.963640525148183\n",
      "epoch: 0 sentence: 44810/57013 loss: 2.5821629090133795\n",
      "epoch: 0 sentence: 44820/57013 loss: 4.023504803841594\n",
      "epoch: 0 sentence: 44830/57013 loss: 3.8922494822771823\n",
      "epoch: 0 sentence: 44840/57013 loss: 4.828586079166135\n",
      "epoch: 0 sentence: 44850/57013 loss: 3.447637413018801\n",
      "epoch: 0 sentence: 44860/57013 loss: 3.3177203594093516\n",
      "epoch: 0 sentence: 44870/57013 loss: 2.7794910991221027\n",
      "epoch: 0 sentence: 44880/57013 loss: 5.263619268464513\n",
      "epoch: 0 sentence: 44890/57013 loss: 5.5902735898632425\n",
      "epoch: 0 sentence: 44900/57013 loss: 3.686592643415014\n",
      "epoch: 0 sentence: 44910/57013 loss: 3.8875401399073173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 44920/57013 loss: 5.51722510248857\n",
      "epoch: 0 sentence: 44930/57013 loss: 4.235120985451773\n",
      "epoch: 0 sentence: 44940/57013 loss: 5.041738221652713\n",
      "epoch: 0 sentence: 44950/57013 loss: 4.113026976032983\n",
      "epoch: 0 sentence: 44960/57013 loss: 4.258883805899832\n",
      "epoch: 0 sentence: 44970/57013 loss: 5.476278818245908\n",
      "epoch: 0 sentence: 44980/57013 loss: 2.5633950338371525\n",
      "epoch: 0 sentence: 44990/57013 loss: 3.0424663829885\n",
      "epoch: 0 sentence: 45000/57013 loss: 4.726621525099896\n",
      "epoch: 0 sentence: 45010/57013 loss: 4.250108314143334\n",
      "epoch: 0 sentence: 45020/57013 loss: 4.453435147822165\n",
      "epoch: 0 sentence: 45030/57013 loss: 4.5656028634294215\n",
      "epoch: 0 sentence: 45040/57013 loss: 5.787242085138005\n",
      "epoch: 0 sentence: 45050/57013 loss: 3.2969983742826403\n",
      "epoch: 0 sentence: 45060/57013 loss: 3.8200960803561816\n",
      "epoch: 0 sentence: 45070/57013 loss: 5.475762505768489\n",
      "epoch: 0 sentence: 45080/57013 loss: 3.913011013190938\n",
      "epoch: 0 sentence: 45090/57013 loss: 4.724772570545643\n",
      "epoch: 0 sentence: 45100/57013 loss: 4.326740449969052\n",
      "epoch: 0 sentence: 45110/57013 loss: 5.447148956292927\n",
      "epoch: 0 sentence: 45120/57013 loss: 5.1117480136715985\n",
      "epoch: 0 sentence: 45130/57013 loss: 4.869635671638554\n",
      "epoch: 0 sentence: 45140/57013 loss: 4.966844550138804\n",
      "epoch: 0 sentence: 45150/57013 loss: 5.008041943494342\n",
      "epoch: 0 sentence: 45160/57013 loss: 4.4362820103536365\n",
      "epoch: 0 sentence: 45170/57013 loss: 4.085954163540652\n",
      "epoch: 0 sentence: 45180/57013 loss: 4.944067309700805\n",
      "epoch: 0 sentence: 45190/57013 loss: 3.8294219903478024\n",
      "epoch: 0 sentence: 45200/57013 loss: 5.741112799841801\n",
      "epoch: 0 sentence: 45210/57013 loss: 2.135820259822593\n",
      "epoch: 0 sentence: 45220/57013 loss: 5.717274777257906\n",
      "epoch: 0 sentence: 45230/57013 loss: 5.638014525038422\n",
      "epoch: 0 sentence: 45240/57013 loss: 4.435796668633361\n",
      "epoch: 0 sentence: 45250/57013 loss: 5.7112418361460655\n",
      "epoch: 0 sentence: 45260/57013 loss: 5.868292157680778\n",
      "epoch: 0 sentence: 45270/57013 loss: 3.629129290674381\n",
      "epoch: 0 sentence: 45280/57013 loss: 5.462977404397264\n",
      "epoch: 0 sentence: 45290/57013 loss: 6.656931665602066\n",
      "epoch: 0 sentence: 45300/57013 loss: 5.17665824311312\n",
      "epoch: 0 sentence: 45310/57013 loss: 4.057997435149199\n",
      "epoch: 0 sentence: 45320/57013 loss: 4.78750022135828\n",
      "epoch: 0 sentence: 45330/57013 loss: 5.01418225930026\n",
      "epoch: 0 sentence: 45340/57013 loss: 5.034175718774597\n",
      "epoch: 0 sentence: 45350/57013 loss: 4.865561337724153\n",
      "epoch: 0 sentence: 45360/57013 loss: 5.071870349084534\n",
      "epoch: 0 sentence: 45370/57013 loss: 5.5294180756561175\n",
      "epoch: 0 sentence: 45380/57013 loss: 4.542947325560358\n",
      "epoch: 0 sentence: 45390/57013 loss: 4.5607747236165\n",
      "epoch: 0 sentence: 45400/57013 loss: 4.886877203810896\n",
      "epoch: 0 sentence: 45410/57013 loss: 4.437575824081966\n",
      "epoch: 0 sentence: 45420/57013 loss: 4.129072052989084\n",
      "epoch: 0 sentence: 45430/57013 loss: 4.140996275311488\n",
      "epoch: 0 sentence: 45440/57013 loss: 5.2971194423712795\n",
      "epoch: 0 sentence: 45450/57013 loss: 4.475884894508975\n",
      "epoch: 0 sentence: 45460/57013 loss: 4.431040679306185\n",
      "epoch: 0 sentence: 45470/57013 loss: 3.65327756022591\n",
      "epoch: 0 sentence: 45480/57013 loss: 5.1820864913436475\n",
      "epoch: 0 sentence: 45490/57013 loss: 3.6531171843445045\n",
      "epoch: 0 sentence: 45500/57013 loss: 4.712364812553407\n",
      "epoch: 0 sentence: 45510/57013 loss: 4.1981286355532\n",
      "epoch: 0 sentence: 45520/57013 loss: 2.6829364003297687\n",
      "epoch: 0 sentence: 45530/57013 loss: 5.118885096409789\n",
      "epoch: 0 sentence: 45540/57013 loss: 3.812194683140684\n",
      "epoch: 0 sentence: 45550/57013 loss: 3.1677850843260384\n",
      "epoch: 0 sentence: 45560/57013 loss: 4.162945591046331\n",
      "epoch: 0 sentence: 45570/57013 loss: 3.7891908141590664\n",
      "epoch: 0 sentence: 45580/57013 loss: 4.798601087369734\n",
      "epoch: 0 sentence: 45590/57013 loss: 3.413128637988614\n",
      "epoch: 0 sentence: 45600/57013 loss: 4.856678009158397\n",
      "epoch: 0 sentence: 45610/57013 loss: 5.788486359093728\n",
      "epoch: 0 sentence: 45620/57013 loss: 5.156941513487995\n",
      "epoch: 0 sentence: 45630/57013 loss: 4.405010098953016\n",
      "epoch: 0 sentence: 45640/57013 loss: 4.903666305858502\n",
      "epoch: 0 sentence: 45650/57013 loss: 4.5096361892395045\n",
      "epoch: 0 sentence: 45660/57013 loss: 5.798131281037585\n",
      "epoch: 0 sentence: 45670/57013 loss: 3.7865844934879256\n",
      "epoch: 0 sentence: 45680/57013 loss: 5.587327523815452\n",
      "epoch: 0 sentence: 45690/57013 loss: 4.166219488711805\n",
      "epoch: 0 sentence: 45700/57013 loss: 4.578986245855031\n",
      "epoch: 0 sentence: 45710/57013 loss: 5.390164758769915\n",
      "epoch: 0 sentence: 45720/57013 loss: 4.759620405916148\n",
      "epoch: 0 sentence: 45730/57013 loss: 5.079808939466462\n",
      "epoch: 0 sentence: 45740/57013 loss: 4.734270806042416\n",
      "epoch: 0 sentence: 45750/57013 loss: 5.0533181785000965\n",
      "epoch: 0 sentence: 45760/57013 loss: 4.57723352118777\n",
      "epoch: 0 sentence: 45770/57013 loss: 3.8088805330049977\n",
      "epoch: 0 sentence: 45780/57013 loss: 3.328985750462166\n",
      "epoch: 0 sentence: 45790/57013 loss: 4.666195758694486\n",
      "epoch: 0 sentence: 45800/57013 loss: 4.204232071783766\n",
      "epoch: 0 sentence: 45810/57013 loss: 4.323548597223357\n",
      "epoch: 0 sentence: 45820/57013 loss: 5.237351389732136\n",
      "epoch: 0 sentence: 45830/57013 loss: 4.341162520863602\n",
      "epoch: 0 sentence: 45840/57013 loss: 5.431891766797668\n",
      "epoch: 0 sentence: 45850/57013 loss: 4.860044024173207\n",
      "epoch: 0 sentence: 45860/57013 loss: 5.443505864241213\n",
      "epoch: 0 sentence: 45870/57013 loss: 3.0236633861799933\n",
      "epoch: 0 sentence: 45880/57013 loss: 4.3734757603111225\n",
      "epoch: 0 sentence: 45890/57013 loss: 5.464225149088582\n",
      "epoch: 0 sentence: 45900/57013 loss: 4.612460869761447\n",
      "epoch: 0 sentence: 45910/57013 loss: 4.02446223869039\n",
      "epoch: 0 sentence: 45920/57013 loss: 3.884259246962884\n",
      "epoch: 0 sentence: 45930/57013 loss: 4.992098304343893\n",
      "epoch: 0 sentence: 45940/57013 loss: 3.029246980878989\n",
      "epoch: 0 sentence: 45950/57013 loss: 4.538430080430558\n",
      "epoch: 0 sentence: 45960/57013 loss: 2.755979609196828\n",
      "epoch: 0 sentence: 45970/57013 loss: 6.153914616939459\n",
      "epoch: 0 sentence: 45980/57013 loss: 7.771004240665076\n",
      "epoch: 0 sentence: 45990/57013 loss: 5.925092025072326\n",
      "epoch: 0 sentence: 46000/57013 loss: 5.967395159508092\n",
      "epoch: 0 sentence: 46010/57013 loss: 4.13379284993526\n",
      "epoch: 0 sentence: 46020/57013 loss: 4.93808815564669\n",
      "epoch: 0 sentence: 46030/57013 loss: 5.196194926485619\n",
      "epoch: 0 sentence: 46040/57013 loss: 4.93108464515951\n",
      "epoch: 0 sentence: 46050/57013 loss: 4.866862069884126\n",
      "epoch: 0 sentence: 46060/57013 loss: 4.445159448335893\n",
      "epoch: 0 sentence: 46070/57013 loss: 5.630020513390422\n",
      "epoch: 0 sentence: 46080/57013 loss: 4.35020744121028\n",
      "epoch: 0 sentence: 46090/57013 loss: 4.614748309433419\n",
      "epoch: 0 sentence: 46100/57013 loss: 4.631631194410322\n",
      "epoch: 0 sentence: 46110/57013 loss: 4.154573880430035\n",
      "epoch: 0 sentence: 46120/57013 loss: 5.385245124390072\n",
      "epoch: 0 sentence: 46130/57013 loss: 4.412190861709244\n",
      "epoch: 0 sentence: 46140/57013 loss: 4.922260234041572\n",
      "epoch: 0 sentence: 46150/57013 loss: 3.441804777653253\n",
      "epoch: 0 sentence: 46160/57013 loss: 5.165268523264388\n",
      "epoch: 0 sentence: 46170/57013 loss: 5.725111418993506\n",
      "epoch: 0 sentence: 46180/57013 loss: 5.6067846312924505\n",
      "epoch: 0 sentence: 46190/57013 loss: 4.760544780502289\n",
      "epoch: 0 sentence: 46200/57013 loss: 3.4320870491395645\n",
      "epoch: 0 sentence: 46210/57013 loss: 5.343574832489557\n",
      "epoch: 0 sentence: 46220/57013 loss: 4.253857804638251\n",
      "epoch: 0 sentence: 46230/57013 loss: 3.90768591265113\n",
      "epoch: 0 sentence: 46240/57013 loss: 6.001161960944248\n",
      "epoch: 0 sentence: 46250/57013 loss: 5.539490248987426\n",
      "epoch: 0 sentence: 46260/57013 loss: 4.825883556358193\n",
      "epoch: 0 sentence: 46270/57013 loss: 4.259857018004821\n",
      "epoch: 0 sentence: 46280/57013 loss: 3.6255977923596094\n",
      "epoch: 0 sentence: 46290/57013 loss: 4.324664745360531\n",
      "epoch: 0 sentence: 46300/57013 loss: 4.499736178483495\n",
      "epoch: 0 sentence: 46310/57013 loss: 3.9148320801744094\n",
      "epoch: 0 sentence: 46320/57013 loss: 3.869846496211589\n",
      "epoch: 0 sentence: 46330/57013 loss: 5.04692163612518\n",
      "epoch: 0 sentence: 46340/57013 loss: 5.1139219631566855\n",
      "epoch: 0 sentence: 46350/57013 loss: 4.840583759171576\n",
      "epoch: 0 sentence: 46360/57013 loss: 5.288630313249676\n",
      "epoch: 0 sentence: 46370/57013 loss: 4.856956236829686\n",
      "epoch: 0 sentence: 46380/57013 loss: 3.8159317371960797\n",
      "epoch: 0 sentence: 46390/57013 loss: 4.396559312692335\n",
      "epoch: 0 sentence: 46400/57013 loss: 5.245529228014274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 46410/57013 loss: 5.570343744712716\n",
      "epoch: 0 sentence: 46420/57013 loss: 4.151711775924295\n",
      "epoch: 0 sentence: 46430/57013 loss: 4.551655972646448\n",
      "epoch: 0 sentence: 46440/57013 loss: 4.519423299915774\n",
      "epoch: 0 sentence: 46450/57013 loss: 4.199944992408147\n",
      "epoch: 0 sentence: 46460/57013 loss: 5.746891391746245\n",
      "epoch: 0 sentence: 46470/57013 loss: 4.0914944569455685\n",
      "epoch: 0 sentence: 46480/57013 loss: 5.926959057825944\n",
      "epoch: 0 sentence: 46490/57013 loss: 5.1428639570272505\n",
      "epoch: 0 sentence: 46500/57013 loss: 3.552911404098786\n",
      "epoch: 0 sentence: 46510/57013 loss: 4.808066746977232\n",
      "epoch: 0 sentence: 46520/57013 loss: 2.994800175281847\n",
      "epoch: 0 sentence: 46530/57013 loss: 2.7555404484908927\n",
      "epoch: 0 sentence: 46540/57013 loss: 4.429430546032986\n",
      "epoch: 0 sentence: 46550/57013 loss: 3.226690650663414\n",
      "epoch: 0 sentence: 46560/57013 loss: 3.082291010614334\n",
      "epoch: 0 sentence: 46570/57013 loss: 5.466074735950278\n",
      "epoch: 0 sentence: 46580/57013 loss: 4.897410804287375\n",
      "epoch: 0 sentence: 46590/57013 loss: 5.307852270959348\n",
      "epoch: 0 sentence: 46600/57013 loss: 5.125665029109904\n",
      "epoch: 0 sentence: 46610/57013 loss: 5.56158520707942\n",
      "epoch: 0 sentence: 46620/57013 loss: 4.611616486563346\n",
      "epoch: 0 sentence: 46630/57013 loss: 4.424490019422253\n",
      "epoch: 0 sentence: 46640/57013 loss: 5.879839589839792\n",
      "epoch: 0 sentence: 46650/57013 loss: 2.4360787414728398\n",
      "epoch: 0 sentence: 46660/57013 loss: 4.26038359024849\n",
      "epoch: 0 sentence: 46670/57013 loss: 5.507044801022561\n",
      "epoch: 0 sentence: 46680/57013 loss: 4.056331154831716\n",
      "epoch: 0 sentence: 46690/57013 loss: 4.556147392320529\n",
      "epoch: 0 sentence: 46700/57013 loss: 3.370066911188509\n",
      "epoch: 0 sentence: 46710/57013 loss: 4.326496833393139\n",
      "epoch: 0 sentence: 46720/57013 loss: 5.3393821301302165\n",
      "epoch: 0 sentence: 46730/57013 loss: 3.976428040038838\n",
      "epoch: 0 sentence: 46740/57013 loss: 5.12199504104003\n",
      "epoch: 0 sentence: 46750/57013 loss: 5.375497439253087\n",
      "epoch: 0 sentence: 46760/57013 loss: 4.61793794794384\n",
      "epoch: 0 sentence: 46770/57013 loss: 3.7281305360750916\n",
      "epoch: 0 sentence: 46780/57013 loss: 4.920999633035142\n",
      "epoch: 0 sentence: 46790/57013 loss: 4.015506313058866\n",
      "epoch: 0 sentence: 46800/57013 loss: 4.586784099603145\n",
      "epoch: 0 sentence: 46810/57013 loss: 4.934812152328522\n",
      "epoch: 0 sentence: 46820/57013 loss: 5.418905153095171\n",
      "epoch: 0 sentence: 46830/57013 loss: 3.4601436280861084\n",
      "epoch: 0 sentence: 46840/57013 loss: 4.22729689972904\n",
      "epoch: 0 sentence: 46850/57013 loss: 5.051487099106274\n",
      "epoch: 0 sentence: 46860/57013 loss: 2.1773192563521686\n",
      "epoch: 0 sentence: 46870/57013 loss: 4.202192858907396\n",
      "epoch: 0 sentence: 46880/57013 loss: 4.137130380551477\n",
      "epoch: 0 sentence: 46890/57013 loss: 4.3063970956162025\n",
      "epoch: 0 sentence: 46900/57013 loss: 3.2982509102305255\n",
      "epoch: 0 sentence: 46910/57013 loss: 5.139413352987372\n",
      "epoch: 0 sentence: 46920/57013 loss: 4.20477796208962\n",
      "epoch: 0 sentence: 46930/57013 loss: 5.041313472162094\n",
      "epoch: 0 sentence: 46940/57013 loss: 4.949259326967431\n",
      "epoch: 0 sentence: 46950/57013 loss: 4.824208911153433\n",
      "epoch: 0 sentence: 46960/57013 loss: 3.6162415616270853\n",
      "epoch: 0 sentence: 46970/57013 loss: 5.07052405790666\n",
      "epoch: 0 sentence: 46980/57013 loss: 4.838621941670149\n",
      "epoch: 0 sentence: 46990/57013 loss: 3.8762405667467386\n",
      "epoch: 0 sentence: 47000/57013 loss: 5.77398051888254\n",
      "epoch: 0 sentence: 47010/57013 loss: 5.47531357340546\n",
      "epoch: 0 sentence: 47020/57013 loss: 4.783005541508016\n",
      "epoch: 0 sentence: 47030/57013 loss: 5.266426683672636\n",
      "epoch: 0 sentence: 47040/57013 loss: 4.9592375260413615\n",
      "epoch: 0 sentence: 47050/57013 loss: 5.100041094876786\n",
      "epoch: 0 sentence: 47060/57013 loss: 3.8948967510662293\n",
      "epoch: 0 sentence: 47070/57013 loss: 3.7787397002873746\n",
      "epoch: 0 sentence: 47080/57013 loss: 3.96065956543906\n",
      "epoch: 0 sentence: 47090/57013 loss: 2.899903738960002\n",
      "epoch: 0 sentence: 47100/57013 loss: 4.3012952925384695\n",
      "epoch: 0 sentence: 47110/57013 loss: 5.465056713709917\n",
      "epoch: 0 sentence: 47120/57013 loss: 4.218669090056037\n",
      "epoch: 0 sentence: 47130/57013 loss: 4.209419519767946\n",
      "epoch: 0 sentence: 47140/57013 loss: 4.162593411916538\n",
      "epoch: 0 sentence: 47150/57013 loss: 5.07861091542012\n",
      "epoch: 0 sentence: 47160/57013 loss: 4.633693512035127\n",
      "epoch: 0 sentence: 47170/57013 loss: 4.077524492799108\n",
      "epoch: 0 sentence: 47180/57013 loss: 3.7468981546120443\n",
      "epoch: 0 sentence: 47190/57013 loss: 5.3868546033685725\n",
      "epoch: 0 sentence: 47200/57013 loss: 3.356305142866262\n",
      "epoch: 0 sentence: 47210/57013 loss: 5.493406433137446\n",
      "epoch: 0 sentence: 47220/57013 loss: 4.29551207217464\n",
      "epoch: 0 sentence: 47230/57013 loss: 5.978503608659898\n",
      "epoch: 0 sentence: 47240/57013 loss: 3.6235007198678892\n",
      "epoch: 0 sentence: 47250/57013 loss: 4.727375975680205\n",
      "epoch: 0 sentence: 47260/57013 loss: 4.654313472662028\n",
      "epoch: 0 sentence: 47270/57013 loss: 4.260470094298905\n",
      "epoch: 0 sentence: 47280/57013 loss: 4.427677083116114\n",
      "epoch: 0 sentence: 47290/57013 loss: 4.42075601486799\n",
      "epoch: 0 sentence: 47300/57013 loss: 5.136716209231917\n",
      "epoch: 0 sentence: 47310/57013 loss: 4.381521471508308\n",
      "epoch: 0 sentence: 47320/57013 loss: 3.8543066832310253\n",
      "epoch: 0 sentence: 47330/57013 loss: 4.788394067553081\n",
      "epoch: 0 sentence: 47340/57013 loss: 5.04597982078128\n",
      "epoch: 0 sentence: 47350/57013 loss: 4.938079933823789\n",
      "epoch: 0 sentence: 47360/57013 loss: 5.075678294210455\n",
      "epoch: 0 sentence: 47370/57013 loss: 5.74295541621497\n",
      "epoch: 0 sentence: 47380/57013 loss: 3.812445266077523\n",
      "epoch: 0 sentence: 47390/57013 loss: 6.05605998575816\n",
      "epoch: 0 sentence: 47400/57013 loss: 4.420345882364834\n",
      "epoch: 0 sentence: 47410/57013 loss: 4.180991670154293\n",
      "epoch: 0 sentence: 47420/57013 loss: 5.468814026755552\n",
      "epoch: 0 sentence: 47430/57013 loss: 5.468109465510243\n",
      "epoch: 0 sentence: 47440/57013 loss: 3.7222022526081404\n",
      "epoch: 0 sentence: 47450/57013 loss: 2.6743381260631915\n",
      "epoch: 0 sentence: 47460/57013 loss: 3.787715059411442\n",
      "epoch: 0 sentence: 47470/57013 loss: 5.3673103296348\n",
      "epoch: 0 sentence: 47480/57013 loss: 5.361843399582521\n",
      "epoch: 0 sentence: 47490/57013 loss: 5.676003957539988\n",
      "epoch: 0 sentence: 47500/57013 loss: 5.1646457722332\n",
      "epoch: 0 sentence: 47510/57013 loss: 4.4094535127779615\n",
      "epoch: 0 sentence: 47520/57013 loss: 4.860813372413093\n",
      "epoch: 0 sentence: 47530/57013 loss: 5.500278563767123\n",
      "epoch: 0 sentence: 47540/57013 loss: 4.155096643251497\n",
      "epoch: 0 sentence: 47550/57013 loss: 3.51179239150299\n",
      "epoch: 0 sentence: 47560/57013 loss: 4.354789724899842\n",
      "epoch: 0 sentence: 47570/57013 loss: 4.985102416070112\n",
      "epoch: 0 sentence: 47580/57013 loss: 3.2398575007013077\n",
      "epoch: 0 sentence: 47590/57013 loss: 4.064090965526632\n",
      "epoch: 0 sentence: 47600/57013 loss: 5.1892347204973435\n",
      "epoch: 0 sentence: 47610/57013 loss: 5.090813209956267\n",
      "epoch: 0 sentence: 47620/57013 loss: 4.634835593850996\n",
      "epoch: 0 sentence: 47630/57013 loss: 4.454783069449015\n",
      "epoch: 0 sentence: 47640/57013 loss: 4.2946168975618395\n",
      "epoch: 0 sentence: 47650/57013 loss: 3.7185092148252092\n",
      "epoch: 0 sentence: 47660/57013 loss: 3.8586610135632315\n",
      "epoch: 0 sentence: 47670/57013 loss: 3.3682690679727143\n",
      "epoch: 0 sentence: 47680/57013 loss: 5.433286991541574\n",
      "epoch: 0 sentence: 47690/57013 loss: 4.213192882714171\n",
      "epoch: 0 sentence: 47700/57013 loss: 4.763419572846455\n",
      "epoch: 0 sentence: 47710/57013 loss: 4.936806680685667\n",
      "epoch: 0 sentence: 47720/57013 loss: 3.9941433902283072\n",
      "epoch: 0 sentence: 47730/57013 loss: 4.339503008168097\n",
      "epoch: 0 sentence: 47740/57013 loss: 4.538933816379114\n",
      "epoch: 0 sentence: 47750/57013 loss: 4.184071352847049\n",
      "epoch: 0 sentence: 47760/57013 loss: 4.391271073885294\n",
      "epoch: 0 sentence: 47770/57013 loss: 3.981445388288629\n",
      "epoch: 0 sentence: 47780/57013 loss: 5.38278210341391\n",
      "epoch: 0 sentence: 47790/57013 loss: 4.470705321845822\n",
      "epoch: 0 sentence: 47800/57013 loss: 4.477462129236921\n",
      "epoch: 0 sentence: 47810/57013 loss: 4.30276663993185\n",
      "epoch: 0 sentence: 47820/57013 loss: 4.7144105509732235\n",
      "epoch: 0 sentence: 47830/57013 loss: 4.36082995754067\n",
      "epoch: 0 sentence: 47840/57013 loss: 4.417379684604678\n",
      "epoch: 0 sentence: 47850/57013 loss: 2.0802426820035094\n",
      "epoch: 0 sentence: 47860/57013 loss: 4.825380652264279\n",
      "epoch: 0 sentence: 47870/57013 loss: 5.730759168290133\n",
      "epoch: 0 sentence: 47880/57013 loss: 3.7824040117621487\n",
      "epoch: 0 sentence: 47890/57013 loss: 3.479429543674072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 47900/57013 loss: 3.539700727906459\n",
      "epoch: 0 sentence: 47910/57013 loss: 3.2972697219278175\n",
      "epoch: 0 sentence: 47920/57013 loss: 4.036141209058508\n",
      "epoch: 0 sentence: 47930/57013 loss: 4.525270996459721\n",
      "epoch: 0 sentence: 47940/57013 loss: 4.862796494243891\n",
      "epoch: 0 sentence: 47950/57013 loss: 4.805539630425554\n",
      "epoch: 0 sentence: 47960/57013 loss: 4.797104531310812\n",
      "epoch: 0 sentence: 47970/57013 loss: 5.417034343539427\n",
      "epoch: 0 sentence: 47980/57013 loss: 4.806696978767\n",
      "epoch: 0 sentence: 47990/57013 loss: 4.030375991161197\n",
      "epoch: 0 sentence: 48000/57013 loss: 6.12347746436051\n",
      "epoch: 0 sentence: 48010/57013 loss: 4.613888871985563\n",
      "epoch: 0 sentence: 48020/57013 loss: 4.582212519973059\n",
      "epoch: 0 sentence: 48030/57013 loss: 3.7089018558797\n",
      "epoch: 0 sentence: 48040/57013 loss: 3.5710185874815603\n",
      "epoch: 0 sentence: 48050/57013 loss: 5.096462589645189\n",
      "epoch: 0 sentence: 48060/57013 loss: 5.910553281193926\n",
      "epoch: 0 sentence: 48070/57013 loss: 3.97072539893651\n",
      "epoch: 0 sentence: 48080/57013 loss: 4.6301759427634375\n",
      "epoch: 0 sentence: 48090/57013 loss: 5.070513232476302\n",
      "epoch: 0 sentence: 48100/57013 loss: 4.52963564170565\n",
      "epoch: 0 sentence: 48110/57013 loss: 4.911776560850273\n",
      "epoch: 0 sentence: 48120/57013 loss: 3.7048415756329414\n",
      "epoch: 0 sentence: 48130/57013 loss: 3.4440871728106757\n",
      "epoch: 0 sentence: 48140/57013 loss: 5.150005846492154\n",
      "epoch: 0 sentence: 48150/57013 loss: 3.7850403685505114\n",
      "epoch: 0 sentence: 48160/57013 loss: 6.435464823435821\n",
      "epoch: 0 sentence: 48170/57013 loss: 8.21663292159564\n",
      "epoch: 0 sentence: 48180/57013 loss: 4.63232825177569\n",
      "epoch: 0 sentence: 48190/57013 loss: 3.7887814436228\n",
      "epoch: 0 sentence: 48200/57013 loss: 4.078377207720911\n",
      "epoch: 0 sentence: 48210/57013 loss: 4.449626550002858\n",
      "epoch: 0 sentence: 48220/57013 loss: 3.6312797076092242\n",
      "epoch: 0 sentence: 48230/57013 loss: 4.489343354596357\n",
      "epoch: 0 sentence: 48240/57013 loss: 3.747272164068825\n",
      "epoch: 0 sentence: 48250/57013 loss: 5.434301079757638\n",
      "epoch: 0 sentence: 48260/57013 loss: 3.310031273142965\n",
      "epoch: 0 sentence: 48270/57013 loss: 4.256719899711611\n",
      "epoch: 0 sentence: 48280/57013 loss: 2.6935346349512095\n",
      "epoch: 0 sentence: 48290/57013 loss: 4.1835699380012255\n",
      "epoch: 0 sentence: 48300/57013 loss: 2.9770615995975396\n",
      "epoch: 0 sentence: 48310/57013 loss: 4.261819272600398\n",
      "epoch: 0 sentence: 48320/57013 loss: 2.9801566150175662\n",
      "epoch: 0 sentence: 48330/57013 loss: 3.5839452653749952\n",
      "epoch: 0 sentence: 48340/57013 loss: 3.8937034916585955\n",
      "epoch: 0 sentence: 48350/57013 loss: 5.65022527233969\n",
      "epoch: 0 sentence: 48360/57013 loss: 4.406994990561903\n",
      "epoch: 0 sentence: 48370/57013 loss: 4.427992596361311\n",
      "epoch: 0 sentence: 48380/57013 loss: 3.708701372679809\n",
      "epoch: 0 sentence: 48390/57013 loss: 4.488686129049346\n",
      "epoch: 0 sentence: 48400/57013 loss: 5.032136433466209\n",
      "epoch: 0 sentence: 48410/57013 loss: 5.239894851658532\n",
      "epoch: 0 sentence: 48420/57013 loss: 5.388115370625324\n",
      "epoch: 0 sentence: 48430/57013 loss: 4.348135620941409\n",
      "epoch: 0 sentence: 48440/57013 loss: 4.389856691492125\n",
      "epoch: 0 sentence: 48450/57013 loss: 4.472913437776847\n",
      "epoch: 0 sentence: 48460/57013 loss: 4.762950461753989\n",
      "epoch: 0 sentence: 48470/57013 loss: 5.035405641843283\n",
      "epoch: 0 sentence: 48480/57013 loss: 5.117094854574225\n",
      "epoch: 0 sentence: 48490/57013 loss: 5.146307203276903\n",
      "epoch: 0 sentence: 48500/57013 loss: 4.386930280719121\n",
      "epoch: 0 sentence: 48510/57013 loss: 5.268320741450589\n",
      "epoch: 0 sentence: 48520/57013 loss: 4.419026732604867\n",
      "epoch: 0 sentence: 48530/57013 loss: 5.522882125062039\n",
      "epoch: 0 sentence: 48540/57013 loss: 3.8793500736352575\n",
      "epoch: 0 sentence: 48550/57013 loss: 4.165705465110843\n",
      "epoch: 0 sentence: 48560/57013 loss: 4.488476200286186\n",
      "epoch: 0 sentence: 48570/57013 loss: 4.684505863445403\n",
      "epoch: 0 sentence: 48580/57013 loss: 2.7142464813562532\n",
      "epoch: 0 sentence: 48590/57013 loss: 6.081775549928722\n",
      "epoch: 0 sentence: 48600/57013 loss: 3.559500687570304\n",
      "epoch: 0 sentence: 48610/57013 loss: 4.71863851059366\n",
      "epoch: 0 sentence: 48620/57013 loss: 4.70844361459494\n",
      "epoch: 0 sentence: 48630/57013 loss: 2.8969311927494754\n",
      "epoch: 0 sentence: 48640/57013 loss: 4.157062134882148\n",
      "epoch: 0 sentence: 48650/57013 loss: 3.3859131373148452\n",
      "epoch: 0 sentence: 48660/57013 loss: 4.411577229760025\n",
      "epoch: 0 sentence: 48670/57013 loss: 4.180924426955498\n",
      "epoch: 0 sentence: 48680/57013 loss: 3.798245549855875\n",
      "epoch: 0 sentence: 48690/57013 loss: 5.081369303739956\n",
      "epoch: 0 sentence: 48700/57013 loss: 3.5160492285992473\n",
      "epoch: 0 sentence: 48710/57013 loss: 4.220960447194282\n",
      "epoch: 0 sentence: 48720/57013 loss: 4.140521427901857\n",
      "epoch: 0 sentence: 48730/57013 loss: 4.93881603497411\n",
      "epoch: 0 sentence: 48740/57013 loss: 4.682340367257791\n",
      "epoch: 0 sentence: 48750/57013 loss: 3.306400430377322\n",
      "epoch: 0 sentence: 48760/57013 loss: 5.055639341829863\n",
      "epoch: 0 sentence: 48770/57013 loss: 3.580093070579366\n",
      "epoch: 0 sentence: 48780/57013 loss: 7.393929690635179\n",
      "epoch: 0 sentence: 48790/57013 loss: 5.045386324976804\n",
      "epoch: 0 sentence: 48800/57013 loss: 4.948515380387583\n",
      "epoch: 0 sentence: 48810/57013 loss: 2.9198087570751508\n",
      "epoch: 0 sentence: 48820/57013 loss: 4.230907973370446\n",
      "epoch: 0 sentence: 48830/57013 loss: 4.8115210419122105\n",
      "epoch: 0 sentence: 48840/57013 loss: 4.619063499723158\n",
      "epoch: 0 sentence: 48850/57013 loss: 4.832249973477246\n",
      "epoch: 0 sentence: 48860/57013 loss: 4.269401343906227\n",
      "epoch: 0 sentence: 48870/57013 loss: 4.078437617888159\n",
      "epoch: 0 sentence: 48880/57013 loss: 4.9380447266641765\n",
      "epoch: 0 sentence: 48890/57013 loss: 4.036427997565116\n",
      "epoch: 0 sentence: 48900/57013 loss: 4.860216944739026\n",
      "epoch: 0 sentence: 48910/57013 loss: 4.119590681616407\n",
      "epoch: 0 sentence: 48920/57013 loss: 3.5286294222687586\n",
      "epoch: 0 sentence: 48930/57013 loss: 5.261246761835253\n",
      "epoch: 0 sentence: 48940/57013 loss: 4.966279424621166\n",
      "epoch: 0 sentence: 48950/57013 loss: 4.959085648717438\n",
      "epoch: 0 sentence: 48960/57013 loss: 5.049070827533608\n",
      "epoch: 0 sentence: 48970/57013 loss: 5.511648332690893\n",
      "epoch: 0 sentence: 48980/57013 loss: 5.334006701947203\n",
      "epoch: 0 sentence: 48990/57013 loss: 3.6403667106505164\n",
      "epoch: 0 sentence: 49000/57013 loss: 5.1209008134764655\n",
      "epoch: 0 sentence: 49010/57013 loss: 4.703395889885406\n",
      "epoch: 0 sentence: 49020/57013 loss: 5.136207744453423\n",
      "epoch: 0 sentence: 49030/57013 loss: 4.0643862979700325\n",
      "epoch: 0 sentence: 49040/57013 loss: 4.7784080894462715\n",
      "epoch: 0 sentence: 49050/57013 loss: 5.3800153189841335\n",
      "epoch: 0 sentence: 49060/57013 loss: 3.0726716766428672\n",
      "epoch: 0 sentence: 49070/57013 loss: 5.999250693520699\n",
      "epoch: 0 sentence: 49080/57013 loss: 4.675064222723263\n",
      "epoch: 0 sentence: 49090/57013 loss: 3.252096568173377\n",
      "epoch: 0 sentence: 49100/57013 loss: 4.7238863410804095\n",
      "epoch: 0 sentence: 49110/57013 loss: 4.646178946485246\n",
      "epoch: 0 sentence: 49120/57013 loss: 3.8417534238071123\n",
      "epoch: 0 sentence: 49130/57013 loss: 2.7615990147983553\n",
      "epoch: 0 sentence: 49140/57013 loss: 5.468550423131148\n",
      "epoch: 0 sentence: 49150/57013 loss: 5.904042516477557\n",
      "epoch: 0 sentence: 49160/57013 loss: 5.328407384701658\n",
      "epoch: 0 sentence: 49170/57013 loss: 4.773674633694032\n",
      "epoch: 0 sentence: 49180/57013 loss: 4.487504628859941\n",
      "epoch: 0 sentence: 49190/57013 loss: 3.6265893527123825\n",
      "epoch: 0 sentence: 49200/57013 loss: 5.116042331201969\n",
      "epoch: 0 sentence: 49210/57013 loss: 1.7411126097751233\n",
      "epoch: 0 sentence: 49220/57013 loss: 4.320002512573036\n",
      "epoch: 0 sentence: 49230/57013 loss: 5.202949586343541\n",
      "epoch: 0 sentence: 49240/57013 loss: 5.488523530167084\n",
      "epoch: 0 sentence: 49250/57013 loss: 4.466428623171746\n",
      "epoch: 0 sentence: 49260/57013 loss: 6.037119235071119\n",
      "epoch: 0 sentence: 49270/57013 loss: 4.19872053030793\n",
      "epoch: 0 sentence: 49280/57013 loss: 4.797151604029848\n",
      "epoch: 0 sentence: 49290/57013 loss: 3.889035824824257\n",
      "epoch: 0 sentence: 49300/57013 loss: 4.651560164557602\n",
      "epoch: 0 sentence: 49310/57013 loss: 3.439771049825321\n",
      "epoch: 0 sentence: 49320/57013 loss: 5.770098063822669\n",
      "epoch: 0 sentence: 49330/57013 loss: 4.022837839790747\n",
      "epoch: 0 sentence: 49340/57013 loss: 4.792458468783757\n",
      "epoch: 0 sentence: 49350/57013 loss: 3.893451269523372\n",
      "epoch: 0 sentence: 49360/57013 loss: 3.7747476679764778\n",
      "epoch: 0 sentence: 49370/57013 loss: 4.834159812848971\n",
      "epoch: 0 sentence: 49380/57013 loss: 7.944785746973049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 49390/57013 loss: 4.311473181586728\n",
      "epoch: 0 sentence: 49400/57013 loss: 5.947230760761571\n",
      "epoch: 0 sentence: 49410/57013 loss: 4.266866953817045\n",
      "epoch: 0 sentence: 49420/57013 loss: 4.522729582306524\n",
      "epoch: 0 sentence: 49430/57013 loss: 6.0643322413267144\n",
      "epoch: 0 sentence: 49440/57013 loss: 6.362270391518887\n",
      "epoch: 0 sentence: 49450/57013 loss: 4.7591353742191105\n",
      "epoch: 0 sentence: 49460/57013 loss: 3.5353847555017777\n",
      "epoch: 0 sentence: 49470/57013 loss: 2.6253750615106997\n",
      "epoch: 0 sentence: 49480/57013 loss: 4.091284192449405\n",
      "epoch: 0 sentence: 49490/57013 loss: 4.1194048212937915\n",
      "epoch: 0 sentence: 49500/57013 loss: 4.440367835775566\n",
      "epoch: 0 sentence: 49510/57013 loss: 5.119192701303819\n",
      "epoch: 0 sentence: 49520/57013 loss: 4.730689852947413\n",
      "epoch: 0 sentence: 49530/57013 loss: 1.6716229877398665\n",
      "epoch: 0 sentence: 49540/57013 loss: 5.206250524348032\n",
      "epoch: 0 sentence: 49550/57013 loss: 4.257396207348809\n",
      "epoch: 0 sentence: 49560/57013 loss: 3.999995995333967\n",
      "epoch: 0 sentence: 49570/57013 loss: 4.4362554253255375\n",
      "epoch: 0 sentence: 49580/57013 loss: 4.696507225676391\n",
      "epoch: 0 sentence: 49590/57013 loss: 4.959002257085391\n",
      "epoch: 0 sentence: 49600/57013 loss: 3.8345296183136153\n",
      "epoch: 0 sentence: 49610/57013 loss: 2.810412679726562\n",
      "epoch: 0 sentence: 49620/57013 loss: 4.513257177778555\n",
      "epoch: 0 sentence: 49630/57013 loss: 3.158111168793972\n",
      "epoch: 0 sentence: 49640/57013 loss: 4.311484171809639\n",
      "epoch: 0 sentence: 49650/57013 loss: 3.992928622601686\n",
      "epoch: 0 sentence: 49660/57013 loss: 5.727929793738176\n",
      "epoch: 0 sentence: 49670/57013 loss: 3.5412033236974865\n",
      "epoch: 0 sentence: 49680/57013 loss: 4.478693084184046\n",
      "epoch: 0 sentence: 49690/57013 loss: 5.28144542697098\n",
      "epoch: 0 sentence: 49700/57013 loss: 4.583304884216581\n",
      "epoch: 0 sentence: 49710/57013 loss: 4.632400485447247\n",
      "epoch: 0 sentence: 49720/57013 loss: 4.68034542704325\n",
      "epoch: 0 sentence: 49730/57013 loss: 4.9811016626286815\n",
      "epoch: 0 sentence: 49740/57013 loss: 3.8810766453951913\n",
      "epoch: 0 sentence: 49750/57013 loss: 4.369426108631336\n",
      "epoch: 0 sentence: 49760/57013 loss: 4.349032938400811\n",
      "epoch: 0 sentence: 49770/57013 loss: 4.09231695939746\n",
      "epoch: 0 sentence: 49780/57013 loss: 4.660275291974831\n",
      "epoch: 0 sentence: 49790/57013 loss: 3.4499323434637774\n",
      "epoch: 0 sentence: 49800/57013 loss: 4.481156814725595\n",
      "epoch: 0 sentence: 49810/57013 loss: 5.379657286654893\n",
      "epoch: 0 sentence: 49820/57013 loss: 4.849851924651072\n",
      "epoch: 0 sentence: 49830/57013 loss: 3.9419564672548524\n",
      "epoch: 0 sentence: 49840/57013 loss: 4.361959938463765\n",
      "epoch: 0 sentence: 49850/57013 loss: 6.021239549280698\n",
      "epoch: 0 sentence: 49860/57013 loss: 4.910083461482712\n",
      "epoch: 0 sentence: 49870/57013 loss: 4.141513068301731\n",
      "epoch: 0 sentence: 49880/57013 loss: 2.5869597955889096\n",
      "epoch: 0 sentence: 49890/57013 loss: 4.604986099564138\n",
      "epoch: 0 sentence: 49900/57013 loss: 5.21140751407406\n",
      "epoch: 0 sentence: 49910/57013 loss: 5.222368113020587\n",
      "epoch: 0 sentence: 49920/57013 loss: 5.784158153270381\n",
      "epoch: 0 sentence: 49930/57013 loss: 5.14939587271289\n",
      "epoch: 0 sentence: 49940/57013 loss: 4.559142303856095\n",
      "epoch: 0 sentence: 49950/57013 loss: 4.457382495506438\n",
      "epoch: 0 sentence: 49960/57013 loss: 5.274488297565269\n",
      "epoch: 0 sentence: 49970/57013 loss: 5.242467541060674\n",
      "epoch: 0 sentence: 49980/57013 loss: 3.1008620358972188\n",
      "epoch: 0 sentence: 49990/57013 loss: 3.623866033991044\n",
      "epoch: 0 sentence: 50000/57013 loss: 4.587530799843397\n",
      "epoch: 0 sentence: 50010/57013 loss: 5.318602807901074\n",
      "epoch: 0 sentence: 50020/57013 loss: 4.572986234935539\n",
      "epoch: 0 sentence: 50030/57013 loss: 5.463807155343713\n",
      "epoch: 0 sentence: 50040/57013 loss: 5.418747572844597\n",
      "epoch: 0 sentence: 50050/57013 loss: 4.071769942481847\n",
      "epoch: 0 sentence: 50060/57013 loss: 2.908424989590632\n",
      "epoch: 0 sentence: 50070/57013 loss: 4.092987859293913\n",
      "epoch: 0 sentence: 50080/57013 loss: 3.5753961753368366\n",
      "epoch: 0 sentence: 50090/57013 loss: 3.725795500680054\n",
      "epoch: 0 sentence: 50100/57013 loss: 3.982723459113937\n",
      "epoch: 0 sentence: 50110/57013 loss: 4.248068721250576\n",
      "epoch: 0 sentence: 50120/57013 loss: 3.1053492086385335\n",
      "epoch: 0 sentence: 50130/57013 loss: 5.416109640234914\n",
      "epoch: 0 sentence: 50140/57013 loss: 4.446635521781169\n",
      "epoch: 0 sentence: 50150/57013 loss: 2.842285863087882\n",
      "epoch: 0 sentence: 50160/57013 loss: 4.737480130788015\n",
      "epoch: 0 sentence: 50170/57013 loss: 4.571480354258785\n",
      "epoch: 0 sentence: 50180/57013 loss: 4.7016633707554245\n",
      "epoch: 0 sentence: 50190/57013 loss: 4.456729845311174\n",
      "epoch: 0 sentence: 50200/57013 loss: 4.54393994198611\n",
      "epoch: 0 sentence: 50210/57013 loss: 4.397872947287814\n",
      "epoch: 0 sentence: 50220/57013 loss: 5.966519498951179\n",
      "epoch: 0 sentence: 50230/57013 loss: 4.362926143992107\n",
      "epoch: 0 sentence: 50240/57013 loss: 2.959504658234302\n",
      "epoch: 0 sentence: 50250/57013 loss: 4.730615928121003\n",
      "epoch: 0 sentence: 50260/57013 loss: 4.99487515679329\n",
      "epoch: 0 sentence: 50270/57013 loss: 4.757931942445248\n",
      "epoch: 0 sentence: 50280/57013 loss: 2.5973435195051198\n",
      "epoch: 0 sentence: 50290/57013 loss: 5.895449229972046\n",
      "epoch: 0 sentence: 50300/57013 loss: 4.4461447772967\n",
      "epoch: 0 sentence: 50310/57013 loss: 3.9513045613993913\n",
      "epoch: 0 sentence: 50320/57013 loss: 2.742804538817226\n",
      "epoch: 0 sentence: 50330/57013 loss: 4.838060810136065\n",
      "epoch: 0 sentence: 50340/57013 loss: 5.057485946730383\n",
      "epoch: 0 sentence: 50350/57013 loss: 4.404373177803118\n",
      "epoch: 0 sentence: 50360/57013 loss: 4.11641069164131\n",
      "epoch: 0 sentence: 50370/57013 loss: 3.7720542098850136\n",
      "epoch: 0 sentence: 50380/57013 loss: 3.8105458034458755\n",
      "epoch: 0 sentence: 50390/57013 loss: 4.3656387829093894\n",
      "epoch: 0 sentence: 50400/57013 loss: 4.150290212935359\n",
      "epoch: 0 sentence: 50410/57013 loss: 4.508755178384988\n",
      "epoch: 0 sentence: 50420/57013 loss: 2.465453909856905\n",
      "epoch: 0 sentence: 50430/57013 loss: 5.567974447020034\n",
      "epoch: 0 sentence: 50440/57013 loss: 3.5304408157384373\n",
      "epoch: 0 sentence: 50450/57013 loss: 1.5614507018543125\n",
      "epoch: 0 sentence: 50460/57013 loss: 5.538829082572491\n",
      "epoch: 0 sentence: 50470/57013 loss: 5.572494374316625\n",
      "epoch: 0 sentence: 50480/57013 loss: 4.154036613723585\n",
      "epoch: 0 sentence: 50490/57013 loss: 2.150612156946304\n",
      "epoch: 0 sentence: 50500/57013 loss: 4.202150391205464\n",
      "epoch: 0 sentence: 50510/57013 loss: 4.871170446890982\n",
      "epoch: 0 sentence: 50520/57013 loss: 3.2309053626876336\n",
      "epoch: 0 sentence: 50530/57013 loss: 4.861682523238501\n",
      "epoch: 0 sentence: 50540/57013 loss: 4.726563113126868\n",
      "epoch: 0 sentence: 50550/57013 loss: 3.9722469457620937\n",
      "epoch: 0 sentence: 50560/57013 loss: 4.14634195843046\n",
      "epoch: 0 sentence: 50570/57013 loss: 3.990777637101628\n",
      "epoch: 0 sentence: 50580/57013 loss: 5.054542410397718\n",
      "epoch: 0 sentence: 50590/57013 loss: 3.8068219417690443\n",
      "epoch: 0 sentence: 50600/57013 loss: 4.735518148728616\n",
      "epoch: 0 sentence: 50610/57013 loss: 4.6993376868741485\n",
      "epoch: 0 sentence: 50620/57013 loss: 4.060503083928138\n",
      "epoch: 0 sentence: 50630/57013 loss: 4.079034656485127\n",
      "epoch: 0 sentence: 50640/57013 loss: 5.325172054895746\n",
      "epoch: 0 sentence: 50650/57013 loss: 4.550538500752362\n",
      "epoch: 0 sentence: 50660/57013 loss: 4.2926773234332325\n",
      "epoch: 0 sentence: 50670/57013 loss: 4.2956198199036955\n",
      "epoch: 0 sentence: 50680/57013 loss: 2.993811866719336\n",
      "epoch: 0 sentence: 50690/57013 loss: 5.076955978875595\n",
      "epoch: 0 sentence: 50700/57013 loss: 3.338085773466991\n",
      "epoch: 0 sentence: 50710/57013 loss: 4.921333434234013\n",
      "epoch: 0 sentence: 50720/57013 loss: 5.473649353918091\n",
      "epoch: 0 sentence: 50730/57013 loss: 4.457920033819987\n",
      "epoch: 0 sentence: 50740/57013 loss: 6.247187831897014\n",
      "epoch: 0 sentence: 50750/57013 loss: 4.3811293361386445\n",
      "epoch: 0 sentence: 50760/57013 loss: 4.888665199377083\n",
      "epoch: 0 sentence: 50770/57013 loss: 5.306691439289912\n",
      "epoch: 0 sentence: 50780/57013 loss: 5.085103566747649\n",
      "epoch: 0 sentence: 50790/57013 loss: 3.6342775910002283\n",
      "epoch: 0 sentence: 50800/57013 loss: 4.774712981390132\n",
      "epoch: 0 sentence: 50810/57013 loss: 4.21092198729081\n",
      "epoch: 0 sentence: 50820/57013 loss: 3.817744973644903\n",
      "epoch: 0 sentence: 50830/57013 loss: 4.58689715890369\n",
      "epoch: 0 sentence: 50840/57013 loss: 2.9690008261726906\n",
      "epoch: 0 sentence: 50850/57013 loss: 5.605488707191612\n",
      "epoch: 0 sentence: 50860/57013 loss: 4.377280221872368\n",
      "epoch: 0 sentence: 50870/57013 loss: 5.360568683004512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 50880/57013 loss: 6.126449296190488\n",
      "epoch: 0 sentence: 50890/57013 loss: 4.714758869003344\n",
      "epoch: 0 sentence: 50900/57013 loss: 4.243533984817136\n",
      "epoch: 0 sentence: 50910/57013 loss: 3.902785830173698\n",
      "epoch: 0 sentence: 50920/57013 loss: 4.610844017260045\n",
      "epoch: 0 sentence: 50930/57013 loss: 3.966851732122913\n",
      "epoch: 0 sentence: 50940/57013 loss: 4.78033879819731\n",
      "epoch: 0 sentence: 50950/57013 loss: 5.0835476318816255\n",
      "epoch: 0 sentence: 50960/57013 loss: 3.362725662513116\n",
      "epoch: 0 sentence: 50970/57013 loss: 4.07254863227914\n",
      "epoch: 0 sentence: 50980/57013 loss: 5.345371252264143\n",
      "epoch: 0 sentence: 50990/57013 loss: 4.299906630031903\n",
      "epoch: 0 sentence: 51000/57013 loss: 4.318100412324452\n",
      "epoch: 0 sentence: 51010/57013 loss: 2.2018442382283485\n",
      "epoch: 0 sentence: 51020/57013 loss: 3.471137373351951\n",
      "epoch: 0 sentence: 51030/57013 loss: 4.100013452091241\n",
      "epoch: 0 sentence: 51040/57013 loss: 3.9646369312047547\n",
      "epoch: 0 sentence: 51050/57013 loss: 4.071718661828626\n",
      "epoch: 0 sentence: 51060/57013 loss: 4.52363478188599\n",
      "epoch: 0 sentence: 51070/57013 loss: 5.327934445811844\n",
      "epoch: 0 sentence: 51080/57013 loss: 4.710681845958302\n",
      "epoch: 0 sentence: 51090/57013 loss: 5.19655791891695\n",
      "epoch: 0 sentence: 51100/57013 loss: 2.371316235097011\n",
      "epoch: 0 sentence: 51110/57013 loss: 4.6051405025681005\n",
      "epoch: 0 sentence: 51120/57013 loss: 4.980277022429163\n",
      "epoch: 0 sentence: 51130/57013 loss: 2.6747088759237503\n",
      "epoch: 0 sentence: 51140/57013 loss: 3.1871691181492374\n",
      "epoch: 0 sentence: 51150/57013 loss: 4.403898236915739\n",
      "epoch: 0 sentence: 51160/57013 loss: 4.313476295365893\n",
      "epoch: 0 sentence: 51170/57013 loss: 5.2161806403287505\n",
      "epoch: 0 sentence: 51180/57013 loss: 4.847322798735026\n",
      "epoch: 0 sentence: 51190/57013 loss: 5.482340245862715\n",
      "epoch: 0 sentence: 51200/57013 loss: 4.793162073615289\n",
      "epoch: 0 sentence: 51210/57013 loss: 3.072326066782736\n",
      "epoch: 0 sentence: 51220/57013 loss: 5.199773846784372\n",
      "epoch: 0 sentence: 51230/57013 loss: 3.8618092128625308\n",
      "epoch: 0 sentence: 51240/57013 loss: 5.1387960229851615\n",
      "epoch: 0 sentence: 51250/57013 loss: 4.841314297221059\n",
      "epoch: 0 sentence: 51260/57013 loss: 3.331161568409805\n",
      "epoch: 0 sentence: 51270/57013 loss: 3.704270705843431\n",
      "epoch: 0 sentence: 51280/57013 loss: 4.289870045245286\n",
      "epoch: 0 sentence: 51290/57013 loss: 5.447990283008682\n",
      "epoch: 0 sentence: 51300/57013 loss: 3.6031203560821115\n",
      "epoch: 0 sentence: 51310/57013 loss: 4.476763857332729\n",
      "epoch: 0 sentence: 51320/57013 loss: 5.032184626880049\n",
      "epoch: 0 sentence: 51330/57013 loss: 3.3387388980136463\n",
      "epoch: 0 sentence: 51340/57013 loss: 5.651332308581665\n",
      "epoch: 0 sentence: 51350/57013 loss: 3.9395032015728852\n",
      "epoch: 0 sentence: 51360/57013 loss: 4.531300780964363\n",
      "epoch: 0 sentence: 51370/57013 loss: 4.425185830923929\n",
      "epoch: 0 sentence: 51380/57013 loss: 4.923224909161222\n",
      "epoch: 0 sentence: 51390/57013 loss: 4.966906718962852\n",
      "epoch: 0 sentence: 51400/57013 loss: 2.559331969445487\n",
      "epoch: 0 sentence: 51410/57013 loss: 4.098147072794367\n",
      "epoch: 0 sentence: 51420/57013 loss: 5.40754455716465\n",
      "epoch: 0 sentence: 51430/57013 loss: 4.300657256324138\n",
      "epoch: 0 sentence: 51440/57013 loss: 4.705871092120216\n",
      "epoch: 0 sentence: 51450/57013 loss: 4.859946539338659\n",
      "epoch: 0 sentence: 51460/57013 loss: 4.143793815995898\n",
      "epoch: 0 sentence: 51470/57013 loss: 4.2183781785053185\n",
      "epoch: 0 sentence: 51480/57013 loss: 4.487704830595481\n",
      "epoch: 0 sentence: 51490/57013 loss: 4.430533465859378\n",
      "epoch: 0 sentence: 51500/57013 loss: 4.170263079793824\n",
      "epoch: 0 sentence: 51510/57013 loss: 4.699258358491375\n",
      "epoch: 0 sentence: 51520/57013 loss: 4.997610996676907\n",
      "epoch: 0 sentence: 51530/57013 loss: 4.234051395204685\n",
      "epoch: 0 sentence: 51540/57013 loss: 5.280832467855714\n",
      "epoch: 0 sentence: 51550/57013 loss: 4.149066015785734\n",
      "epoch: 0 sentence: 51560/57013 loss: 2.7468902857738757\n",
      "epoch: 0 sentence: 51570/57013 loss: 3.544288114190534\n",
      "epoch: 0 sentence: 51580/57013 loss: 5.10103013532226\n",
      "epoch: 0 sentence: 51590/57013 loss: 5.939847860691812\n",
      "epoch: 0 sentence: 51600/57013 loss: 4.668642557173889\n",
      "epoch: 0 sentence: 51610/57013 loss: 5.205467231702921\n",
      "epoch: 0 sentence: 51620/57013 loss: 4.024750202617116\n",
      "epoch: 0 sentence: 51630/57013 loss: 4.666824360286777\n",
      "epoch: 0 sentence: 51640/57013 loss: 4.606557393648263\n",
      "epoch: 0 sentence: 51650/57013 loss: 5.330498518217154\n",
      "epoch: 0 sentence: 51660/57013 loss: 4.466049374421483\n",
      "epoch: 0 sentence: 51670/57013 loss: 4.1184152875179105\n",
      "epoch: 0 sentence: 51680/57013 loss: 4.843877525889822\n",
      "epoch: 0 sentence: 51690/57013 loss: 4.4028314069085\n",
      "epoch: 0 sentence: 51700/57013 loss: 3.8272336847640616\n",
      "epoch: 0 sentence: 51710/57013 loss: 4.298304511287079\n",
      "epoch: 0 sentence: 51720/57013 loss: 3.9215597040083736\n",
      "epoch: 0 sentence: 51730/57013 loss: 4.74250106431359\n",
      "epoch: 0 sentence: 51740/57013 loss: 4.7544702156443535\n",
      "epoch: 0 sentence: 51750/57013 loss: 3.6308344694344523\n",
      "epoch: 0 sentence: 51760/57013 loss: 4.883190325262371\n",
      "epoch: 0 sentence: 51770/57013 loss: 4.335445858033928\n",
      "epoch: 0 sentence: 51780/57013 loss: 3.5875597651464677\n",
      "epoch: 0 sentence: 51790/57013 loss: 4.787997882966118\n",
      "epoch: 0 sentence: 51800/57013 loss: 3.7547936434662477\n",
      "epoch: 0 sentence: 51810/57013 loss: 4.157836439422142\n",
      "epoch: 0 sentence: 51820/57013 loss: 5.20427629768834\n",
      "epoch: 0 sentence: 51830/57013 loss: 3.211159278109965\n",
      "epoch: 0 sentence: 51840/57013 loss: 4.971239290115626\n",
      "epoch: 0 sentence: 51850/57013 loss: 4.233221656594412\n",
      "epoch: 0 sentence: 51860/57013 loss: 4.825675696879863\n",
      "epoch: 0 sentence: 51870/57013 loss: 5.39561759772983\n",
      "epoch: 0 sentence: 51880/57013 loss: 5.155206197692266\n",
      "epoch: 0 sentence: 51890/57013 loss: 6.459544563575848\n",
      "epoch: 0 sentence: 51900/57013 loss: 3.9236563443131685\n",
      "epoch: 0 sentence: 51910/57013 loss: 4.561102794810407\n",
      "epoch: 0 sentence: 51920/57013 loss: 4.451441347839067\n",
      "epoch: 0 sentence: 51930/57013 loss: 5.60238771634755\n",
      "epoch: 0 sentence: 51940/57013 loss: 4.460533816858669\n",
      "epoch: 0 sentence: 51950/57013 loss: 4.464738132907979\n",
      "epoch: 0 sentence: 51960/57013 loss: 4.154911613190654\n",
      "epoch: 0 sentence: 51970/57013 loss: 5.492116613673077\n",
      "epoch: 0 sentence: 51980/57013 loss: 4.946216112265275\n",
      "epoch: 0 sentence: 51990/57013 loss: 4.335522079735211\n",
      "epoch: 0 sentence: 52000/57013 loss: 3.7586480983757657\n",
      "epoch: 0 sentence: 52010/57013 loss: 6.203207212352391\n",
      "epoch: 0 sentence: 52020/57013 loss: 5.661066516053267\n",
      "epoch: 0 sentence: 52030/57013 loss: 6.088082372002415\n",
      "epoch: 0 sentence: 52040/57013 loss: 3.078738766099281\n",
      "epoch: 0 sentence: 52050/57013 loss: 5.184604477836205\n",
      "epoch: 0 sentence: 52060/57013 loss: 4.193314777232017\n",
      "epoch: 0 sentence: 52070/57013 loss: 5.485024526175701\n",
      "epoch: 0 sentence: 52080/57013 loss: 5.179156420055223\n",
      "epoch: 0 sentence: 52090/57013 loss: 3.713245539569693\n",
      "epoch: 0 sentence: 52100/57013 loss: 5.045866362623367\n",
      "epoch: 0 sentence: 52110/57013 loss: 5.077944134697412\n",
      "epoch: 0 sentence: 52120/57013 loss: 5.263094863179252\n",
      "epoch: 0 sentence: 52130/57013 loss: 4.892871114301234\n",
      "epoch: 0 sentence: 52140/57013 loss: 3.8645790599607137\n",
      "epoch: 0 sentence: 52150/57013 loss: 5.289172404852858\n",
      "epoch: 0 sentence: 52160/57013 loss: 4.153169549988724\n",
      "epoch: 0 sentence: 52170/57013 loss: 4.306479479258047\n",
      "epoch: 0 sentence: 52180/57013 loss: 4.76504054104422\n",
      "epoch: 0 sentence: 52190/57013 loss: 3.9350959660499374\n",
      "epoch: 0 sentence: 52200/57013 loss: 3.301661560383922\n",
      "epoch: 0 sentence: 52210/57013 loss: 3.4686651466616962\n",
      "epoch: 0 sentence: 52220/57013 loss: 5.938104488040907\n",
      "epoch: 0 sentence: 52230/57013 loss: 4.699272910210771\n",
      "epoch: 0 sentence: 52240/57013 loss: 3.634919896482275\n",
      "epoch: 0 sentence: 52250/57013 loss: 5.116273208739921\n",
      "epoch: 0 sentence: 52260/57013 loss: 5.206373508519972\n",
      "epoch: 0 sentence: 52270/57013 loss: 3.242165175253946\n",
      "epoch: 0 sentence: 52280/57013 loss: 5.739911525271982\n",
      "epoch: 0 sentence: 52290/57013 loss: 5.106403023244718\n",
      "epoch: 0 sentence: 52300/57013 loss: 3.6927587524477143\n",
      "epoch: 0 sentence: 52310/57013 loss: 5.211553509800679\n",
      "epoch: 0 sentence: 52320/57013 loss: 3.7215025762585396\n",
      "epoch: 0 sentence: 52330/57013 loss: 5.198665929550865\n",
      "epoch: 0 sentence: 52340/57013 loss: 4.408066184061207\n",
      "epoch: 0 sentence: 52350/57013 loss: 4.681814202602208\n",
      "epoch: 0 sentence: 52360/57013 loss: 4.433390120209629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 52370/57013 loss: 4.977508992735937\n",
      "epoch: 0 sentence: 52380/57013 loss: 4.971021651657898\n",
      "epoch: 0 sentence: 52390/57013 loss: 4.816299918563251\n",
      "epoch: 0 sentence: 52400/57013 loss: 4.057701483461416\n",
      "epoch: 0 sentence: 52410/57013 loss: 5.199732977450306\n",
      "epoch: 0 sentence: 52420/57013 loss: 4.117226468959417\n",
      "epoch: 0 sentence: 52430/57013 loss: 4.289656212423773\n",
      "epoch: 0 sentence: 52440/57013 loss: 3.928660211432731\n",
      "epoch: 0 sentence: 52450/57013 loss: 4.330125208488044\n",
      "epoch: 0 sentence: 52460/57013 loss: 5.432792686791625\n",
      "epoch: 0 sentence: 52470/57013 loss: 5.102229196734056\n",
      "epoch: 0 sentence: 52480/57013 loss: 4.521044223680447\n",
      "epoch: 0 sentence: 52490/57013 loss: 5.1544111289258625\n",
      "epoch: 0 sentence: 52500/57013 loss: 3.782629925606866\n",
      "epoch: 0 sentence: 52510/57013 loss: 4.638020141491197\n",
      "epoch: 0 sentence: 52520/57013 loss: 3.5581885063326104\n",
      "epoch: 0 sentence: 52530/57013 loss: 3.8103844196296803\n",
      "epoch: 0 sentence: 52540/57013 loss: 4.979006593809557\n",
      "epoch: 0 sentence: 52550/57013 loss: 5.0244035442256685\n",
      "epoch: 0 sentence: 52560/57013 loss: 5.191582928684836\n",
      "epoch: 0 sentence: 52570/57013 loss: 3.790004621376856\n",
      "epoch: 0 sentence: 52580/57013 loss: 4.368150555558892\n",
      "epoch: 0 sentence: 52590/57013 loss: 4.4711372370081905\n",
      "epoch: 0 sentence: 52600/57013 loss: 3.5576495161296045\n",
      "epoch: 0 sentence: 52610/57013 loss: 4.197150743820065\n",
      "epoch: 0 sentence: 52620/57013 loss: 1.9773550206290453\n",
      "epoch: 0 sentence: 52630/57013 loss: 4.590853016062582\n",
      "epoch: 0 sentence: 52640/57013 loss: 5.344394502897326\n",
      "epoch: 0 sentence: 52650/57013 loss: 4.619222661593195\n",
      "epoch: 0 sentence: 52660/57013 loss: 3.2589528750632595\n",
      "epoch: 0 sentence: 52670/57013 loss: 5.49536897598787\n",
      "epoch: 0 sentence: 52680/57013 loss: 5.56083185367937\n",
      "epoch: 0 sentence: 52690/57013 loss: 4.442762864455393\n",
      "epoch: 0 sentence: 52700/57013 loss: 4.253702862930622\n",
      "epoch: 0 sentence: 52710/57013 loss: 3.9140304414598805\n",
      "epoch: 0 sentence: 52720/57013 loss: 5.749499401626161\n",
      "epoch: 0 sentence: 52730/57013 loss: 3.558554803737637\n",
      "epoch: 0 sentence: 52740/57013 loss: 4.890824355380185\n",
      "epoch: 0 sentence: 52750/57013 loss: 4.8068364884440316\n",
      "epoch: 0 sentence: 52760/57013 loss: 4.679858614758692\n",
      "epoch: 0 sentence: 52770/57013 loss: 6.477691150919931\n",
      "epoch: 0 sentence: 52780/57013 loss: 4.931628109977513\n",
      "epoch: 0 sentence: 52790/57013 loss: 3.585069243504589\n",
      "epoch: 0 sentence: 52800/57013 loss: 4.8618555640253245\n",
      "epoch: 0 sentence: 52810/57013 loss: 4.858226466879665\n",
      "epoch: 0 sentence: 52820/57013 loss: 4.711716868912237\n",
      "epoch: 0 sentence: 52830/57013 loss: 4.476185852634116\n",
      "epoch: 0 sentence: 52840/57013 loss: 5.189863007274938\n",
      "epoch: 0 sentence: 52850/57013 loss: 4.399624537171155\n",
      "epoch: 0 sentence: 52860/57013 loss: 5.946962816045607\n",
      "epoch: 0 sentence: 52870/57013 loss: 5.71660699509428\n",
      "epoch: 0 sentence: 52880/57013 loss: 4.173155565100306\n",
      "epoch: 0 sentence: 52890/57013 loss: 4.817651887200687\n",
      "epoch: 0 sentence: 52900/57013 loss: 5.471738993885543\n",
      "epoch: 0 sentence: 52910/57013 loss: 4.663758506238306\n",
      "epoch: 0 sentence: 52920/57013 loss: 4.831253398125362\n",
      "epoch: 0 sentence: 52930/57013 loss: 5.460657229178401\n",
      "epoch: 0 sentence: 52940/57013 loss: 4.44278623506644\n",
      "epoch: 0 sentence: 52950/57013 loss: 3.336138898761159\n",
      "epoch: 0 sentence: 52960/57013 loss: 3.868276635105406\n",
      "epoch: 0 sentence: 52970/57013 loss: 3.9504681449269983\n",
      "epoch: 0 sentence: 52980/57013 loss: 5.2274355303844935\n",
      "epoch: 0 sentence: 52990/57013 loss: 4.836933591720411\n",
      "epoch: 0 sentence: 53000/57013 loss: 4.723097197904424\n",
      "epoch: 0 sentence: 53010/57013 loss: 5.47904812761323\n",
      "epoch: 0 sentence: 53020/57013 loss: 3.8883801152416657\n",
      "epoch: 0 sentence: 53030/57013 loss: 4.010557407872103\n",
      "epoch: 0 sentence: 53040/57013 loss: 5.754765673375681\n",
      "epoch: 0 sentence: 53050/57013 loss: 4.505547907009158\n",
      "epoch: 0 sentence: 53060/57013 loss: 4.090965273424627\n",
      "epoch: 0 sentence: 53070/57013 loss: 4.150198220976487\n",
      "epoch: 0 sentence: 53080/57013 loss: 4.343136519402842\n",
      "epoch: 0 sentence: 53090/57013 loss: 3.695546495643462\n",
      "epoch: 0 sentence: 53100/57013 loss: 3.0036395151452067\n",
      "epoch: 0 sentence: 53110/57013 loss: 4.510101222248074\n",
      "epoch: 0 sentence: 53120/57013 loss: 5.181826292362285\n",
      "epoch: 0 sentence: 53130/57013 loss: 4.568103575314817\n",
      "epoch: 0 sentence: 53140/57013 loss: 4.984012779797859\n",
      "epoch: 0 sentence: 53150/57013 loss: 4.301023899091384\n",
      "epoch: 0 sentence: 53160/57013 loss: 3.7535151518690677\n",
      "epoch: 0 sentence: 53170/57013 loss: 4.157190729621515\n",
      "epoch: 0 sentence: 53180/57013 loss: 4.720383350816578\n",
      "epoch: 0 sentence: 53190/57013 loss: 4.155099159269874\n",
      "epoch: 0 sentence: 53200/57013 loss: 6.024348027352694\n",
      "epoch: 0 sentence: 53210/57013 loss: 4.694006126453562\n",
      "epoch: 0 sentence: 53220/57013 loss: 4.856655123953443\n",
      "epoch: 0 sentence: 53230/57013 loss: 2.4446197772868716\n",
      "epoch: 0 sentence: 53240/57013 loss: 3.509489215261256\n",
      "epoch: 0 sentence: 53250/57013 loss: 4.767129975139431\n",
      "epoch: 0 sentence: 53260/57013 loss: 5.5203736079856025\n",
      "epoch: 0 sentence: 53270/57013 loss: 5.435506189099659\n",
      "epoch: 0 sentence: 53280/57013 loss: 5.871999892017312\n",
      "epoch: 0 sentence: 53290/57013 loss: 4.8854658668328295\n",
      "epoch: 0 sentence: 53300/57013 loss: 5.9026770758926945\n",
      "epoch: 0 sentence: 53310/57013 loss: 5.3654047446602275\n",
      "epoch: 0 sentence: 53320/57013 loss: 4.0789986143275625\n",
      "epoch: 0 sentence: 53330/57013 loss: 4.920969767136822\n",
      "epoch: 0 sentence: 53340/57013 loss: 3.6092028151048043\n",
      "epoch: 0 sentence: 53350/57013 loss: 1.528188725088711\n",
      "epoch: 0 sentence: 53360/57013 loss: 3.3690020911046523\n",
      "epoch: 0 sentence: 53370/57013 loss: 4.282036229600255\n",
      "epoch: 0 sentence: 53380/57013 loss: 4.289322571678853\n",
      "epoch: 0 sentence: 53390/57013 loss: 4.03812013526491\n",
      "epoch: 0 sentence: 53400/57013 loss: 5.676810921674795\n",
      "epoch: 0 sentence: 53410/57013 loss: 4.247410227606305\n",
      "epoch: 0 sentence: 53420/57013 loss: 3.2754689927147607\n",
      "epoch: 0 sentence: 53430/57013 loss: 3.9556282355092067\n",
      "epoch: 0 sentence: 53440/57013 loss: 4.643432837690702\n",
      "epoch: 0 sentence: 53450/57013 loss: 3.9286101630661214\n",
      "epoch: 0 sentence: 53460/57013 loss: 5.5057383592481965\n",
      "epoch: 0 sentence: 53470/57013 loss: 4.001284187802874\n",
      "epoch: 0 sentence: 53480/57013 loss: 5.439502475186472\n",
      "epoch: 0 sentence: 53490/57013 loss: 3.4402809293883676\n",
      "epoch: 0 sentence: 53500/57013 loss: 4.501435469685494\n",
      "epoch: 0 sentence: 53510/57013 loss: 3.8090237690912683\n",
      "epoch: 0 sentence: 53520/57013 loss: 4.3913571284720145\n",
      "epoch: 0 sentence: 53530/57013 loss: 5.417334413911972\n",
      "epoch: 0 sentence: 53540/57013 loss: 5.022450191977803\n",
      "epoch: 0 sentence: 53550/57013 loss: 4.577737069878304\n",
      "epoch: 0 sentence: 53560/57013 loss: 4.477575061544655\n",
      "epoch: 0 sentence: 53570/57013 loss: 3.206630899513536\n",
      "epoch: 0 sentence: 53580/57013 loss: 5.223057101139784\n",
      "epoch: 0 sentence: 53590/57013 loss: 4.972701182596838\n",
      "epoch: 0 sentence: 53600/57013 loss: 4.706772739858356\n",
      "epoch: 0 sentence: 53610/57013 loss: 4.059179937166977\n",
      "epoch: 0 sentence: 53620/57013 loss: 4.874341236166758\n",
      "epoch: 0 sentence: 53630/57013 loss: 4.067801761097421\n",
      "epoch: 0 sentence: 53640/57013 loss: 4.975598518488754\n",
      "epoch: 0 sentence: 53650/57013 loss: 4.653563572476896\n",
      "epoch: 0 sentence: 53660/57013 loss: 5.177657467696136\n",
      "epoch: 0 sentence: 53670/57013 loss: 5.752700582877973\n",
      "epoch: 0 sentence: 53680/57013 loss: 3.2512163126585842\n",
      "epoch: 0 sentence: 53690/57013 loss: 5.248865371238255\n",
      "epoch: 0 sentence: 53700/57013 loss: 5.301466503643176\n",
      "epoch: 0 sentence: 53710/57013 loss: 3.8060396549080524\n",
      "epoch: 0 sentence: 53720/57013 loss: 4.670739324341332\n",
      "epoch: 0 sentence: 53730/57013 loss: 4.569228480418845\n",
      "epoch: 0 sentence: 53740/57013 loss: 4.738308586209223\n",
      "epoch: 0 sentence: 53750/57013 loss: 3.674778395208644\n",
      "epoch: 0 sentence: 53760/57013 loss: 4.227992868174631\n",
      "epoch: 0 sentence: 53770/57013 loss: 4.860892698035353\n",
      "epoch: 0 sentence: 53780/57013 loss: 3.5436125506746268\n",
      "epoch: 0 sentence: 53790/57013 loss: 4.335874399880667\n",
      "epoch: 0 sentence: 53800/57013 loss: 4.8211394528566025\n",
      "epoch: 0 sentence: 53810/57013 loss: 4.588044883522493\n",
      "epoch: 0 sentence: 53820/57013 loss: 4.959426044576618\n",
      "epoch: 0 sentence: 53830/57013 loss: 4.502391756300426\n",
      "epoch: 0 sentence: 53840/57013 loss: 4.234341940429064\n",
      "epoch: 0 sentence: 53850/57013 loss: 5.6461147330134285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 53860/57013 loss: 4.49593014904547\n",
      "epoch: 0 sentence: 53870/57013 loss: 4.043547344977572\n",
      "epoch: 0 sentence: 53880/57013 loss: 5.725597095326617\n",
      "epoch: 0 sentence: 53890/57013 loss: 5.636465047158478\n",
      "epoch: 0 sentence: 53900/57013 loss: 4.057472227048081\n",
      "epoch: 0 sentence: 53910/57013 loss: 5.1864469902020645\n",
      "epoch: 0 sentence: 53920/57013 loss: 5.795782237269687\n",
      "epoch: 0 sentence: 53930/57013 loss: 3.5521339591276617\n",
      "epoch: 0 sentence: 53940/57013 loss: 3.5937550815714006\n",
      "epoch: 0 sentence: 53950/57013 loss: 3.516701316716969\n",
      "epoch: 0 sentence: 53960/57013 loss: 4.874046559750309\n",
      "epoch: 0 sentence: 53970/57013 loss: 2.763363552649927\n",
      "epoch: 0 sentence: 53980/57013 loss: 5.126913280536165\n",
      "epoch: 0 sentence: 53990/57013 loss: 3.161232503731731\n",
      "epoch: 0 sentence: 54000/57013 loss: 5.427087923971754\n",
      "epoch: 0 sentence: 54010/57013 loss: 4.9172584962152905\n",
      "epoch: 0 sentence: 54020/57013 loss: 2.3660001137960496\n",
      "epoch: 0 sentence: 54030/57013 loss: 4.835435697811043\n",
      "epoch: 0 sentence: 54040/57013 loss: 5.163116962252597\n",
      "epoch: 0 sentence: 54050/57013 loss: 4.464724561135682\n",
      "epoch: 0 sentence: 54060/57013 loss: 4.579592385479312\n",
      "epoch: 0 sentence: 54070/57013 loss: 2.7744791540709537\n",
      "epoch: 0 sentence: 54080/57013 loss: 4.706415377480181\n",
      "epoch: 0 sentence: 54090/57013 loss: 3.2928136282434606\n",
      "epoch: 0 sentence: 54100/57013 loss: 4.124304703948014\n",
      "epoch: 0 sentence: 54110/57013 loss: 5.673592959318673\n",
      "epoch: 0 sentence: 54120/57013 loss: 4.477279665715525\n",
      "epoch: 0 sentence: 54130/57013 loss: 5.4078175114019\n",
      "epoch: 0 sentence: 54140/57013 loss: 5.063451237060351\n",
      "epoch: 0 sentence: 54150/57013 loss: 4.264727168340856\n",
      "epoch: 0 sentence: 54160/57013 loss: 4.7708225833626114\n",
      "epoch: 0 sentence: 54170/57013 loss: 5.342508201941493\n",
      "epoch: 0 sentence: 54180/57013 loss: 4.24056129375426\n",
      "epoch: 0 sentence: 54190/57013 loss: 3.787620491623718\n",
      "epoch: 0 sentence: 54200/57013 loss: 5.294033623533353\n",
      "epoch: 0 sentence: 54210/57013 loss: 3.9703981686038374\n",
      "epoch: 0 sentence: 54220/57013 loss: 5.556309308960435\n",
      "epoch: 0 sentence: 54230/57013 loss: 5.804031800308488\n",
      "epoch: 0 sentence: 54240/57013 loss: 5.934318232063393\n",
      "epoch: 0 sentence: 54250/57013 loss: 3.9563357713133738\n",
      "epoch: 0 sentence: 54260/57013 loss: 4.674866941921373\n",
      "epoch: 0 sentence: 54270/57013 loss: 4.802593538273838\n",
      "epoch: 0 sentence: 54280/57013 loss: 4.593367182430113\n",
      "epoch: 0 sentence: 54290/57013 loss: 3.8751389084602295\n",
      "epoch: 0 sentence: 54300/57013 loss: 3.3228440790038265\n",
      "epoch: 0 sentence: 54310/57013 loss: 5.057173168939229\n",
      "epoch: 0 sentence: 54320/57013 loss: 4.168248277368649\n",
      "epoch: 0 sentence: 54330/57013 loss: 4.338796328766688\n",
      "epoch: 0 sentence: 54340/57013 loss: 5.2554672506168325\n",
      "epoch: 0 sentence: 54350/57013 loss: 4.619679129961855\n",
      "epoch: 0 sentence: 54360/57013 loss: 5.112666912621801\n",
      "epoch: 0 sentence: 54370/57013 loss: 4.148748417836954\n",
      "epoch: 0 sentence: 54380/57013 loss: 5.0089672253293065\n",
      "epoch: 0 sentence: 54390/57013 loss: 4.7172024972689925\n",
      "epoch: 0 sentence: 54400/57013 loss: 2.8654004816719887\n",
      "epoch: 0 sentence: 54410/57013 loss: 5.485250078978246\n",
      "epoch: 0 sentence: 54420/57013 loss: 4.031037132402287\n",
      "epoch: 0 sentence: 54430/57013 loss: 4.758443817397007\n",
      "epoch: 0 sentence: 54440/57013 loss: 4.598646396446984\n",
      "epoch: 0 sentence: 54450/57013 loss: 4.233295746709436\n",
      "epoch: 0 sentence: 54460/57013 loss: 4.33336948282151\n",
      "epoch: 0 sentence: 54470/57013 loss: 5.351908239149812\n",
      "epoch: 0 sentence: 54480/57013 loss: 5.069048560923943\n",
      "epoch: 0 sentence: 54490/57013 loss: 4.375285324029693\n",
      "epoch: 0 sentence: 54500/57013 loss: 4.431058783358962\n",
      "epoch: 0 sentence: 54510/57013 loss: 3.4169090127319826\n",
      "epoch: 0 sentence: 54520/57013 loss: 4.846293854545246\n",
      "epoch: 0 sentence: 54530/57013 loss: 4.0765400259849525\n",
      "epoch: 0 sentence: 54540/57013 loss: 3.8576893392871567\n",
      "epoch: 0 sentence: 54550/57013 loss: 4.76059988860456\n",
      "epoch: 0 sentence: 54560/57013 loss: 2.8983522205344436\n",
      "epoch: 0 sentence: 54570/57013 loss: 3.949720815205498\n",
      "epoch: 0 sentence: 54580/57013 loss: 4.804092420090088\n",
      "epoch: 0 sentence: 54590/57013 loss: 4.807204141311947\n",
      "epoch: 0 sentence: 54600/57013 loss: 4.315564663903361\n",
      "epoch: 0 sentence: 54610/57013 loss: 4.395932739893489\n",
      "epoch: 0 sentence: 54620/57013 loss: 3.876615855937921\n",
      "epoch: 0 sentence: 54630/57013 loss: 3.7115698619208186\n",
      "epoch: 0 sentence: 54640/57013 loss: 4.732579824009939\n",
      "epoch: 0 sentence: 54650/57013 loss: 4.238875095585036\n",
      "epoch: 0 sentence: 54660/57013 loss: 5.534224114335063\n",
      "epoch: 0 sentence: 54670/57013 loss: 4.336050620993878\n",
      "epoch: 0 sentence: 54680/57013 loss: 6.515099753094931\n",
      "epoch: 0 sentence: 54690/57013 loss: 4.340541135213358\n",
      "epoch: 0 sentence: 54700/57013 loss: 4.816912446639855\n",
      "epoch: 0 sentence: 54710/57013 loss: 4.4086323797739855\n",
      "epoch: 0 sentence: 54720/57013 loss: 4.156570470548608\n",
      "epoch: 0 sentence: 54730/57013 loss: 4.573362277785814\n",
      "epoch: 0 sentence: 54740/57013 loss: 3.6652901185845237\n",
      "epoch: 0 sentence: 54750/57013 loss: 4.963369098448718\n",
      "epoch: 0 sentence: 54760/57013 loss: 4.400125025760722\n",
      "epoch: 0 sentence: 54770/57013 loss: 5.9152206570380415\n",
      "epoch: 0 sentence: 54780/57013 loss: 3.511964347699516\n",
      "epoch: 0 sentence: 54790/57013 loss: 3.606753281749633\n",
      "epoch: 0 sentence: 54800/57013 loss: 4.868675560710862\n",
      "epoch: 0 sentence: 54810/57013 loss: 3.997449848817937\n",
      "epoch: 0 sentence: 54820/57013 loss: 4.241209286519314\n",
      "epoch: 0 sentence: 54830/57013 loss: 4.565560854439157\n",
      "epoch: 0 sentence: 54840/57013 loss: 5.908605434342205\n",
      "epoch: 0 sentence: 54850/57013 loss: 5.14608295323823\n",
      "epoch: 0 sentence: 54860/57013 loss: 5.1791268108569914\n",
      "epoch: 0 sentence: 54870/57013 loss: 4.395693016460772\n",
      "epoch: 0 sentence: 54880/57013 loss: 5.521413053967838\n",
      "epoch: 0 sentence: 54890/57013 loss: 4.509106294923579\n",
      "epoch: 0 sentence: 54900/57013 loss: 3.889497852258705\n",
      "epoch: 0 sentence: 54910/57013 loss: 4.370728856227924\n",
      "epoch: 0 sentence: 54920/57013 loss: 4.1780053341834265\n",
      "epoch: 0 sentence: 54930/57013 loss: 4.447327601696623\n",
      "epoch: 0 sentence: 54940/57013 loss: 4.209453390224751\n",
      "epoch: 0 sentence: 54950/57013 loss: 4.316354561197926\n",
      "epoch: 0 sentence: 54960/57013 loss: 4.769886945437173\n",
      "epoch: 0 sentence: 54970/57013 loss: 4.775017969017547\n",
      "epoch: 0 sentence: 54980/57013 loss: 6.099382190394235\n",
      "epoch: 0 sentence: 54990/57013 loss: 5.554145935467385\n",
      "epoch: 0 sentence: 55000/57013 loss: 3.6923051684424824\n",
      "epoch: 0 sentence: 55010/57013 loss: 5.402743667501864\n",
      "epoch: 0 sentence: 55020/57013 loss: 5.2057720600067405\n",
      "epoch: 0 sentence: 55030/57013 loss: 4.5996901519498925\n",
      "epoch: 0 sentence: 55040/57013 loss: 4.125467817039434\n",
      "epoch: 0 sentence: 55050/57013 loss: 3.1487919515463605\n",
      "epoch: 0 sentence: 55060/57013 loss: 5.1047216575858005\n",
      "epoch: 0 sentence: 55070/57013 loss: 4.418630427190016\n",
      "epoch: 0 sentence: 55080/57013 loss: 4.52857992823237\n",
      "epoch: 0 sentence: 55090/57013 loss: 5.819762566402761\n",
      "epoch: 0 sentence: 55100/57013 loss: 4.639295966036069\n",
      "epoch: 0 sentence: 55110/57013 loss: 5.1264846056392\n",
      "epoch: 0 sentence: 55120/57013 loss: 4.222836462652606\n",
      "epoch: 0 sentence: 55130/57013 loss: 4.22681055398123\n",
      "epoch: 0 sentence: 55140/57013 loss: 4.1353624931541075\n",
      "epoch: 0 sentence: 55150/57013 loss: 5.200366881187788\n",
      "epoch: 0 sentence: 55160/57013 loss: 3.902707339600552\n",
      "epoch: 0 sentence: 55170/57013 loss: 3.938537187232062\n",
      "epoch: 0 sentence: 55180/57013 loss: 4.306646457700072\n",
      "epoch: 0 sentence: 55190/57013 loss: 4.025645473127955\n",
      "epoch: 0 sentence: 55200/57013 loss: 5.85870174719454\n",
      "epoch: 0 sentence: 55210/57013 loss: 4.158498152548897\n",
      "epoch: 0 sentence: 55220/57013 loss: 5.570022392273152\n",
      "epoch: 0 sentence: 55230/57013 loss: 4.2847709318897325\n",
      "epoch: 0 sentence: 55240/57013 loss: 4.10555700748375\n",
      "epoch: 0 sentence: 55250/57013 loss: 4.605391584417952\n",
      "epoch: 0 sentence: 55260/57013 loss: 2.0091707537793666\n",
      "epoch: 0 sentence: 55270/57013 loss: 3.346811884649629\n",
      "epoch: 0 sentence: 55280/57013 loss: 5.272992512114288\n",
      "epoch: 0 sentence: 55290/57013 loss: 5.282472608614549\n",
      "epoch: 0 sentence: 55300/57013 loss: 4.03795784749224\n",
      "epoch: 0 sentence: 55310/57013 loss: 4.246028990978091\n",
      "epoch: 0 sentence: 55320/57013 loss: 4.052835015952058\n",
      "epoch: 0 sentence: 55330/57013 loss: 5.269499333389831\n",
      "epoch: 0 sentence: 55340/57013 loss: 4.662037769531255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 55350/57013 loss: 5.241875642135931\n",
      "epoch: 0 sentence: 55360/57013 loss: 4.768495165935033\n",
      "epoch: 0 sentence: 55370/57013 loss: 4.671534207574445\n",
      "epoch: 0 sentence: 55380/57013 loss: 5.101217316882063\n",
      "epoch: 0 sentence: 55390/57013 loss: 4.363259140397438\n",
      "epoch: 0 sentence: 55400/57013 loss: 3.366264206375972\n",
      "epoch: 0 sentence: 55410/57013 loss: 3.5958825621750727\n",
      "epoch: 0 sentence: 55420/57013 loss: 4.049642224289853\n",
      "epoch: 0 sentence: 55430/57013 loss: 5.5083735536817215\n",
      "epoch: 0 sentence: 55440/57013 loss: 3.9167970072533693\n",
      "epoch: 0 sentence: 55450/57013 loss: 5.858030042379502\n",
      "epoch: 0 sentence: 55460/57013 loss: 3.883748592354608\n",
      "epoch: 0 sentence: 55470/57013 loss: 2.695692021995085\n",
      "epoch: 0 sentence: 55480/57013 loss: 4.60698136895008\n",
      "epoch: 0 sentence: 55490/57013 loss: 5.620671970064575\n",
      "epoch: 0 sentence: 55500/57013 loss: 4.431082955316702\n",
      "epoch: 0 sentence: 55510/57013 loss: 4.312632501227875\n",
      "epoch: 0 sentence: 55520/57013 loss: 3.608675474381448\n",
      "epoch: 0 sentence: 55530/57013 loss: 5.2888505109965145\n",
      "epoch: 0 sentence: 55540/57013 loss: 3.375248711742298\n",
      "epoch: 0 sentence: 55550/57013 loss: 4.621340521914258\n",
      "epoch: 0 sentence: 55560/57013 loss: 3.9871862382752497\n",
      "epoch: 0 sentence: 55570/57013 loss: 2.4684782981756483\n",
      "epoch: 0 sentence: 55580/57013 loss: 4.522906516785573\n",
      "epoch: 0 sentence: 55590/57013 loss: 3.7229020949077127\n",
      "epoch: 0 sentence: 55600/57013 loss: 5.273819539593649\n",
      "epoch: 0 sentence: 55610/57013 loss: 5.1181916304241435\n",
      "epoch: 0 sentence: 55620/57013 loss: 4.752076012326444\n",
      "epoch: 0 sentence: 55630/57013 loss: 4.662230802929615\n",
      "epoch: 0 sentence: 55640/57013 loss: 4.578129373677998\n",
      "epoch: 0 sentence: 55650/57013 loss: 5.297437578558672\n",
      "epoch: 0 sentence: 55660/57013 loss: 2.760662970897516\n",
      "epoch: 0 sentence: 55670/57013 loss: 3.7044570822139096\n",
      "epoch: 0 sentence: 55680/57013 loss: 1.8900318986802938\n",
      "epoch: 0 sentence: 55690/57013 loss: 1.928316241531783\n",
      "epoch: 0 sentence: 55700/57013 loss: 5.475239496372197\n",
      "epoch: 0 sentence: 55710/57013 loss: 3.6327212040771726\n",
      "epoch: 0 sentence: 55720/57013 loss: 4.39459354835977\n",
      "epoch: 0 sentence: 55730/57013 loss: 4.009212465213636\n",
      "epoch: 0 sentence: 55740/57013 loss: 4.081644925549711\n",
      "epoch: 0 sentence: 55750/57013 loss: 4.898557644777136\n",
      "epoch: 0 sentence: 55760/57013 loss: 5.163224079056398\n",
      "epoch: 0 sentence: 55770/57013 loss: 5.04646369931097\n",
      "epoch: 0 sentence: 55780/57013 loss: 5.149248144704105\n",
      "epoch: 0 sentence: 55790/57013 loss: 4.672875153586792\n",
      "epoch: 0 sentence: 55800/57013 loss: 5.097661867837343\n",
      "epoch: 0 sentence: 55810/57013 loss: 3.899047828355674\n",
      "epoch: 0 sentence: 55820/57013 loss: 4.112402309164239\n",
      "epoch: 0 sentence: 55830/57013 loss: 1.4942370355566448\n",
      "epoch: 0 sentence: 55840/57013 loss: 4.7556492557259356\n",
      "epoch: 0 sentence: 55850/57013 loss: 4.987733858169839\n",
      "epoch: 0 sentence: 55860/57013 loss: 4.028742340095193\n",
      "epoch: 0 sentence: 55870/57013 loss: 2.865959593331022\n",
      "epoch: 0 sentence: 55880/57013 loss: 4.194153855702876\n",
      "epoch: 0 sentence: 55890/57013 loss: 5.342466425757763\n",
      "epoch: 0 sentence: 55900/57013 loss: 2.4391636487909136\n",
      "epoch: 0 sentence: 55910/57013 loss: 4.89355907038115\n",
      "epoch: 0 sentence: 55920/57013 loss: 4.1575857823292806\n",
      "epoch: 0 sentence: 55930/57013 loss: 5.01286875638229\n",
      "epoch: 0 sentence: 55940/57013 loss: 2.9715520460958493\n",
      "epoch: 0 sentence: 55950/57013 loss: 3.7337764162734834\n",
      "epoch: 0 sentence: 55960/57013 loss: 5.607864606970766\n",
      "epoch: 0 sentence: 55970/57013 loss: 3.7122291039879007\n",
      "epoch: 0 sentence: 55980/57013 loss: 2.543822821958875\n",
      "epoch: 0 sentence: 55990/57013 loss: 5.378850674392194\n",
      "epoch: 0 sentence: 56000/57013 loss: 4.293756511169096\n",
      "epoch: 0 sentence: 56010/57013 loss: 4.0331710324816825\n",
      "epoch: 0 sentence: 56020/57013 loss: 5.631934749967549\n",
      "epoch: 0 sentence: 56030/57013 loss: 4.16681633658823\n",
      "epoch: 0 sentence: 56040/57013 loss: 4.698525869378569\n",
      "epoch: 0 sentence: 56050/57013 loss: 4.723431270811431\n",
      "epoch: 0 sentence: 56060/57013 loss: 3.8237750665241146\n",
      "epoch: 0 sentence: 56070/57013 loss: 3.427768121596697\n",
      "epoch: 0 sentence: 56080/57013 loss: 4.982823126863007\n",
      "epoch: 0 sentence: 56090/57013 loss: 5.059407116932874\n",
      "epoch: 0 sentence: 56100/57013 loss: 4.377205581928687\n",
      "epoch: 0 sentence: 56110/57013 loss: 4.790728580842013\n",
      "epoch: 0 sentence: 56120/57013 loss: 3.915372818321509\n",
      "epoch: 0 sentence: 56130/57013 loss: 3.087560878678705\n",
      "epoch: 0 sentence: 56140/57013 loss: 4.56235605057128\n",
      "epoch: 0 sentence: 56150/57013 loss: 3.7145234465127377\n",
      "epoch: 0 sentence: 56160/57013 loss: 3.221731994453112\n",
      "epoch: 0 sentence: 56170/57013 loss: 3.5095984818763313\n",
      "epoch: 0 sentence: 56180/57013 loss: 4.77491507978398\n",
      "epoch: 0 sentence: 56190/57013 loss: 5.197490764900578\n",
      "epoch: 0 sentence: 56200/57013 loss: 3.513467814992701\n",
      "epoch: 0 sentence: 56210/57013 loss: 4.375536354869371\n",
      "epoch: 0 sentence: 56220/57013 loss: 4.385552191222163\n",
      "epoch: 0 sentence: 56230/57013 loss: 5.1165407903505296\n",
      "epoch: 0 sentence: 56240/57013 loss: 3.9216196564257415\n",
      "epoch: 0 sentence: 56250/57013 loss: 4.615256011386646\n",
      "epoch: 0 sentence: 56260/57013 loss: 4.864278651716808\n",
      "epoch: 0 sentence: 56270/57013 loss: 3.991818707241806\n",
      "epoch: 0 sentence: 56280/57013 loss: 3.4630603990644797\n",
      "epoch: 0 sentence: 56290/57013 loss: 4.391960676438619\n",
      "epoch: 0 sentence: 56300/57013 loss: 3.6613351879095792\n",
      "epoch: 0 sentence: 56310/57013 loss: 5.0067119098559685\n",
      "epoch: 0 sentence: 56320/57013 loss: 2.736612024447907\n",
      "epoch: 0 sentence: 56330/57013 loss: 4.944170119532352\n",
      "epoch: 0 sentence: 56340/57013 loss: 5.553520260670862\n",
      "epoch: 0 sentence: 56350/57013 loss: 4.1422333093610995\n",
      "epoch: 0 sentence: 56360/57013 loss: 3.9198382298376817\n",
      "epoch: 0 sentence: 56370/57013 loss: 5.336560506650857\n",
      "epoch: 0 sentence: 56380/57013 loss: 5.800127252697156\n",
      "epoch: 0 sentence: 56390/57013 loss: 4.802479494003045\n",
      "epoch: 0 sentence: 56400/57013 loss: 5.669487856416291\n",
      "epoch: 0 sentence: 56410/57013 loss: 4.100356174926691\n",
      "epoch: 0 sentence: 56420/57013 loss: 5.673402789913761\n",
      "epoch: 0 sentence: 56430/57013 loss: 4.908174989184572\n",
      "epoch: 0 sentence: 56440/57013 loss: 4.370215149664356\n",
      "epoch: 0 sentence: 56450/57013 loss: 4.615088469115989\n",
      "epoch: 0 sentence: 56460/57013 loss: 5.27756512687003\n",
      "epoch: 0 sentence: 56470/57013 loss: 3.988953891748559\n",
      "epoch: 0 sentence: 56480/57013 loss: 4.026135942673582\n",
      "epoch: 0 sentence: 56490/57013 loss: 3.6620306521639727\n",
      "epoch: 0 sentence: 56500/57013 loss: 2.1088569888382502\n",
      "epoch: 0 sentence: 56510/57013 loss: 3.3290783453105828\n",
      "epoch: 0 sentence: 56520/57013 loss: 4.966567582283574\n",
      "epoch: 0 sentence: 56530/57013 loss: 4.090598998782678\n",
      "epoch: 0 sentence: 56540/57013 loss: 4.603617985076554\n",
      "epoch: 0 sentence: 56550/57013 loss: 3.236571091628195\n",
      "epoch: 0 sentence: 56560/57013 loss: 4.380833991484447\n",
      "epoch: 0 sentence: 56570/57013 loss: 5.876859104733308\n",
      "epoch: 0 sentence: 56580/57013 loss: 5.105452817367949\n",
      "epoch: 0 sentence: 56590/57013 loss: 5.1931303377093085\n",
      "epoch: 0 sentence: 56600/57013 loss: 2.238864148617886\n",
      "epoch: 0 sentence: 56610/57013 loss: 4.08546039489073\n",
      "epoch: 0 sentence: 56620/57013 loss: 4.739490433895842\n",
      "epoch: 0 sentence: 56630/57013 loss: 4.280832569218472\n",
      "epoch: 0 sentence: 56640/57013 loss: 3.46526855948085\n",
      "epoch: 0 sentence: 56650/57013 loss: 6.27288267701182\n",
      "epoch: 0 sentence: 56660/57013 loss: 4.862350432748211\n",
      "epoch: 0 sentence: 56670/57013 loss: 5.5084933583203055\n",
      "epoch: 0 sentence: 56680/57013 loss: 3.8146149280708066\n",
      "epoch: 0 sentence: 56690/57013 loss: 3.7590267841258838\n",
      "epoch: 0 sentence: 56700/57013 loss: 4.653352762231421\n",
      "epoch: 0 sentence: 56710/57013 loss: 3.7422870615744395\n",
      "epoch: 0 sentence: 56720/57013 loss: 3.9900877636741\n",
      "epoch: 0 sentence: 56730/57013 loss: 4.986890596367731\n",
      "epoch: 0 sentence: 56740/57013 loss: 3.7474461420211544\n",
      "epoch: 0 sentence: 56750/57013 loss: 5.357532701178129\n",
      "epoch: 0 sentence: 56760/57013 loss: 5.503683420462079\n",
      "epoch: 0 sentence: 56770/57013 loss: 4.046622212584802\n",
      "epoch: 0 sentence: 56780/57013 loss: 4.413647114771413\n",
      "epoch: 0 sentence: 56790/57013 loss: 4.647293398420301\n",
      "epoch: 0 sentence: 56800/57013 loss: 4.846300750213448\n",
      "epoch: 0 sentence: 56810/57013 loss: 4.319080381142613\n",
      "epoch: 0 sentence: 56820/57013 loss: 3.9321806156194796\n",
      "epoch: 0 sentence: 56830/57013 loss: 4.756796167280511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 sentence: 56840/57013 loss: 5.286891963356859\n",
      "epoch: 0 sentence: 56850/57013 loss: 5.100164550122602\n",
      "epoch: 0 sentence: 56860/57013 loss: 3.710546711791358\n",
      "epoch: 0 sentence: 56870/57013 loss: 3.227890370318649\n",
      "epoch: 0 sentence: 56880/57013 loss: 5.087238908129031\n",
      "epoch: 0 sentence: 56890/57013 loss: 4.740916765939279\n",
      "epoch: 0 sentence: 56900/57013 loss: 4.336270506587508\n",
      "epoch: 0 sentence: 56910/57013 loss: 1.7023424797379973\n",
      "epoch: 0 sentence: 56920/57013 loss: 3.1700702232965643\n",
      "epoch: 0 sentence: 56930/57013 loss: 4.984565866148129\n",
      "epoch: 0 sentence: 56940/57013 loss: 4.612917122583947\n",
      "epoch: 0 sentence: 56950/57013 loss: 3.9128394691166974\n",
      "epoch: 0 sentence: 56960/57013 loss: 3.652357494344897\n",
      "epoch: 0 sentence: 56970/57013 loss: 3.4977791238965317\n",
      "epoch: 0 sentence: 56980/57013 loss: 4.068184384230703\n",
      "epoch: 0 sentence: 56990/57013 loss: 4.150800523120144\n",
      "epoch: 0 sentence: 57000/57013 loss: 4.543242753235747\n",
      "epoch: 0 sentence: 57010/57013 loss: 2.615081807406654\n",
      "Elapsed time training: 0:54:30.686724\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-0175e5001a8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Elapsed time training:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;31m# plot a horizontal line for the bigram loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "sentences, word2idx = get_sentences_with_word2idx_limit_vocab(2000)\n",
    "# sentences, word2idx = get_sentences_with_word2idx()\n",
    "\n",
    "# vocab size\n",
    "V = len(word2idx)\n",
    "print(\"Vocab size:\", V)\n",
    "\n",
    "# we will also treat beginning of sentence and end of sentence as bigrams\n",
    "# START -> first word\n",
    "# last word -> END\n",
    "start_idx = word2idx['START']\n",
    "end_idx = word2idx['END']\n",
    "\n",
    "\n",
    "# a matrix where:\n",
    "# row = last word\n",
    "# col = current word\n",
    "# value at [row, col] = p(current word | last word)\n",
    "bigram_probs = get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=0.1)\n",
    "\n",
    "# train a logistic model\n",
    "W = np.random.randn(V, V) / np.sqrt(V)\n",
    "\n",
    "losses = []\n",
    "epochs = 1\n",
    "lr = 1e-1\n",
    "  \n",
    "def softmax(a):\n",
    "    a = a - a.max()\n",
    "    exp_a = np.exp(a)\n",
    "    return exp_a / exp_a.sum(axis=1, keepdims=True)\n",
    "\n",
    "# what is the loss if we set W = log(bigram_probs)?\n",
    "W_bigram = np.log(bigram_probs)\n",
    "bigram_losses = []\n",
    "\n",
    "\n",
    "t0 = datetime.now()\n",
    "for epoch in range(epochs):\n",
    "    # shuffle sentences at each epoch\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    j = 0 # keep track of iterations\n",
    "    for sentence in sentences:\n",
    "        # convert sentence into one-hot encoded inputs and targets\n",
    "        sentence = [start_idx] + sentence + [end_idx]\n",
    "        n = len(sentence)\n",
    "        inputs = np.zeros((n - 1, V))\n",
    "        targets = np.zeros((n - 1, V))\n",
    "        inputs[np.arange(n - 1), sentence[:n-1]] = 1\n",
    "        targets[np.arange(n - 1), sentence[1:]] = 1\n",
    "\n",
    "        # get output predictions\n",
    "        predictions = softmax(inputs.dot(W))\n",
    "\n",
    "        # do a gradient descent step\n",
    "        W = W - lr * inputs.T.dot(predictions - targets)\n",
    "\n",
    "        # keep track of the loss\n",
    "        loss = -np.sum(targets * np.log(predictions)) / (n - 1)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # keep track of the bigram loss\n",
    "        # only do it for the first epoch to avoid redundancy\n",
    "        if epoch == 0:\n",
    "            bigram_predictions = softmax(inputs.dot(W_bigram))\n",
    "            bigram_loss = -np.sum(targets * np.log(bigram_predictions)) / (n - 1)\n",
    "            bigram_losses.append(bigram_loss)\n",
    "\n",
    "\n",
    "        if j % 10 == 0:\n",
    "            print(\"epoch:\", epoch, \"sentence: %s/%s\" % (j, len(sentences)), \"loss:\", loss)\n",
    "        j += 1\n",
    "\n",
    "print(\"Elapsed time training:\", datetime.now() - t0)\n",
    "plt.plot(losses)\n",
    "\n",
    "# plot a horizontal line for the bigram loss\n",
    "avg_bigram_loss = np.mean(bigram_losses)\n",
    "print(\"avg_bigram_loss:\", avg_bigram_loss)\n",
    "plt.axhline(y=avg_bigram_loss, color='r', linestyle='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot smoothed losses to reduce variability\n",
    "def smoothed_loss(x, decay=0.99):\n",
    "    y = np.zeros(len(x))\n",
    "    last = 0\n",
    "    for t in range(len(x)):\n",
    "        z = decay * last + (1 - decay) * x[t]\n",
    "        y[t] = z / (1 - decay ** (t + 1))\n",
    "        last = z\n",
    "    return y\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(smoothed_loss(losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that LogisticRegression is a 1-layer FNN. It can be generalized to FNN of 2 or more layers to get more complex models to the problem described in 3.2.1.\n",
    "\n",
    "If we use a LogisticRegression model with vocabulary size 10000, the number of parameters is about $10^8$.\n",
    "\n",
    "If we use a 2-layers FNN with the hidden layer of size 100, the number of parameters is about $2\\times 10^6$ (much better). Using FNN is a kind of compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LearningWord Embeddings - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Language Modeling is one way to learn Word Embeddings. (see 3.)\n",
    "\n",
    "We modelize the sentence using one-hot-vector.\n",
    "\n",
    "<img src=\"F2.png\"></img>\n",
    "\n",
    "Instead of using the whole sentence, we can use only some previous words:\n",
    "\n",
    "<img src=\"F3.png\"></img>\n",
    "\n",
    "( **skip-grams**)\n",
    "\n",
    "etc.\n",
    "\n",
    "We can learn them by Text classification, Word Analogies, Similarity... In this section, we will learn word embeddings by language modelling. In fact, we have done it in 3.2.3 with an FNN model. The transformed input at hidden vector can be represented as word embeddings for the vocabulary. Practical application has shown that word embeddings learned by bigram is not good enough. We will see some improvements of bigram and reach its modified version called **Word2Vec**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 CBOW\n",
    "\n",
    "- CBOW stands for Continuous Bag of Words\n",
    "- It is a parametric model for classification, can learn word embeddings\n",
    "- It can be represented as an FNN model\n",
    "- Different with bigram: it uses some previous and some following words as context\n",
    "\n",
    "**Model (as FNN)**\n",
    "Let $C$ denote the set of $m_1$ previous and $m_2$ following words of the targer word\n",
    "- Input: ($|C| = m_1 + m_2$ context words ($|C|\\times |V|$-dimensional vectors))\n",
    "- Output: a word ($|V|-$dimensional vector)\n",
    "\n",
    "- Layer 1:\n",
    "$$\n",
    "a_1 = \\frac1{|C|} \\sum_{c\\in C} W^{(1)}c\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_1 = a_1\n",
    "$$\n",
    "- Layer 2:\n",
    "$$\n",
    "a_2 = W^{(2)} z_1 \n",
    "$$\n",
    "\n",
    "$$\n",
    "z_2 = \\mathrm {softmax}(a_2)\n",
    "$$\n",
    "\n",
    "The model will learn parameters $W^{(1)}, W^{(2)}$. $a_1$ can therefore be viewed as a word embedding of the vocabulary. $W^{(1)}$ contains the images of the one-hot vectors representing words in the lower-dimensional subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Skipgram\n",
    "\n",
    "- Skipgram is like the opposite of CBOW\n",
    "\n",
    " Example: We have a sentence `I would like to drink some tea.`\n",
    " \n",
    " In CBOW: Input: (`I`, `would`, `to`, `drink`) $\\to$ Output: (`like`)\n",
    " \n",
    " In Skipgram: \n",
    " - Input: `like` -> Output: `I`\n",
    " - Input: `like` -> Output: `would`\n",
    " - Input: `like` -> Output: `to`\n",
    " - Input: `like` -> Output: `drink`\n",
    " \n",
    "- Skipgram is like bigram, but with some skipped-word\n",
    "  \n",
    "  In bigram: Input: `like` -> Output: `to` (only)\n",
    "  In skipgram: Possible: Input: `like` -> Output: `drink` (`to` is skipped)\n",
    "  \n",
    "- The model is similar to CBOW or bigram with FNN. Only input and output are different.\n",
    " \n",
    "** The model:**\n",
    " - Input: a word ($|V|$-dimensional vector))\n",
    " - Output: a word ($|V|-$dimensional vector)\n",
    " - Layer 1:\n",
    " $$\n",
    " a_1 = W^{(1)} x\n",
    " $$\n",
    "\n",
    " $$\n",
    " z_1 = a_1\n",
    " $$\n",
    " - Layer 2:\n",
    " $$\n",
    " a_2 = W^{(2)} z_1\n",
    " $$\n",
    "\n",
    " $$\n",
    " z_2 = \\mathrm {softmax}(a_2)\n",
    " $$\n",
    "\n",
    " Similarly, the model will learn parameters $W^{(1)}, W^{(2)}$ and predict $z_2$ as $\\mathbf P(y|x)$. Again, $W^{(1)}$ can be  viewed as a word embedding for the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Reduce Complexity of the Prediction Task\n",
    "\n",
    "If we use softmax as the output activation, the complexity is $O(|V|)$ where $V$ denotes the vocabulary. $|V|$ can be extremely large (3 million for word2vec).\n",
    "\n",
    "We consider 2 solutions to reduce the complexity: Hierarchical softmax and \n",
    "\n",
    "### 4.3.1 Hierarchical Softmax\n",
    "\n",
    "It is in fact the hierarchical strategy for multiclass classification. (See DSC101, lesson 8)\n",
    "\n",
    "The idea of hierarchical softmax is to organize words into a binary tree where leaves represent words.\n",
    "\n",
    "<img src=\"F4.png\"></img>\n",
    "\n",
    "An example of the tree is the Huffman coding (https://en.wikipedia.org/wiki/Huffman_coding). Words with high frequenct (\"a\", \"the\", \"in\", ...) are on top. Rare words are found in the bottom.\n",
    "\n",
    "Then do a first classification for the first level. The problem becomes predicting\n",
    "\n",
    "$\\mathbf P(\\textrm{\"left branch\"}| x)$, $\\mathbf P(\\textrm{\"right branch\"}| x)$\n",
    "\n",
    "and it can be done by\n",
    "$$\n",
    "\\sigma(f(x))\n",
    "$$\n",
    "\n",
    "where $f(x)$ is the output of $x$ via an FNN model before performing the sigmoid ($\\sigma$) at the output.\n",
    "\n",
    "Hence, we reduce the multiclass classification problem to $O(\\log |V|)$ binary classification problems. The output of each of these problems is just a probability. Hence the general output size reduces to $O(\\log |V|)$. This reduces the complexity in prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3.2 Negative Sampling\n",
    "\n",
    "Examples that we face in the corpus or in reality are considered as **positive** example.\n",
    "```\n",
    "like -> would\n",
    "like -> drink\n",
    "```\n",
    "are positive examples.\n",
    "\n",
    "We can think of building negative examples.\n",
    "```\n",
    "like -> like\n",
    "like -> brown\n",
    "```\n",
    "\n",
    "Then transform the problem to binary classification.\n",
    "\n",
    "** The model:**\n",
    " - Input: a context word and a target word ($(C+1)\\times|V|$-dimensional vector))\n",
    " - Output: a number between 0, 1\n",
    " - Loss function: Binary crossentropy\n",
    " $$\n",
    " J = \\sum_{c\\in C}\\log\\sigma(f(c)) + \\sum_{n\\in Neg} \\log(1-\\sigma f(n))\n",
    " $$\n",
    " ($Neg$: negative target word, $C$: positive target word)\n",
    " - Layer 1:\n",
    " $$\n",
    " a_1 = W^{(1)} x\n",
    " $$\n",
    "\n",
    " $$\n",
    " z_1 = a_1\n",
    " $$\n",
    " - Layer 2:\n",
    " $$\n",
    " a_2 = W^{(2)} z_1\n",
    " $$\n",
    "\n",
    " $$\n",
    " z_2 = \\mathrm {softmax}(a_2)\n",
    " $$\n",
    "\n",
    "This strategy will increase the training set's size but the problem becomes binary classification.\n",
    "\n",
    "In fact in application of language modelling (like speech to text), by some way we can reduce the number of potential missing words, say $K$. Doing binary classification on these words is fine to get the best word for prediction.\n",
    "\n",
    "**Important remarks for negative sampling**\n",
    "- How many negative samplings should we choose for a context word?\n",
    "  It depends on the dataset. Practical experiences show that 5 to 10 are most common choice.\n",
    "  \n",
    "- How do we choose negative samples?\n",
    "  - Use the probability of a single word occurring (for example, frequency of a word $count(w)/|V|$, or the smoothly modified one). \n",
    "  - Sampling by this probability\n",
    "  \n",
    "- Another strategy for doing negative sampling:\n",
    "\n",
    "Remind the positive examples:\n",
    "```\n",
    "like -> would\n",
    "like -> drink\n",
    "```\n",
    "\n",
    "Instead of sampling\n",
    "```\n",
    "like -> like\n",
    "like -> brown\n",
    "```\n",
    "we do an inverse job\n",
    "```\n",
    "brown -> would\n",
    "fox -> would\n",
    "```\n",
    "\n",
    "This appears to work also well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Summary about Word2vec\n",
    "\n",
    "Word2vec is a modification on the bigram neural network, where we use **skip gram** and **negative sampling** to transform a multiclass classification to a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Word Embeddings with Word2vec\n",
    "\n",
    "We see the parameters in the model\n",
    "\n",
    "- $W^{(1)}$ is a $V \\times D$ vector ($D$ is the number of hidden units)\n",
    "- $W^{(2)}$ is a $D \\times D$ vector\n",
    "\n",
    "In theory, the embedding matrix is $W^{(1)}$. In practice, we can choose one of the following matrix and the word embeddings matrix\n",
    "\n",
    "- $W^{(1)}$\n",
    "- $\\left[ W^{(1)}, (W^{(2)})^t \\right]$ (concatenation)\n",
    "- $(W^{(1)}+(W^{(2)})^t)/2$ (or another linear combination between them)\n",
    "- Normalized version of the above vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Implementation\n",
    "\n",
    "From scratch: https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/word2vec.py\n",
    "\n",
    "With Tensorflow: https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/word2vec_tf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Word Embeddings [3] - GloVe\n",
    "\n",
    "## 5.1 Recommender System\n",
    "\n",
    "The objective of a recommender system is to make a score for any pair client-product, that predicts the client's opinion to product. \n",
    "\n",
    "For examble\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>\n",
    "        </th>\n",
    "        <th>\n",
    "            Movie 1\n",
    "        </th>\n",
    "        <th>\n",
    "            Movie 2\n",
    "        </th>\n",
    "        <th>\n",
    "            Movie 3\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            User 1\n",
    "        </td>\n",
    "        <td>\n",
    "            4\n",
    "        </td>\n",
    "        <td>\n",
    "            5\n",
    "        </td>\n",
    "        <td>\n",
    "            5\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            User 2\n",
    "        </td>\n",
    "        <td>\n",
    "            3\n",
    "        </td>\n",
    "        <td>\n",
    "            4\n",
    "        </td>\n",
    "        <td>\n",
    "            2\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ...\n",
    "        </td>\n",
    "        <td>\n",
    "            ...\n",
    "        </td>        \n",
    "        <td>\n",
    "            ...\n",
    "        </td>        \n",
    "        <td>\n",
    "            ...\n",
    "        </td>  \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Suppose there are $N$ users and $M$ movies, the scores can be represented in the form of an $N \\times M$ matrix. If the users manually rate their products, we have a training set as such a matrix. But the matrix is sparse, there are lots of missing values because one user does not use all products.\n",
    "\n",
    "The recommender system can fill the missing values as result of some prediction algorithm. If a high score of user $s$ and product $p$ is found, $p$ will be recommended to $s$.\n",
    "\n",
    "Such a prediction algorithm's example is **collaborative filtering**, namely, we use other users'data to help make predictions for the interested user. The intuition is any users with the same interest can be put into one group whose action is potentially alike.\n",
    "\n",
    "<img src=\"F5.png\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Application of Recommender System in NLP\n",
    "\n",
    "In NLP, we can consider a word as a user and a document as a product. For example, it may be reasonable to get some data like this\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>\n",
    "            Count of\n",
    "        </th>\n",
    "        <th>\n",
    "            Doc 1\n",
    "        </th>\n",
    "        <th>\n",
    "            Doc 2\n",
    "        </th>\n",
    "        <th>\n",
    "            Doc 3\n",
    "        </th>\n",
    "        <th>\n",
    "            Doc 4\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            `quantum`\n",
    "        </td>\n",
    "        <td>\n",
    "            100\n",
    "        </td>\n",
    "        <td>\n",
    "            30\n",
    "        </td>\n",
    "        <td>\n",
    "            58\n",
    "        </td>\n",
    "        <td>\n",
    "            0\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            `gravity`\n",
    "        </td>\n",
    "        <td>\n",
    "            99\n",
    "        </td>\n",
    "        <td>\n",
    "            50\n",
    "        </td>\n",
    "        <td>\n",
    "            74\n",
    "        </td>\n",
    "        <td>\n",
    "            3\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            `mitochodria`\n",
    "        </td>\n",
    "        <td>\n",
    "            0\n",
    "        </td>        \n",
    "        <td>\n",
    "            1\n",
    "        </td>        \n",
    "        <td>\n",
    "            0\n",
    "        </td>\n",
    "        <td>\n",
    "            500\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            `cell membrane`\n",
    "        </td>\n",
    "        <td>\n",
    "            0\n",
    "        </td>        \n",
    "        <td>\n",
    "            0\n",
    "        </td>        \n",
    "        <td>\n",
    "            2\n",
    "        </td>\n",
    "        <td>\n",
    "            130\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "We can consider 2 words ae similar if their counts in the documents are similar.\n",
    "\n",
    "In machine learning, 2 features are called redundant if they are correlated with each other. In reality, documents are redundants. To expolot redundancy, we use dimensionality reduction like SVD or PCA.\n",
    "\n",
    "**SVD**\n",
    "\n",
    "$$\n",
    "R_{(|V|\\times D)} = U_{(|V|\\times K)} S_{(K\\times K)} V_{(K\\times D)}^T\n",
    "$$\n",
    "\n",
    "where $|V|$ size of the vocabulary, $D$ number of documents, $K$ some small number.\n",
    "\n",
    "SVD does not work well for recommender system. PCA doesn't, either.\n",
    "\n",
    "Another strategy is matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Matrix Factorization\n",
    "\n",
    "### 5.3.1 Idea\n",
    "\n",
    "Let $R$ denote the word-document matrix as shown in section 5.2. We want to write\n",
    "$$\n",
    "R \\approx \\hat R = WU^t\n",
    "$$\n",
    "\n",
    "So that the score that user $n$ gives to product $m$ is\n",
    "$$\n",
    "\\hat R_{n,m} = W_n \\cdot U_m\n",
    "$$\n",
    "\n",
    "To consider if users $n$ likes product $m$, we just calculate $R_{n,m}$.\n",
    "\n",
    "Example:\n",
    "- User 1 likes action-adventure, doesn't care about comedy, dislike romance. It is represented by $w = [1,0,1]$.\n",
    "- Movie 1 have no action-adventure, has comedy and lots of romance. It is represented by $[-1, 1, 1]$\n",
    "- By dot product we see the similarity is -2 (not recommended)\n",
    "- We can see that $[1, 0, -1]$ is an embedding for the user while $[-1, 1, 1]$ is an embedding for the movie.\n",
    "\n",
    "### 5.3.2 The Optimization Problem\n",
    "\n",
    "To factorize a matrix, we can solve the problem\n",
    "\n",
    "$$\n",
    "\\min_{W, U} J = \\sum_{(n, m)\\in\\Omega} (R_{n,m} - \\hat R_{n, m})^2 = \\sum_{(n, m)\\in\\Omega} (R_{n,m} - W_n \\cdot U_m)^2\n",
    "$$\n",
    "\n",
    "where $\\Omega$ is the set of all pairs $(n, m)$ where user $n$ rates product $m$. In the NLP problem that we translated in section 5.2, $\\Omega$ is just $V \\times\\mathcal D$ (Cartesian product between the vocabulary and the set of documents).\n",
    "\n",
    "We will also note $\\Omega_m$ the set of users $n$ who rated product $m$ and $\\Psi_n$ the set of product $m$ which user $n$ rated.\n",
    "\n",
    "### 5.3.3 Solution\n",
    "\n",
    "By derivation, we arrive to\n",
    "$$\n",
    "W_n = \\left( \\sum_{m \\in \\Psi_n} U_m U_m^t\\right)^{-1} \\sum_{m \\in \\Psi_n} R_{n,m} U_m\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "U_m = \\left( \\sum_{n \\in \\Omega_m} W_n W_n^t \\right)^{-1} \\sum_{n \\in \\Omega_m} R_{n,m} W_n\n",
    "$$\n",
    "\n",
    "We see that the solution for $W$ depends on $U$ and for $U$ depends on $W$. So, closed from solution cannot be attained, iterative methods are used in stead.\n",
    "\n",
    "### 5.3.4 Improve the Model\n",
    "\n",
    "Depending on the user, or the movie, there may be bias terms. Example, for user 1, 2.5 means average but for user 2, 2.5 means very bad.\n",
    "\n",
    "So, we would like to add some bias terms to the matrix factorization model.\n",
    "\n",
    "$$\n",
    "\\hat R_{nm} =W_n \\cdot U_m + b_n + c_m + \\mu \n",
    "$$\n",
    "\n",
    "where $b_n$ is the user bias, $c_m$ the movie bias and $\\mu$ the global average.\n",
    "\n",
    "We arrive to the optimization problem\n",
    "\n",
    "$$\n",
    "\\min_{W, U, b, c, \\mu} J = \\sum_{(n, m)\\in\\Omega} (R_{n,m} - \\hat R_{n, m})^2 = \\sum_{(n, m)\\in\\Omega} (R_{n,m} - W_n \\cdot U_m - b_n - c_m - \\mu)^2\n",
    "$$\n",
    "\n",
    "The solution becomes\n",
    "$$\n",
    "W_n = \\left( \\sum_{m \\in \\Psi_n} U_m U_m^t\\right)^{-1} \\sum_{m \\in \\Psi_n} (R_{n,m} - b_n - c_m - \\mu) U_m\n",
    "$$\n",
    "\n",
    "$$\n",
    "U_m = \\left( \\sum_{n \\in \\Omega_m} W_n W_n^t \\right)^{-1} \\sum_{n \\in \\Omega_m} (R_{n,m} -b_n - c_m - \\mu) W_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_n = \\frac1{|\\Psi_n|}\\sum_{m \\in \\Psi_n} (R_{n,m} - W_n\\cdot U_m - c_m - \\mu)\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_m = \\frac1{|\\Omega_m|}\\sum_{n \\in \\Omega_m} (R_{n,m} - W_n\\cdot U_m - b_n - \\mu)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu = \\frac1{|\\Omega|} R_{ij}\n",
    "$$\n",
    "(no need to update \\mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.5 Regularization\n",
    "\n",
    "Like other ML model, Matrix Factorization can suffer overfitting problem and regularization is one approach to reduce it.\n",
    "\n",
    "We modify the loss function, for example by adding a $L_2$-term penalty\n",
    "\n",
    "$$\n",
    "\\min_{W, U, b, c, \\mu}  J= \\sum_{(n, m)\\in\\Omega} (R_{n,m} - W_n \\cdot U_m - b_n - c_m - \\mu)^2 + \\lambda(\\Vert W\\Vert_F^2 + \\Vert U\\Vert_F^2 + \\Vert b\\Vert_2^2 + \\Vert c\\Vert_2^2)\n",
    "$$\n",
    "\n",
    "The solution is now:\n",
    "\n",
    "$$\n",
    "W_n = \\left( \\sum_{m \\in \\Psi_n} U_m U_m^t + \\lambda I \\right)^{-1} \\sum_{m \\in \\Psi_n} (R_{n,m} - b_n - c_m - \\mu) U_m\n",
    "$$\n",
    "\n",
    "$$\n",
    "U_m = \\left( \\sum_{n \\in \\Omega_m} W_n W_n^t + \\lambda I \\right)^{-1} \\sum_{n \\in \\Omega_m} (R_{n,m} -b_n - c_m - \\mu) W_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_n = \\frac1{|\\Psi_n|+\\lambda}\\sum_{m \\in \\Psi_n} (R_{n,m} - W_n\\cdot U_m - c_m - \\mu)\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_m = \\frac1{|\\Omega_m|+\\lambda}\\sum_{n \\in \\Omega_m} (R_{n,m} - W_n\\cdot U_m - b_n - \\mu)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu = \\frac1{|\\Omega|} R_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Summary about GloVe\n",
    "\n",
    "- GloVe stands for Global vectors for word representation. It uses the matrix factorization (recommender system) to learn word embeddings (instead of language modelling like word2vec)\n",
    "- Except some preprocessing, it is exactly like matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Implementation\n",
    "\n",
    "From scratch: https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/glove.py\n",
    "\n",
    "With Tensorflow: https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/glove_tf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "(3) Bengio et al., 2003, *A neural probabilistic language model.*\n",
    "\n",
    "(4) Mikolov et al., 2015, *Efficient estimation of word representatin in vector space.*\n",
    "\n",
    "(5) w2v pretrained model: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "\n",
    "(6) Glove pretrained model: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "(7) https://www.nltk.org/book/ch02.html\n",
    "\n",
    "(8) https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/\n",
    "\n",
    "(9) The Brown corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
