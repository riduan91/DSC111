{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amphi 8 - More about NLP [2]: Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Name Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Sequence-To-Sequence Model\n",
    "\n",
    "Remind that sequence-to-sequence models are RNNs that follow the below schema\n",
    "\n",
    "<img src=\"F1.png\"></img>\n",
    "\n",
    "In terms of probability, we would like to modelize the probability\n",
    "\n",
    "$$\n",
    "\\mathbf P(y^{<1>}, \\ldots, y^{<T_y>} |x^{<1>}, \\ldots, x^{<T_x>})\n",
    "$$\n",
    "\n",
    "These models are used in case of machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 The Problem\n",
    "\n",
    "Suppose we have a French sentence that can be translated to English as follows:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            $x^{<1>}$\n",
    "        </td>\n",
    "        <td>\n",
    "            $x^{<2>}$\n",
    "        </td>\n",
    "        <td>\n",
    "            $x^{<3>}$\n",
    "        </td>\n",
    "        <td>\n",
    "            $x^{<4>}$\n",
    "        </td>\n",
    "        <td>\n",
    "            $x^{<5>}$\n",
    "        </td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Jane\n",
    "        </td>\n",
    "        <td>\n",
    "            visite\n",
    "        </td>\n",
    "        <td>\n",
    "            l'Afrique\n",
    "        </td>\n",
    "        <td>\n",
    "            en\n",
    "        </td>\n",
    "        <td>\n",
    "            Septembre\n",
    "        </td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            $y^{<1>}$\n",
    "        </td>\n",
    "        <td>\n",
    "            $y^{<2>}$\n",
    "        </td>\n",
    "        <td>\n",
    "            $y^{<3>}$\n",
    "        </td>\n",
    "        <td>\n",
    "            $y^{<4>}$\n",
    "        </td>\n",
    "        <td>\n",
    "            $y^{<5>}$\n",
    "        </td>\n",
    "        <td>\n",
    "            $y^{<6>}$\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Jane\n",
    "        </td>\n",
    "        <td>\n",
    "            is\n",
    "        </td>\n",
    "        <td>\n",
    "            visiting\n",
    "        </td>\n",
    "        <td>\n",
    "            Africa\n",
    "        </td>\n",
    "        <td>\n",
    "            in\n",
    "        </td>   \n",
    "        <td>\n",
    "            September\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in section 3.1, sequence-to-sequence models can be used for this translation problem. We can feed the French sentence as input and the English one as output. If the training set (corpus) is large enough, practice experiences show that the model works well for the translation of new sentences.\n",
    "\n",
    "The big problem of this model is that the dimensionality of output is exponentially large. If we have a vocabulary of size $V$ then we ends up with $|V|^{T_y^{max}}$ possibilities of output when looking for all possibility of sentences of no more than $T_y^{max}$ words. We can reduce this complexity by an **approximate search out** algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Beam Search\n",
    "\n",
    "### 3.3.1 Basic Algorithm\n",
    "\n",
    "**Beam Search** is an example of **approximate search out** algorithm.\n",
    "\n",
    "Supposing we have a vocabulary of $|V|=10000$ words.\n",
    "\n",
    "<p align=\"center\">\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            a\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ...\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            in\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ...\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            jane\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ...\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            september\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ...\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            zyzy\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "    \n",
    "We can calculate\n",
    "$$\n",
    "\\mathbf P(y^{<1>}|x)\n",
    "$$\n",
    "\n",
    "for all words $y^{<1>}$ in the vocabulary.\n",
    "\n",
    "**Step 1**\n",
    "\n",
    "Beam search suggests picking up **3 words** with the highest conditional probabilities. For example, after calculating probability for the first word, it finds that the 3 best suitable words are \"in\", \"jane\" and \"september\"\n",
    "\n",
    "\n",
    "**Step 2**\n",
    "\n",
    "With each of these 3 choices of $y^{<1>}$, say $y^{<1>}_{option 1}, \\ldots, y^{<1>}_{option 3}$, it considers the second word and calculate\n",
    "$$\n",
    "\\mathbf P(y^{<2>}| x, y^{<1>}_{option i})\n",
    "$$\n",
    "Now, we can deduce\n",
    "$$\n",
    "\\mathbf P(y^{<1>}, y^{<2>}|x) = \\mathbf P(y^{<1>}|x) \\mathbf P(y^{<2>}| x, y^{<1>})\n",
    "$$\n",
    "\n",
    "After calculating the $3|V|$ = 30000 possibilities, it chooses the best 3 pairs of $(y^{<1>}, y^{<2>})$. For example, \"in september\", \"jane is\" and \"jane visits\".\n",
    "\n",
    "...\n",
    "\n",
    "**Repeat the steps until the \"END\" token appears in all of the 3 possibilities.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Refinement of Beam Search\n",
    "\n",
    "**Length Normalization**\n",
    "\n",
    "We talked about Beam Search as maximizing the probability\n",
    "\n",
    "$$\n",
    "\\arg\\max_y \\prod_{t=1}^{T_y} \\mathbf P(y^{<t>}|x, y^{<1>}, \\ldots, y^{<t-1>})\n",
    "$$\n",
    "\n",
    "This probability is very small. Instead, we use the log form.\n",
    "\n",
    "$$\n",
    "\\arg\\max_y \\sum_{t=1}^{T_y} \\log \\mathbf P(y^{<t>}|x, y^{<1>}, \\ldots, y^{<t-1>})\n",
    "$$\n",
    "\n",
    "Although we work with logarithm, very long sentences can reduce the performance of the algorithm. Hence, although it seems not natural, we can avoid having long translated sentences by re-evaluating\n",
    "\n",
    "$$\n",
    "\\sum_{t=1}^{T_y} \\frac1{T_y} \\log \\mathbf P(y^{<t>}|x, y^{<1>}, \\ldots, y^{<t-1>})\n",
    "$$\n",
    "\n",
    "or\n",
    "$$\n",
    "\\sum_{t=1}^{T_y} \\frac1{T_y^\\alpha} \\log \\mathbf P(y^{<t>}|x, y^{<1>}, \\ldots, y^{<t-1>})\n",
    "$$\n",
    "\n",
    "with some predefined $\\alpha$.\n",
    "\n",
    "**Modified Algorithm for Length Normalization**\n",
    "\n",
    "- Do a standard Beam Search with \n",
    "$$\n",
    "\\sum_{t=1}^{T_y}\\log \\mathbf P(y^{<t>}|x, y^{<1>}, \\ldots, y^{<t-1>})\n",
    "$$\n",
    "\n",
    "- Then, at the end, we retrieve for example 3 sentences of length 1, 3 sentences of length 2, ..., 3 sentences of length 29, 2 sentences of length 30, 2 sentences of length 31, 1 sentences of length 32. For these 92 sentences, calculate\n",
    "\n",
    "$$\n",
    "\\sum_{t=1}^{T_y} \\frac1{T_y^\\alpha} \\log \\mathbf P(y^{<t>}|x, y^{<1>}, \\ldots, y^{<t-1>})\n",
    "$$\n",
    "\n",
    "and choose the one that maximize that modified object function.\n",
    "\n",
    "**The beam width**\n",
    "\n",
    "In the algorithm we chose the beam width $B=3$. It is possible to choose other $B$. Larger $B$ gives better results, but make the algorithm slower, while small $B$ inversely.\n",
    "\n",
    "People often try $B = 1, 3, 10, 100$.\n",
    "\n",
    "Finally, keep in mind that this is just an approximate search out algorith. It does not guarantee to find the exact maximum for $\\arg\\max_y P(y|x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Error Analysis in Beam Search\n",
    "\n",
    "Suppose we have a French sentence\n",
    "\n",
    "`Jane visite l'Afrique en Septembre.`\n",
    "\n",
    "We expect a translation like `Jane visits Africa in September.`\n",
    "\n",
    "But somehow, the machine algorithm predicts: `Jane visited Africa last September.`\n",
    "\n",
    "We would like to blame the error to one of the two elements:\n",
    "\n",
    "- The RNN model\n",
    "- The Beam Search algorithm\n",
    "\n",
    "What should we blame on?\n",
    "\n",
    "Let $y^\\star$ denote the expected answer ( `Jane visits Africa in September.`) and $\\hat y$ denote the machine's answer (`Jane visited Africa last September.`).\n",
    "\n",
    "In fact, RNN computes $\\mathbf P(y|x)$. We can compute $\\mathbf P(y^\\star|x)$, $\\mathbf P(\\hat y|x)$.\n",
    "\n",
    "**Case 1**\n",
    "$$\\mathbf P(y^\\star|x) > \\mathbf P(\\hat y|x)$$\n",
    "and Beam search chose $\\hat y$.\n",
    "\n",
    "Conclusion: Beam search is at fault.\n",
    "\n",
    "Solution: Try with larger $B$\n",
    "\n",
    "**Case 2**\n",
    "$$\\mathbf P(y^\\star|x) \\leq \\mathbf P(\\hat y|x)$$\n",
    "But RNN predicted $P(y^\\star|x) < P(\\hat y|x)$\n",
    "\n",
    "Conclusion: RNN model is at fault.\n",
    "\n",
    "Solution: Try training more data, or regularize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Evaluate Machine Translation. BLEU Score\n",
    "\n",
    "BLEU = Bilingual evaluation understudy\n",
    "\n",
    "- French: Le chat est sur le tapis.\n",
    "- Reference 1: The cat is on the mat.\n",
    "- Reference 2: There is a cat on the mat.\n",
    "- MT Output: the the the the the the the.\n",
    "\n",
    "Calculate the precision for one word:\n",
    "- Denominator: Count of a word in the MT output\n",
    "- Numerator: Max of a word in the reference\n",
    "- Precision: $\\frac27$\n",
    "\n",
    "Calculate the precision for a pair of words:\n",
    "\n",
    "- Reference 1: The cat is on the mat.\n",
    "- Reference 2: There is a cat on the mat.\n",
    "- MT Output: the the the the the the the.\n",
    "\n",
    "- Denominator: Count of a word in the MT output\n",
    "- Numerator: Max of a word in the reference\n",
    "- Precision: $\\frac27$\n",
    "\n",
    "|         | count | max count in reference | Precision |\n",
    "|---------|-------|------------------------|-----------|\n",
    "| the cat | 2     | 1                      | $\\frac12$ |\n",
    "| cat the | 1     | 0                      | 0         |\n",
    "| cat on  | 1     | 1                      | 1         |\n",
    "| on the  | 1     | 1                      | 1         |\n",
    "| the mat | 1     | 1                      | 1         |\n",
    "\n",
    "**BLEU score** on unigram\n",
    "\n",
    "$$\n",
    "p_1 =\\frac{\\sum_{unigram \\in \\hat y} count_{ref}(unigram)}{\\sum_{unigram\\in \\hat y}count(unigram)}\n",
    "$$\n",
    "\n",
    "**BLEU score** on n-gram\n",
    "\n",
    "$$\n",
    "p_1 =\\frac{\\sum_{n-gram \\in \\hat y} count_{ref}(n-gram)}{\\sum_{n-gram\\in \\hat y}count(n-gram)}\n",
    "$$\n",
    "\n",
    "**Combined bleu score**\n",
    "$$\n",
    "BP = \\exp(\\frac14 \\sum_{n=1}^4 p_n)\n",
    "$$\n",
    "\n",
    "BLEU score is used to evaluate if a machine translation is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Attention Model\n",
    "\n",
    "The problem of long sequences is it takes very long history to memorize and would not able to perform well a decoding on translation step.\n",
    "\n",
    "Normally that's what human does: split sentences into parts, generate parts of the translation then combine them in sequence.\n",
    "\n",
    "The **Attention Model** try to look at part of the sentence at a time.\n",
    "\n",
    "<img src=\"F2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Speech Recognition Problem\n",
    "\n",
    "In speech recognition problem, $x$ is an audioclip and $y$ is a transcipt.\n",
    "\n",
    "A preprocessing step can translate the raw clip to spectrogram (false back output). The human ear does the computation quite likely the same as this preprocessing.\n",
    "\n",
    "Nowadays, the preprocessing is not necessary. Instead, people use large dataset (10000h of audio) for training and the model (sequence-to-sequence, attention model) will be able to directly generate the transcript.\n",
    "\n",
    "## 4.1 CTC Cost for Speech Recognition\n",
    "\n",
    "If your audio has frequency 100 Hrtz in 10s, we will have 1000 inputs.\n",
    "It will not output \"the quick brown fox\", but something like that\n",
    "```\n",
    "ttttttttttttt_h_eeeeeee_______[ ]_________qqq__...\n",
    "```\n",
    "\n",
    "## 4.2 Trigger Word Decision\n",
    "\n",
    "Trigger word = \"OK google\", \"Hey Siri\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]\n",
    "\n",
    "[2]\n",
    "\n",
    "[3] Panneni et al., 2002, Bleu: A method for automatic evaluation on machine translation.\n",
    "\n",
    "[4] Najdanau et al., 2014, Neural machine translation by jointly learning to align and translate\n",
    "\n",
    "[5] https://medium.com/machine-learning-bites/deeplearning-series-attention-model-and-speech-recognition-deeb50632152"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
